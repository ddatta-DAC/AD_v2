{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = 'us_import'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let us create APE style test & training sets\n",
    "## us import :  Train on 2015(01-07) Test(08-09)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(_type='all'):\n",
    "    data_dir = os.path.join(\n",
    "        './../wwf_data_v1',\n",
    "        DIR\n",
    "    )\n",
    "    if _type == 'train':\n",
    "        files = sorted(glob.glob(os.path.join(data_dir,'*0[1-4]**2015*.csv')))\n",
    "    elif _type == 'test':\n",
    "        files = sorted(glob.glob(os.path.join(data_dir,'*0[5-6]*2015*.csv')))\n",
    "            \n",
    "    else:\n",
    "        files = sorted(glob.glob(os.path.join(data_dir,'*.csv')))\n",
    "        \n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cols = [ \n",
    "    'PanjivaRecordID',\n",
    "    'ConsigneeCountry',\n",
    "    'ConsigneePanjivaID',\n",
    "    'ShipperCountry',\n",
    "    'ShipperPanjivaID',\n",
    "    'ShipmentOrigin',\n",
    "    'ShipmentDestination',\n",
    "    'hscode_6',\n",
    "    'PortOfUnlading',\n",
    "    'PortOfLading',\n",
    "    'Carrier',\n",
    "]\n",
    "\n",
    "freq_bound = 5\n",
    "column_value_filters = {\n",
    "    'Carrier':['(Usa))']\n",
    "}\n",
    "id_col = 'PanjivaRecordID'\n",
    "ns_id_col = 'NegSampleID'\n",
    "term_2_col = 'term_2'\n",
    "term_4_col = 'term_4'\n",
    "num_neg_samples = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_attr_with_id(row, attr, val2id_dict):\n",
    "    val = row[attr]\n",
    "    if val not in val2id_dict.keys():\n",
    "        print(attr,val)\n",
    "        return None\n",
    "    else:\n",
    "        return val2id_dict[val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./../wwf_data_v1/us_import/panjiva_us_imports_01_2015_filtered.csv', './../wwf_data_v1/us_import/panjiva_us_imports_02_2015_filtered.csv', './../wwf_data_v1/us_import/panjiva_us_imports_03_2015_filtered.csv', './../wwf_data_v1/us_import/panjiva_us_imports_04_2015_filtered.csv', './../wwf_data_v1/us_import/panjiva_us_imports_05_2015_filtered.csv', './../wwf_data_v1/us_import/panjiva_us_imports_06_2015_filtered.csv', './../wwf_data_v1/us_import/panjiva_us_imports_07_2015_filtered.csv']\n",
      "55209\n",
      "Index(['PanjivaRecordID', 'ConsigneeCity', 'ConsigneeStateRegion',\n",
      "       'ConsigneeCountry', 'ConsigneePanjivaID', 'ShipperCity',\n",
      "       'ShipperCountry', 'ShipperPanjivaID', 'Carrier', 'ShipmentOrigin',\n",
      "       'ShipmentDestination', 'PortOfUnlading', 'PortOfLading', 'VolumeTEU',\n",
      "       'WeightKg', 'ValueOfGoodsUSD', 'hscode_6'],\n",
      "      dtype='object')\n",
      "51774\n",
      "Index(['PanjivaRecordID', 'ConsigneeCity', 'ConsigneeStateRegion',\n",
      "       'ConsigneeCountry', 'ConsigneePanjivaID', 'ShipperCity',\n",
      "       'ShipperCountry', 'ShipperPanjivaID', 'Carrier', 'ShipmentOrigin',\n",
      "       'ShipmentDestination', 'PortOfUnlading', 'PortOfLading', 'VolumeTEU',\n",
      "       'WeightKg', 'ValueOfGoodsUSD', 'hscode_6'],\n",
      "      dtype='object')\n",
      "67528\n",
      "Index(['PanjivaRecordID', 'ConsigneeCity', 'ConsigneeStateRegion',\n",
      "       'ConsigneeCountry', 'ConsigneePanjivaID', 'ShipperCity',\n",
      "       'ShipperCountry', 'ShipperPanjivaID', 'Carrier', 'ShipmentOrigin',\n",
      "       'ShipmentDestination', 'PortOfUnlading', 'PortOfLading', 'VolumeTEU',\n",
      "       'WeightKg', 'ValueOfGoodsUSD', 'hscode_6'],\n",
      "      dtype='object')\n",
      "58218\n",
      "Index(['PanjivaRecordID', 'ConsigneeCity', 'ConsigneeStateRegion',\n",
      "       'ConsigneeCountry', 'ConsigneePanjivaID', 'ShipperCity',\n",
      "       'ShipperCountry', 'ShipperPanjivaID', 'Carrier', 'ShipmentOrigin',\n",
      "       'ShipmentDestination', 'PortOfUnlading', 'PortOfLading', 'VolumeTEU',\n",
      "       'WeightKg', 'ValueOfGoodsUSD', 'hscode_6'],\n",
      "      dtype='object')\n",
      "65601\n",
      "Index(['PanjivaRecordID', 'ConsigneeCity', 'ConsigneeStateRegion',\n",
      "       'ConsigneeCountry', 'ConsigneePanjivaID', 'ShipperCity',\n",
      "       'ShipperCountry', 'ShipperPanjivaID', 'Carrier', 'ShipmentOrigin',\n",
      "       'ShipmentDestination', 'PortOfUnlading', 'PortOfLading', 'VolumeTEU',\n",
      "       'WeightKg', 'ValueOfGoodsUSD', 'hscode_6'],\n",
      "      dtype='object')\n",
      "67047\n",
      "Index(['PanjivaRecordID', 'ConsigneeCity', 'ConsigneeStateRegion',\n",
      "       'ConsigneeCountry', 'ConsigneePanjivaID', 'ShipperCity',\n",
      "       'ShipperCountry', 'ShipperPanjivaID', 'Carrier', 'ShipmentOrigin',\n",
      "       'ShipmentDestination', 'PortOfUnlading', 'PortOfLading', 'VolumeTEU',\n",
      "       'WeightKg', 'ValueOfGoodsUSD', 'hscode_6'],\n",
      "      dtype='object')\n",
      "65135\n",
      "Index(['PanjivaRecordID', 'ConsigneeCity', 'ConsigneeStateRegion',\n",
      "       'ConsigneeCountry', 'ConsigneePanjivaID', 'ShipperCity',\n",
      "       'ShipperCountry', 'ShipperPanjivaID', 'Carrier', 'ShipmentOrigin',\n",
      "       'ShipmentDestination', 'PortOfUnlading', 'PortOfLading', 'VolumeTEU',\n",
      "       'WeightKg', 'ValueOfGoodsUSD', 'hscode_6'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "files = get_files(_type='train')\n",
    "print(files)\n",
    "for f in files:\n",
    "    df = pd.read_csv(f)\n",
    "    print(len(df))\n",
    "    print(df.columns)\n",
    "    feature_cols = list(df.columns)\n",
    "    feature_cols.remove(id_col)\n",
    "    feature_cols = list(sorted(feature_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_ids(\n",
    "    df,  \n",
    "    save_dir\n",
    "):\n",
    "    global id_col\n",
    "    \n",
    "    feature_columns = list(df.columns)\n",
    "    feature_columns.remove(id_col)\n",
    "    domain_dims_dict = {}\n",
    "    col_val2id_dict = {}\n",
    "  \n",
    "    for col in sorted(feature_columns):\n",
    "        \n",
    "        vals = list(set(df[col]))\n",
    "        # 0 : item1 , 1 :item2, ....\n",
    "        id2val_dict = {\n",
    "            e[0]: e[1]\n",
    "            for e in enumerate(vals, 0)\n",
    "        }\n",
    "\n",
    "        # item1 : 0, item2 : 1, ...\n",
    "        val2id_dict = {v: k for k, v in id2val_dict.items()}\n",
    "        \n",
    "        col_val2id_dict[col] = val2id_dict\n",
    "        \n",
    "        # replace\n",
    "        df[col] = df.apply(\n",
    "            replace_attr_with_id,\n",
    "            axis=1,\n",
    "            args=(\n",
    "                col,\n",
    "                val2id_dict,\n",
    "            )\n",
    "        )\n",
    "        domain_dims_dict[col] = len(id2val_dict)\n",
    "    domain_dims = []\n",
    "    domain_dims_res = {}\n",
    "    print(list(df.columns))\n",
    "    \n",
    "    for col in list(df.columns):\n",
    "        if col in domain_dims_dict.keys():\n",
    "            print(col)\n",
    "            domain_dims_res[col] = domain_dims_dict[col]\n",
    "            domain_dims.append(domain_dims_dict[col])\n",
    "        \n",
    "    domain_dims =  np.array(domain_dims)\n",
    "    print(domain_dims_res)\n",
    "\n",
    "    file = 'domain_dims.pkl'\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "        \n",
    "    f_path = os.path.join(save_dir, file)\n",
    "    \n",
    "    with open(f_path,'wb') as fh:\n",
    "        pickle.dump(\n",
    "            domain_dims_res,\n",
    "            fh,\n",
    "            pickle.HIGHEST_PROTOCOL\n",
    "        )\n",
    "    return df, col_val2id_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(file_list):\n",
    "    global id_col\n",
    "    _master_df = None\n",
    "    for file in file_list:\n",
    "        _df = pd.read_csv(\n",
    "            file, \n",
    "            low_memory=False,\n",
    "            usecols = use_cols\n",
    "        )\n",
    "        _df = _df.dropna()\n",
    "        if _master_df is None:\n",
    "            _master_df = pd.DataFrame(_df)\n",
    "        else:\n",
    "            _master_df = _master_df.append(\n",
    "                _df,\n",
    "                ignore_index=True\n",
    "            )\n",
    "    feature_cols = list(_master_df.columns)\n",
    "    feature_cols.remove(id_col)\n",
    "    feature_cols = list(sorted(feature_cols))\n",
    "    all_cols = [id_col]\n",
    "    all_cols.extend(feature_cols)\n",
    "    print(all_cols)\n",
    "    _master_df = _master_df[all_cols]\n",
    "    return _master_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_low_frequency_values(_df):\n",
    "    global id_col\n",
    "    global freq_bound \n",
    "    from collections import Counter\n",
    "    \n",
    "    freq_column_value_filters = {}\n",
    "    \n",
    "    feature_cols = list(_df.columns)\n",
    "    feature_cols.remove(id_col)\n",
    "    \n",
    "    for c in feature_cols:\n",
    "        values = list(_df[c])\n",
    "        freq_column_value_filters [c] = []\n",
    "        items = set(values)\n",
    "        obj_counter = Counter(values)\n",
    "        for _item, _count in  obj_counter.items():\n",
    "            if _count < freq_bound :\n",
    "                  freq_column_value_filters[c].append(_item)\n",
    "    \n",
    "    for c,_items in freq_column_value_filters.items():\n",
    "        print(c, len(_items))\n",
    "    print(len(_df))\n",
    "    for col,val in freq_column_value_filters.items():\n",
    "        _df = _df.loc[\n",
    "            (~_df[col].isin(val))\n",
    "        ]\n",
    "    print(len(_df))   \n",
    "    return _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(row,ref_df):\n",
    "    global id_col\n",
    "    query_str = []\n",
    "    for _c, _i in row.to_dict().items():\n",
    "        if _c == id_col: \n",
    "            continue\n",
    "        query_str.append(' ' + _c + ' == ' + str(_i) )\n",
    "    query_str = ' & '.join(query_str)\n",
    "    res_query = ref_df.query(query_str)\n",
    "  \n",
    "    if len(res_query) > 0 : \n",
    "        return False\n",
    "    return True \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "returns c random items as a dict\n",
    "column_name : item_id\n",
    "'''\n",
    "\n",
    "def  get_c_vals(anomaly_cols, col_val2id_dict):\n",
    "    res_dict = {}\n",
    "    for col in anomaly_cols:\n",
    "        res_dict[col] = random.sample(list(col_val2id_dict[col].values()), 1)[0]\n",
    "    return res_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_anomalies(test_df, train_df, col_val2id_dict, c=3):\n",
    "    global id_col\n",
    "    feature_cols = list(test_df.columns)\n",
    "    feature_cols.remove(id_col)\n",
    "    feature_cols_id = { e[0]:e[1] for e in enumerate(feature_cols)}\n",
    "    ref_df = pd.DataFrame(train_df,copy=True)\n",
    "    ref_df = ref_df.append(\n",
    "        test_df,\n",
    "        ignore_index=True\n",
    "    )\n",
    "    new_df = pd.DataFrame(columns=list(test_df.columns))\n",
    "    \n",
    "    for i,row  in test_df.iterrows():   \n",
    "         while True:\n",
    "            _anomaly_cols = [feature_cols_id[_] \n",
    "                             for _ in random.sample(\n",
    "                                 list(feature_cols_id.keys()), \n",
    "                                 k=3\n",
    "                             )\n",
    "                        ]\n",
    "            c_vals = get_c_vals(_anomaly_cols, col_val2id_dict)\n",
    "            row_copy = pd.Series(row,copy = True)\n",
    "            for _col, _item_id in c_vals.items():\n",
    "                row_copy[_col] = _item_id\n",
    "            if validate(row_copy, ref_df):\n",
    "                row_copy[id_col] = int( str(row_copy[id_col]) + '01' )\n",
    "                new_df = new_df.append(row_copy, ignore_index=True)\n",
    "                break;\n",
    "        \n",
    "    # sample c cols\n",
    "    new_df = new_df.drop_duplicates(subset=feature_cols)\n",
    "    print(' Length of anomalies_df ',new_df)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_testing_data( test_df, train_df, col_val2id_dict):\n",
    "    global id_col\n",
    "    # Replace with None if ids are not in train_set\n",
    "    print('----')\n",
    "    feature_cols = list(test_df.columns)\n",
    "    feature_cols.remove(id_col)\n",
    "    \n",
    "    for col in feature_cols:\n",
    "        valid_items = list(col_val2id_dict[col].keys())\n",
    "        test_df = test_df.loc[test_df[col].isin(valid_items)]\n",
    "        \n",
    "    print(' Length of testing data' , len(test_df))\n",
    "    \n",
    "    \n",
    "    # First convert to to ids\n",
    "    for col in feature_cols:\n",
    "        val2id_dict = col_val2id_dict[col]\n",
    "        test_df[col] = test_df.apply(\n",
    "                replace_attr_with_id,\n",
    "                axis = 1,\n",
    "                args = (\n",
    "                    col,\n",
    "                    val2id_dict,\n",
    "                )\n",
    "            )\n",
    "    '''\n",
    "    Remove duplicates :\n",
    "    '''\n",
    "    \n",
    "    print(' Length of test df :: ', len(test_df) )\n",
    "    new_test_df = pd.DataFrame( columns= list(test_df.columns))\n",
    "    \n",
    "    for i,row in test_df.iterrows():\n",
    "        if validate(row, train_df):\n",
    "            new_test_df = new_test_df.append(row,ignore_index=True)\n",
    "            print(len(new_test_df))\n",
    "    print(' After deduplication :: ', len(new_test_df))\n",
    "    \n",
    "    anomalies_df = create_anomalies(new_test_df, train_df, col_val2id_dict, c=3)\n",
    "    return new_test_df, anomalies_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_us_import_train_test_sets():\n",
    "    global use_cols\n",
    "    global DIR\n",
    "    global save_dir\n",
    "    global column_value_filters\n",
    "    train_files = get_files('train')\n",
    "    test_files = get_files('test')\n",
    "  \n",
    "    # combine train_data :\n",
    "    train_master_df = collate(train_files)\n",
    "    test_master_df = collate(test_files)\n",
    "    \n",
    "    print(' Train initial ', len(train_master_df)) \n",
    "    print(' Test initial ', len(test_master_df)) \n",
    "            \n",
    "    save_dir = os.path.join('./../generated_data',DIR)\n",
    "    \n",
    "    '''\n",
    "    test data preprocessing\n",
    "    '''\n",
    "    print(len(train_master_df))\n",
    "    \n",
    "    '''\n",
    "    Remove values that are garbage\n",
    "    '''\n",
    "    for col,val in column_value_filters.items():\n",
    "        train_master_df = train_master_df.loc[\n",
    "            (~train_master_df[col].isin(val))\n",
    "        ]\n",
    "         \n",
    "    print(' Length of training data ', len(train_master_df))  \n",
    "    \n",
    "    train_master_df = remove_low_frequency_values(\n",
    "        train_master_df\n",
    "    )\n",
    "    \n",
    "    train_master_df_1, col_val2id_dict = convert_to_ids(\n",
    "        train_master_df,\n",
    "        save_dir\n",
    "    )\n",
    "    \n",
    "    new_test_df, anomalies_df = setup_testing_data(\n",
    "        test_master_df,\n",
    "        train_master_df_1,\n",
    "        col_val2id_dict\n",
    "    )\n",
    "    \n",
    "    # Save the data\n",
    "    new_test_df.to_csv(os.path.join(save_dir,'test_data.csv'),index=False)\n",
    "    train_master_df_1.to_csv(os.path.join(save_dir,'train_data.csv'),index=False)\n",
    "    anomalies_df.to_csv(os.path.join(save_dir,'anomalies_test_data.csv'),index=False)\n",
    "    \n",
    "    return \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PanjivaRecordID', 'Carrier', 'ConsigneeCountry', 'ConsigneePanjivaID', 'PortOfLading', 'PortOfUnlading', 'ShipmentDestination', 'ShipmentOrigin', 'ShipperCountry', 'ShipperPanjivaID', 'hscode_6']\n",
      "['PanjivaRecordID', 'Carrier', 'ConsigneeCountry', 'ConsigneePanjivaID', 'PortOfLading', 'PortOfUnlading', 'ShipmentDestination', 'ShipmentOrigin', 'ShipperCountry', 'ShipperPanjivaID', 'hscode_6']\n",
      " Train initial  177242\n",
      " Test initial  100900\n",
      "177242\n",
      " Length of training data  177242\n",
      "Carrier 272\n",
      "ConsigneeCountry 37\n",
      "ConsigneePanjivaID 19758\n",
      "PortOfLading 139\n",
      "PortOfUnlading 32\n",
      "ShipmentDestination 51\n",
      "ShipmentOrigin 39\n",
      "ShipperCountry 46\n",
      "ShipperPanjivaID 20545\n",
      "hscode_6 8\n",
      "177242\n",
      "134662\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-5e9a92194928>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcreate_us_import_train_test_sets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-49b72f9dfee6>\u001b[0m in \u001b[0;36mcreate_us_import_train_test_sets\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m     train_master_df_1, col_val2id_dict = convert_to_ids(\n\u001b[1;32m     38\u001b[0m         \u001b[0mtrain_master_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0msave_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     )\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-56c4139daad8>\u001b[0m in \u001b[0;36mconvert_to_ids\u001b[0;34m(df, save_dir)\u001b[0m\n\u001b[1;32m     30\u001b[0m             args=(\n\u001b[1;32m     31\u001b[0m                 \u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                 \u001b[0mval2id_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m             )\n\u001b[1;32m     34\u001b[0m         )\n",
      "\u001b[0;32m~/Code/ad_1/venv/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, broadcast, raw, reduce, result_type, args, **kwds)\u001b[0m\n\u001b[1;32m   6485\u001b[0m                          \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6486\u001b[0m                          kwds=kwds)\n\u001b[0;32m-> 6487\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6489\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapplymap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/ad_1/venv/lib/python3.6/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_empty_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/ad_1/venv/lib/python3.6/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m                                           \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m                                           \u001b[0mdummy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdummy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m                                           labels=labels)\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor_sliced\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/reduction.pyx\u001b[0m in \u001b[0;36mpandas._libs.reduction.reduce\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/reduction.pyx\u001b[0m in \u001b[0;36mpandas._libs.reduction.Reducer.get_result\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/Code/ad_1/venv/lib/python3.6/site-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-59999ab3285e>\u001b[0m in \u001b[0;36mreplace_attr_with_id\u001b[0;34m(row, attr, val2id_dict)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreplace_attr_with_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval2id_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval2id_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/ad_1/venv/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/ad_1/venv/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   4368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4369\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4370\u001b[0;31m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4372\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_scalar_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'getitem'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "create_us_import_train_test_sets()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "def get_neg_sample_ape(_k, column_id, column_name, ref_df, column_valid_values, orig_row, P_A, feature_cols_id ):\n",
    "    global id_col\n",
    "    global ns_id_col\n",
    "    global term_4_col \n",
    "    global term_2_col\n",
    "    \n",
    "    new_row = pd.Series(orig_row,copy=True)\n",
    "    Pid_val = orig_row[id_col]\n",
    "    while True:\n",
    "        _random = random.sample(\n",
    "            column_valid_values[column_name], 1\n",
    "        )[0]\n",
    "        new_row[column_name] = _random\n",
    "        if validate(new_row, ref_df):\n",
    "            new_row = pd.Series(orig_row,copy=True)\n",
    "            new_row[ns_id_col] = int( '10' + str(_k) + str(column_id) + str(Pid_val) + '01' )\n",
    "            new_row[term_4_col] =  np.log(P_A[column_id][_random])\n",
    "            _tmp = 0                         \n",
    "            for _fci, _fcn  in feature_cols_id.items():\n",
    "                _val = P_A[_fci][orig_row[_fcn]]\n",
    "                _tmp += math.log( _val, math.e)\n",
    "            _tmp /= len(feature_cols_id)\n",
    "            new_row[term_2_col] = _tmp                             \n",
    "            return new_row   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_negative_samples_ape_aux(idx, df_chunk, feature_cols, ref_df, column_valid_values, save_dir, P_A, feature_cols_id):\n",
    "    global ns_id_col\n",
    "    global term_4_col\n",
    "    global term_2_col\n",
    "    global id_col\n",
    "    \n",
    "    ns_id_col='NegSampleID'\n",
    "    \n",
    "    term_2_col = 'term_2'\n",
    "    term_4_col = 'term_4'\n",
    "    feature_cols_id = { \n",
    "        e[0]:e[1] \n",
    "        for e in enumerate(feature_cols)\n",
    "    }\n",
    "    \n",
    "    new_df = pd.DataFrame(\n",
    "        columns=list(ref_df.columns)\n",
    "    )\n",
    "    \n",
    "    new_df[ns_id_col] = 0\n",
    "    new_df[term_4_col] = 0\n",
    "    new_df[term_2_col] = 0\n",
    "    \n",
    "    for i,row  in df_chunk.iterrows():\n",
    "        # for each column\n",
    "        Pid_val = row[id_col]\n",
    "        for column_id,column_name in feature_cols_id.items():\n",
    "             for _k in range(num_neg_samples):\n",
    "                _res = get_neg_sample_ape(\n",
    "                    _k, column_id, column_name, ref_df, column_valid_values, row,  P_A, feature_cols_id\n",
    "                )   \n",
    "                new_df = new_df.append(\n",
    "                    _res,\n",
    "                    ignore_index=True\n",
    "                )    \n",
    "                \n",
    "            \n",
    "    if not os.path.exists(os.path.join(save_dir, 'tmp')):\n",
    "        os.mkdir(os.path.join(save_dir, 'tmp'))\n",
    "    f_name = os.path.join(save_dir, 'tmp', 'tmp_df_'+str(idx)+'.csv')\n",
    "    new_df.to_csv(\n",
    "        f_name,\n",
    "        index=None\n",
    "    )\n",
    "    \n",
    "    return f_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def create_negative_samples_ape():\n",
    "    global DIR\n",
    "    global save_dir\n",
    "    global id_col\n",
    "    global ns_id_col\n",
    "    global num_neg_samples\n",
    "    save_dir = os.path.join(\n",
    "        './../generated_data',\n",
    "        DIR\n",
    "    )\n",
    "    \n",
    "    train_data_file = os.path.join(save_dir, 'train_data.csv')\n",
    "    test_data_file = os.path.join(save_dir, 'test_data.csv')\n",
    "    \n",
    "    train_df = pd.read_csv(\n",
    "        train_data_file, \n",
    "        index_col=None\n",
    "    )\n",
    "       \n",
    "    '''\n",
    "    Randomly generate samples\n",
    "    choose k=3 * m=7 = 21 negative samples per training instance\n",
    "    For negative samples pick one entity & replace it it randomly \n",
    "    Validate if generated negative sample is not part of the test or training set\n",
    "    '''\n",
    "    ref_df = pd.DataFrame(\n",
    "        train_df,\n",
    "        copy=True\n",
    "    ) \n",
    "       \n",
    "    feature_cols = list(train_df.columns)\n",
    "    feature_cols.remove(id_col)\n",
    "    feature_cols_id = { \n",
    "        e[0]:e[1] \n",
    "        for e in enumerate(feature_cols)\n",
    "    }\n",
    "    \n",
    "    # get the domain dimensions\n",
    "    with open(os.path.join(save_dir,'domain_dims.pkl'),'rb') as fh:\n",
    "        domain_dims = pickle.load(fh)\n",
    "        \n",
    "    print(domain_dims)\n",
    "    \n",
    "    # This id for the 4th term\n",
    "    P_A = {}\n",
    "    for _fci, _fcn  in feature_cols_id.items():\n",
    "        _series = pd.Series(train_df[_fcn])\n",
    "        tmp = _series.value_counts(normalize=True)\n",
    "        P_Aa = tmp.to_dict()\n",
    "        for _z in range(domain_dims[_fcn]):\n",
    "            if _z not in P_Aa.keys():\n",
    "                P_Aa[_z] = math.pow(10, -3)\n",
    "        P_A[_fci] = P_Aa\n",
    "        \n",
    "        \n",
    "    # Store what are valid values for each columns\n",
    "    column_valid_values = {}\n",
    "    for _fc_name in feature_cols:\n",
    "        column_valid_values[_fc_name] = list(set(list(ref_df[_fc_name])))\n",
    "        \n",
    "    num_chunks = 10\n",
    "    chunk_len = int(len(train_df)/(num_chunks-1))\n",
    "    \n",
    "    list_df_chunks = np.split(\n",
    "        train_df.head(\n",
    "            chunk_len*(num_chunks-1)\n",
    "        ),num_chunks-1\n",
    "    )\n",
    "    \n",
    "    end_len = len(train_df) -  chunk_len*(num_chunks-1)\n",
    "    list_df_chunks.append(train_df.tail(end_len))\n",
    "    for _l in  range(len(list_df_chunks)):\n",
    "        print(len(list_df_chunks[_l]), _l)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    #     for _i in range(len(list_df_chunks)):\n",
    "    #         _res = create_negative_samples_aux(_i, list_df_chunks[_i], feature_cols, ref_df, column_valid_values, save_dir)\n",
    "    #         results.append(_res)\n",
    "\n",
    "    results = Parallel(n_jobs = 10)(delayed\n",
    "        (create_negative_samples_ape_aux)(\n",
    "            _i, list_df_chunks[_i], feature_cols, ref_df, column_valid_values, save_dir, P_A, feature_cols_id)\n",
    "            for _i in range(\n",
    "                len(list_df_chunks)\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    new_df = None\n",
    "    for _f in results :\n",
    "        _df = pd.read_csv(_f, index_col = None)\n",
    "        \n",
    "        if new_df is None:\n",
    "            new_df = _df\n",
    "        else :\n",
    "            new_df = new_df.append(_df, ignore_index=True)\n",
    "        print(' >> ' ,len(new_df))\n",
    "    \n",
    "    new_df.to_csv(os.path.join(save_dir,'negative_samples_ape_1.csv'),index=False)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Carrier': 502, 'ConsigneeCountry': 67, 'ConsigneePanjivaID': 4197, 'PortOfLading': 225, 'PortOfUnlading': 63, 'ShipmentDestination': 108, 'ShipmentOrigin': 112, 'ShipperCountry': 111, 'ShipperPanjivaID': 5133, 'hscode_6': 93}\n",
      "14962 0\n",
      "14962 1\n",
      "14962 2\n",
      "14962 3\n",
      "14962 4\n",
      "14962 5\n",
      "14962 6\n",
      "14962 7\n",
      "14962 8\n",
      "4 9\n",
      " >>  448860\n",
      " >>  897720\n",
      " >>  1346580\n",
      " >>  1795440\n",
      " >>  2244300\n",
      " >>  2693160\n",
      " >>  3142020\n",
      " >>  3590880\n",
      " >>  4039740\n",
      " >>  4039860\n"
     ]
    }
   ],
   "source": [
    "neg_df = create_negative_samples_ape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Create numpy arrays \n",
    "Store in .pkl files\n",
    "'''\n",
    "\n",
    "def create_ape_model_data():\n",
    "    global DIR\n",
    "    global term_2_col\n",
    "    global term_4_col\n",
    "    global save_dir\n",
    "    global id_col\n",
    "    global ns_id_col\n",
    "    global num_neg_samples\n",
    "    save_dir = os.path.join(\n",
    "        './../generated_data',\n",
    "        DIR\n",
    "    )\n",
    "    \n",
    "    train_pos_data_file = os.path.join(save_dir, 'train_data.csv')\n",
    "    train_neg_data_file = os.path.join(save_dir, 'negative_samples_ape_1.csv')\n",
    "    test_data_file = os.path.join(save_dir, 'test_data.csv')\n",
    "    anomalies_data_file = os.path.join(save_dir, 'anomalies_test_data.csv')\n",
    "    \n",
    "    # ------------------- #\n",
    "    \n",
    "    train_pos_df = pd.read_csv(\n",
    "        train_pos_data_file, \n",
    "        index_col=None\n",
    "    )\n",
    "    \n",
    "    test_df = pd.read_csv(\n",
    "        test_data_file, \n",
    "        index_col=None\n",
    "    )\n",
    "    \n",
    "    neg_samples_df = pd.read_csv(\n",
    "        train_neg_data_file, \n",
    "        index_col=None\n",
    "    )\n",
    "    \n",
    "    anomalies_df = pd.read_csv(\n",
    "        anomalies_data_file,\n",
    "        index_col=None\n",
    "    )\n",
    "    \n",
    "    feature_cols =  list(train_pos_df.columns)\n",
    "    feature_cols.remove(id_col)\n",
    "    neg_samples = num_neg_samples * len(feature_cols)\n",
    "    \n",
    "    \n",
    "    # Anomalies generated have fake panjiva id\n",
    "    test_anomaly_idList = list(anomalies_df[id_col])\n",
    "    test_normal_idList = list(test_df[id_col])\n",
    "    \n",
    "    try:\n",
    "        del test_df[id_col] \n",
    "        del anomalies_df[id_col]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    matrix_test = test_df.values\n",
    "    matrix_anomaly = anomalies_df.values\n",
    "    \n",
    "    num_data_pts = len(train_pos_df)\n",
    "    num_domains = len(feature_cols)\n",
    "    \n",
    "    matrix_pos = []\n",
    "    matrix_neg = []\n",
    "    \n",
    "    term_2 = []\n",
    "    term_4 = []\n",
    "    \n",
    "    index = 0 \n",
    "    for i,row in train_pos_df.iterrows():\n",
    "        _tmp = pd.DataFrame(\n",
    "            neg_samples_df.loc[neg_samples_df[id_col]==row[id_col]],\n",
    "            copy=True\n",
    "        )\n",
    "        \n",
    "        _term_2 = list(_tmp[term_2_col])[0]\n",
    "        _term_4 = list(_tmp[term_4_col])\n",
    "        \n",
    "        del _tmp[ns_id_col]\n",
    "        del _tmp[id_col]\n",
    "        del _tmp[term_2_col]\n",
    "        del _tmp[term_4_col]\n",
    "        del row[id_col]\n",
    "        \n",
    "        vals_n = np.array(_tmp.values)\n",
    "        vals_p = list(row.values)\n",
    "        matrix_neg.append(vals_n)\n",
    "        matrix_pos.append(vals_p)\n",
    "        \n",
    "        term_2.append(_term_2)\n",
    "        term_4.append(_term_4)\n",
    "        index += 1 \n",
    "       \n",
    "    matrix_pos = np.array(matrix_pos)\n",
    "    matrix_neg = np.array(matrix_neg)\n",
    "    \n",
    "    term_2 = np.array(term_2)\n",
    "    term_4 = np.array(term_4)\n",
    "    \n",
    "    print(matrix_pos.shape, matrix_neg.shape)\n",
    "    print(term_2.shape, term_4.shape)\n",
    "    \n",
    "    # Save files\n",
    "    f_path =  os.path.join(\n",
    "        save_dir,\n",
    "        'matrix_train_positive.pkl'\n",
    "    )\n",
    "    \n",
    "    with open(f_path,'wb') as fh:\n",
    "        pickle.dump(\n",
    "            matrix_pos,\n",
    "            fh,\n",
    "            pickle.HIGHEST_PROTOCOL\n",
    "        )\n",
    "    f_path =  os.path.join(save_dir,'ape_negative_samples.pkl')\n",
    "    with open(f_path,'wb') as fh:\n",
    "        pickle.dump(\n",
    "            matrix_neg,\n",
    "            fh,\n",
    "            pickle.HIGHEST_PROTOCOL\n",
    "    )\n",
    "    \n",
    "    \n",
    "    f_path =  os.path.join(save_dir,'ape_term_2.pkl')\n",
    "    with open(f_path,'wb') as fh:\n",
    "        pickle.dump(\n",
    "            term_2,\n",
    "            fh,\n",
    "            pickle.HIGHEST_PROTOCOL\n",
    "    )\n",
    "        \n",
    "    f_path =  os.path.join(save_dir,'ape_term_4.pkl')\n",
    "    with open(f_path,'wb') as fh:\n",
    "        pickle.dump(\n",
    "            term_4,\n",
    "            fh,\n",
    "            pickle.HIGHEST_PROTOCOL\n",
    "    )\n",
    "    \n",
    "    \n",
    "    f_path =  os.path.join(save_dir,'matrix_test_positive.pkl')\n",
    "    with open(f_path,'wb') as fh:\n",
    "        pickle.dump(\n",
    "            matrix_test,\n",
    "            fh,\n",
    "            pickle.HIGHEST_PROTOCOL\n",
    "    )\n",
    "        \n",
    "    \n",
    "    f_path =  os.path.join(save_dir,'matrix_test_anomalies.pkl')\n",
    "    with open(f_path,'wb') as fh:\n",
    "        pickle.dump(\n",
    "            matrix_anomaly,\n",
    "            fh,\n",
    "            pickle.HIGHEST_PROTOCOL\n",
    "    )\n",
    "        \n",
    "    f_path =  os.path.join(save_dir,'test_idList.pkl')\n",
    "    with open(f_path,'wb') as fh:\n",
    "        pickle.dump(\n",
    "            [test_anomaly_idList, test_normal_idList],\n",
    "            fh,\n",
    "            pickle.HIGHEST_PROTOCOL\n",
    "    )\n",
    "        \n",
    "        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(134662, 10) (134662, 30, 10)\n",
      "(134662,) (134662, 30)\n"
     ]
    }
   ],
   "source": [
    "create_ape_model_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # --------------------------------------------------------------- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neg_sample_v1(\n",
    "    _k,\n",
    "    ref_df,\n",
    "    column_valid_values,\n",
    "    orig_row,\n",
    "    feature_cols_id\n",
    "):\n",
    "    \n",
    "    global id_col\n",
    "    global ns_id_col\n",
    "    \n",
    "    \n",
    "    Pid_val = orig_row[id_col]\n",
    "    num_features = len(feature_cols_id)\n",
    "    num_randomizations = random.randint(1,int(num_features/2))\n",
    "    \n",
    "    # iterate while a real noise is not generated\n",
    "    while True:\n",
    "        \n",
    "        target_cols = [feature_cols_id[_] \n",
    "                             for _ in random.sample(\n",
    "                                 list(feature_cols_id.keys()), \n",
    "                                 k=num_randomizations\n",
    "                             )\n",
    "                        ]\n",
    "        c_vals = {}\n",
    "        for _tc in target_cols:\n",
    "            c_vals[_tc] = random.sample(column_valid_values[_tc], 1)[0]\n",
    "        \n",
    "        new_row = pd.Series(orig_row,copy=True)\n",
    "        for _col, _item_id in c_vals.items():\n",
    "            new_row[_col] = _item_id\n",
    "            \n",
    "            \n",
    "        if validate(new_row, ref_df):\n",
    "            new_row[ns_id_col] = int(  str(Pid_val) + '01' + str(_k)  )\n",
    "            break\n",
    "            \n",
    "    return new_row   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_negative_samples_v1_aux(idx, df_chunk, feature_cols, ref_df, column_valid_values, save_dir, feature_cols_id , num_neg_samples_v1):\n",
    "    \n",
    "    global ns_id_col\n",
    "    global id_col\n",
    "    \n",
    "    ns_id_col='NegSampleID'\n",
    "    \n",
    "    feature_cols_id = { \n",
    "        e[0]:e[1] \n",
    "        for e in enumerate(feature_cols)\n",
    "    }\n",
    "    \n",
    "    new_df = pd.DataFrame(\n",
    "        columns=list(ref_df.columns)\n",
    "    )\n",
    "    \n",
    "    new_df[ns_id_col] = 0\n",
    "    for i,row  in df_chunk.iterrows():\n",
    "        \n",
    "        Pid_val = row[id_col]\n",
    "        for _k in range(num_neg_samples_v1):\n",
    "                    \n",
    "                _res = get_neg_sample_v1(\n",
    "                    _k, ref_df, column_valid_values, row, feature_cols_id\n",
    "                )   \n",
    "                new_df = new_df.append(\n",
    "                    _res,\n",
    "                    ignore_index=True\n",
    "                )    \n",
    "       \n",
    "            \n",
    "    if not os.path.exists(os.path.join(save_dir, 'tmp')):\n",
    "        os.mkdir(os.path.join(save_dir, 'tmp'))\n",
    "    f_name = os.path.join(save_dir, 'tmp', 'tmp_df_'+str(idx)+'.csv')\n",
    "    new_df.to_csv(\n",
    "        f_name,\n",
    "        index=None\n",
    "    )\n",
    "    \n",
    "    return f_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_negative_samples_v1():\n",
    "    global DIR\n",
    "    global save_dir\n",
    "    global id_col\n",
    "    global ns_id_col\n",
    "    \n",
    "    save_dir = os.path.join(\n",
    "        './../generated_data',\n",
    "        DIR\n",
    "    )\n",
    "    \n",
    "    train_data_file = os.path.join(save_dir, 'train_data.csv')\n",
    "    \n",
    "    train_df = pd.read_csv(\n",
    "        train_data_file, \n",
    "        index_col=None\n",
    "    )\n",
    "       \n",
    "    '''\n",
    "    Randomly generate samples\n",
    "    choose 15 negative samples per training instance\n",
    "    For negative samples pick m entities & replace it it randomly \n",
    "    m randomly between (1, d/2)\n",
    "    Validate if generated negative sample is not part of the test or training set\n",
    "    '''\n",
    "    ref_df = pd.DataFrame(\n",
    "        train_df,\n",
    "        copy=True\n",
    "    ) \n",
    "    num_neg_samples_v1 = 15   \n",
    "    feature_cols = list(train_df.columns)\n",
    "    feature_cols.remove(id_col)\n",
    "    feature_cols_id = { \n",
    "        e[0]:e[1] \n",
    "        for e in enumerate(feature_cols)\n",
    "    }\n",
    "    \n",
    "    # get the domain dimensions\n",
    "    with open(\n",
    "        os.path.join(save_dir,'domain_dims.pkl'),'rb'\n",
    "    ) as fh:\n",
    "        domain_dims = pickle.load(fh)        \n",
    "        \n",
    "    # Store what are valid values for each columns\n",
    "    column_valid_values = {}\n",
    "    for _fc_name in feature_cols:\n",
    "        column_valid_values[_fc_name] = list(set(list(ref_df[_fc_name])))\n",
    "        \n",
    "    num_chunks = 10\n",
    "    chunk_len = int(len(train_df)/(num_chunks-1))\n",
    "    \n",
    "    list_df_chunks = np.split(\n",
    "        train_df.head(\n",
    "            chunk_len*(num_chunks-1)\n",
    "        ),num_chunks-1\n",
    "    )\n",
    "    \n",
    "    end_len = len(train_df) -  chunk_len*(num_chunks-1)\n",
    "    list_df_chunks.append(train_df.tail(end_len))\n",
    "    for _l in  range(len(list_df_chunks)):\n",
    "        print(len(list_df_chunks[_l]), _l)\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    results = Parallel(n_jobs = 10)(delayed\n",
    "        (create_negative_samples_v1_aux)(\n",
    "            _i, list_df_chunks[_i], feature_cols, ref_df, column_valid_values, save_dir, feature_cols_id, num_neg_samples_v1)\n",
    "            for _i in range(\n",
    "                len(list_df_chunks)\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    new_df = None\n",
    "    for _f in results :\n",
    "        _df = pd.read_csv(_f, index_col = None)\n",
    "        \n",
    "        if new_df is None:\n",
    "            new_df = _df\n",
    "        else :\n",
    "            new_df = new_df.append(_df, ignore_index=True)\n",
    "        print(' >> ' ,len(new_df))\n",
    "    \n",
    "    new_df.to_csv(os.path.join(save_dir,'negative_samples_v1.csv'),index=False)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14962 0\n",
      "14962 1\n",
      "14962 2\n",
      "14962 3\n",
      "14962 4\n",
      "14962 5\n",
      "14962 6\n",
      "14962 7\n",
      "14962 8\n",
      "4 9\n",
      " >>  224430\n",
      " >>  448860\n",
      " >>  673290\n",
      " >>  897720\n",
      " >>  1122150\n",
      " >>  1346580\n",
      " >>  1571010\n",
      " >>  1795440\n",
      " >>  2019870\n",
      " >>  2019930\n"
     ]
    }
   ],
   "source": [
    "new_df = create_negative_samples_v1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_model_data_v1():\n",
    "    global DIR\n",
    "    global term_2_col\n",
    "    global term_4_col\n",
    "    global save_dir\n",
    "    global id_col\n",
    "    global ns_id_col\n",
    "    global num_neg_samples\n",
    "    save_dir = os.path.join(\n",
    "        './../generated_data',\n",
    "        DIR\n",
    "    )\n",
    "    \n",
    "    train_pos_data_file = os.path.join(save_dir, 'train_data.csv')\n",
    "    train_neg_data_file = os.path.join(save_dir, 'negative_samples_v1.csv')\n",
    "    \n",
    "    # ------------------- #\n",
    "    \n",
    "    train_pos_df = pd.read_csv(\n",
    "        train_pos_data_file, \n",
    "        index_col=None\n",
    "    )\n",
    "    \n",
    "    neg_samples_df = pd.read_csv(\n",
    "        train_neg_data_file, \n",
    "        index_col=None\n",
    "    )\n",
    "    \n",
    "    \n",
    "    feature_cols =  list(train_pos_df.columns)\n",
    "    feature_cols.remove(id_col)\n",
    "    neg_samples = num_neg_samples * len(feature_cols)\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        del test_df[id_col] \n",
    "        del anomalies_df[id_col]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    num_data_pts = len(train_pos_df)\n",
    "    num_domains = len(feature_cols)\n",
    "    \n",
    "    matrix_pos = []\n",
    "    matrix_neg = []\n",
    "\n",
    "    \n",
    "    index = 0 \n",
    "    for i,row in train_pos_df.iterrows():\n",
    "        _row = pd.Series(row,copy=True)\n",
    "        _tmp = pd.DataFrame(\n",
    "            neg_samples_df.loc[neg_samples_df[id_col]==row[id_col]],\n",
    "            copy=True\n",
    "        )\n",
    "        \n",
    "        del _tmp[ns_id_col]\n",
    "        del _tmp[id_col]\n",
    "        del _row[id_col]\n",
    "        \n",
    "        vals_n = np.array(_tmp.values)\n",
    "        vals_p = list(_row.values)\n",
    "        matrix_neg.append(vals_n)\n",
    "        matrix_pos.append(vals_p)\n",
    "        \n",
    "        index += 1 \n",
    "       \n",
    "    matrix_pos = np.array(matrix_pos)\n",
    "    matrix_neg = np.array(matrix_neg)\n",
    "\n",
    "    \n",
    "    print(matrix_pos.shape, matrix_neg.shape)\n",
    "\n",
    "    \n",
    "    # Save files\n",
    "    f_path =  os.path.join(\n",
    "        save_dir,\n",
    "        'matrix_train_positive_v1.pkl'\n",
    "    )\n",
    "    \n",
    "    with open(f_path,'wb') as fh:\n",
    "        pickle.dump(\n",
    "            matrix_pos,\n",
    "            fh,\n",
    "            pickle.HIGHEST_PROTOCOL\n",
    "        )\n",
    "    f_path =  os.path.join(save_dir,'negative_samples_v1.pkl')\n",
    "    with open(f_path,'wb') as fh:\n",
    "        pickle.dump(\n",
    "            matrix_neg,\n",
    "            fh,\n",
    "            pickle.HIGHEST_PROTOCOL\n",
    "    )\n",
    "\n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(134662, 10) (134662, 15, 10)\n"
     ]
    }
   ],
   "source": [
    " create_model_data_v1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
