{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './../../generated_data'\n",
    "DIR = 'peru_export'\n",
    "id_col = 'PanjivaRecordID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up an indexing scheme where each of the entities of each column is given a separate id\n",
    "# Join the train & test sets\n",
    "# For each column take the set of entities and \"enumerate\" them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_file = os.path.join( DATA_DIR, DIR, 'train_data.csv' )\n",
    "test_data_file = os.path.join( DATA_DIR, DIR, 'test_data.csv' )\n",
    "anomaly_data_file_c1 = os.path.join( DATA_DIR, DIR, 'anomalies_c1_data.csv' )\n",
    "anomaly_data_file_c2 = os.path.join( DATA_DIR, DIR, 'anomalies_c2_data.csv' )\n",
    "anomaly_data_file_c3 = os.path.join( DATA_DIR, DIR, 'anomalies_c3_data.csv' )\n",
    "\n",
    "train_df = pd.read_csv(train_data_file)\n",
    "test_df = pd.read_csv(test_data_file)\n",
    "anomaly_df_c1 = pd.read_csv(anomaly_data_file_c1)\n",
    "anomaly_df_c2 = pd.read_csv(anomaly_data_file_c2)\n",
    "anomaly_df_c3 = pd.read_csv(anomaly_data_file_c3)\n",
    "\n",
    "target_dfs = [ train_df, test_df, anomaly_df_c1, anomaly_df_c2, anomaly_df_c3 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = list(train_df.columns)\n",
    "feature_cols.remove(id_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_col2id = { e[0]:e[1] for e in enumerate(feature_cols,0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'CustomsCode',\n",
       " 1: 'LocationCode',\n",
       " 2: 'PortOfUnladingUNLOCODE',\n",
       " 3: 'ShipmentDestination',\n",
       " 4: 'ShipperPanjivaID',\n",
       " 5: 'TransportMethod',\n",
       " 6: 'hscode_6'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add _<col_id> to all the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the dataframe data to id s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entity_map(train_df, feature_cols, target_dfs):\n",
    "    \n",
    "    feature_col2id = { e[1]:e[0] for e in enumerate(feature_cols,0)}\n",
    "    ref_df = pd.DataFrame(train_df, copy=True)\n",
    "    \n",
    "    # ----- Auxillary functions ------- #\n",
    "    def _append_colId(row, col, feature_col2id):\n",
    "        return str(row[col]) + '_' + str(feature_col2id[col])\n",
    "    \n",
    "    def replace_with_entity_id(row,col,entity2id_map):\n",
    "        v = row[col]\n",
    "        return entity2id_map[v]\n",
    "    # -------------------------------- #\n",
    "    \n",
    "    for col in feature_cols:\n",
    "        ref_df[col] = ref_df.apply(_append_colId,axis=1,args=(col, feature_col2id,))\n",
    "    \n",
    "    # Create a set of all entities\n",
    "    all_entities = []\n",
    "    for col in feature_cols:\n",
    "        _tmp = list(sorted(set(ref_df[col])))\n",
    "        all_entities.extend(_tmp)\n",
    "        pass\n",
    "    entity2id_map = {e[1]:e[0] for e in enumerate(all_entities,0) }\n",
    "    \n",
    "    # ------------------------------------------ #\n",
    "    # Convert the entities in train and test sets\n",
    "    for _df in target_dfs:\n",
    "        for col in feature_cols:\n",
    "            _df[col] = _df.apply(\n",
    "                _append_colId,\n",
    "                axis=1,\n",
    "                args=(col,feature_col2id,)\n",
    "            )\n",
    "             \n",
    "            _df[col] = _df.apply(\n",
    "                replace_with_entity_id,\n",
    "                axis=1,\n",
    "                args=(col,entity2id_map,)\n",
    "            )\n",
    "        \n",
    "    return target_dfs   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = get_entity_map(train_df, feature_cols, target_dfs)\n",
    "train_df = result[0]\n",
    "test_df =  result[1] \n",
    "anomaly_df_c1 =  result[2] \n",
    "anomaly_df_c2 =  result[3] \n",
    "anomaly_df_c3 =  result[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up text files for the matlab input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('np.txt', train_df.values, fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
