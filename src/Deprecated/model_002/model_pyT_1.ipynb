{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True 0 Tesla P100-PCIE-16GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ddatta/anaconda3/envs/AD_v2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:493: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ddatta/anaconda3/envs/AD_v2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:494: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ddatta/anaconda3/envs/AD_v2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:495: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ddatta/anaconda3/envs/AD_v2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:496: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ddatta/anaconda3/envs/AD_v2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:497: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ddatta/anaconda3/envs/AD_v2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:502: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import logging\n",
    "sys.path.append('./..')\n",
    "sys.path.append('./../..')\n",
    "import multiprocessing as mp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from random import shuffle\n",
    "\n",
    "print( torch.cuda.is_available(), torch.cuda.current_device(),torch.cuda.get_device_name(0))\n",
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda:0\"\n",
    "else:\n",
    "    dev = \"cpu\"\n",
    "\n",
    "device = torch.device(dev)\n",
    "\n",
    "\n",
    "try:\n",
    "    from . import get_embeddings\n",
    "    from . import context_vector_model_1 as c2v\n",
    "    from .src.data_fetcher import data_fetcher\n",
    "    from . import utils_1\n",
    "except:\n",
    "    import get_embeddings\n",
    "    import context_vector_model_1 as c2v\n",
    "    from src.data_fetcher import data_fetcher\n",
    "    import utils_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Set up config\n"
     ]
    }
   ],
   "source": [
    "CONFIG_FILE = 'config_1.yaml'\n",
    "DIR = None\n",
    "OP_DIR = None\n",
    "modelData_SaveDir = None\n",
    "DATA_DIR = None\n",
    "num_jobs = None\n",
    "CONFIG = None\n",
    "Refresh_Embeddings = None\n",
    "logger = None\n",
    "domain_dims = None\n",
    "\n",
    "# ------ #\n",
    "\n",
    "def get_domain_dims(dd_file_path):\n",
    "    with open(dd_file_path, 'rb') as fh:\n",
    "        domain_dims = pickle.load(fh)\n",
    "    _tmpDF = pd.DataFrame.from_dict(domain_dims, orient='index')\n",
    "    _tmpDF = _tmpDF.reset_index()\n",
    "    _tmpDF = _tmpDF.rename(columns={'index': 'domain'})\n",
    "    _tmpDF = _tmpDF.sort_values(by=['domain'])\n",
    "    res = {k: v for k, v in zip(_tmpDF['domain'], _tmpDF[0])}\n",
    "    return res\n",
    "\n",
    "\n",
    "def setup_config(_DIR=None):\n",
    "    global CONFIG_FILE\n",
    "    global DATA_DIR\n",
    "    global modelData_SaveDir\n",
    "    global OP_DIR\n",
    "    global DIR\n",
    "    global num_jobs\n",
    "    global Refresh_Embeddings\n",
    "    global logger\n",
    "    global CONFIG\n",
    "    global domain_dims\n",
    "\n",
    "    with open(CONFIG_FILE) as f:\n",
    "        CONFIG = yaml.safe_load(f)\n",
    "    if _DIR is None:\n",
    "        DIR = CONFIG['DIR']\n",
    "    else:\n",
    "        DIR = _DIR\n",
    "\n",
    "    DATA_DIR = os.path.join(CONFIG['DATA_DIR'])\n",
    "    modelData_SaveDir = os.path.join(\n",
    "        CONFIG['model_data_save_dir'],\n",
    "        DIR\n",
    "    )\n",
    "\n",
    "    if not os.path.exists(CONFIG['OP_DIR']):\n",
    "        os.mkdir(CONFIG['OP_DIR'])\n",
    "    OP_DIR = os.path.join(CONFIG['OP_DIR'], DIR)\n",
    "    if not os.path.exists(OP_DIR):\n",
    "        os.mkdir(OP_DIR)\n",
    "\n",
    "    Refresh_Embeddings = CONFIG[DIR]['Refresh_Embeddings']\n",
    "    cpu_count = mp.cpu_count()\n",
    "    num_jobs = min(cpu_count, CONFIG['num_jobs'])\n",
    "\n",
    "    if not os.path.exists(CONFIG['model_data_save_dir']):\n",
    "        os.mkdir(CONFIG['model_data_save_dir'])\n",
    "\n",
    "    if not os.path.exists(modelData_SaveDir):\n",
    "        os.mkdir(modelData_SaveDir)\n",
    "    \n",
    "    domain_dims_file = os.path.join(DATA_DIR, DIR, \"domain_dims.pkl\")\n",
    "    domain_dims = get_domain_dims(domain_dims_file)\n",
    "    print(' Set up config')\n",
    "    return\n",
    "\n",
    "setup_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_data():\n",
    "    global DATA_DIR\n",
    "    global DIR\n",
    "\n",
    "    train_x_pos, train_x_neg, _, _, _, _ = data_fetcher.get_data_MEAD(\n",
    "        DATA_DIR,\n",
    "        DIR,\n",
    "        anomaly_type=1\n",
    "    )\n",
    "    train_x_pos = np.reshape(train_x_pos, [-1, train_x_pos.shape[1], 1])\n",
    "    train_x_neg = np.reshape(train_x_neg, [-1, train_x_neg.shape[1], train_x_neg.shape[2], 1])\n",
    "    return train_x_pos, train_x_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_pred, y_true):\n",
    "    loss = torch.mean(y_pred)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        # this is the place where you instantiate all your modules\n",
    "        # you can later access them using the same names you've given them in\n",
    "        # here\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        \n",
    "    def setup_model(\n",
    "        self, \n",
    "        emb_dim,\n",
    "        domain_dims,\n",
    "        domain_emb_wt,\n",
    "        lstm_dim,\n",
    "        interaction_layer_dim,\n",
    "        intermediate_context_dims,\n",
    "        num_neg_samples,\n",
    "        domain_emb_wt_list = None\n",
    "    ):\n",
    "        \n",
    "        self.num_domains = len(domain_dims)\n",
    "        self.domain_dims = domain_dims\n",
    "        self.lstm_dim = lstm_dim\n",
    "        self.num_neg_samples = num_neg_samples\n",
    "        self.list_Entity_Embed = []\n",
    "        self.interaction_layer_dim = interaction_layer_dim\n",
    "        \n",
    "        for d_idx in range(self.num_domains):\n",
    "            # e = nn.Embedding(num_embeddings= self.domain_dims[d_idx], embedding_dim=emb_dim)\n",
    "            weight = domain_emb_wt[d_idx]\n",
    "            e = nn.Embedding.from_pretrained(weight)\n",
    "            self.list_Entity_Embed.append(e)\n",
    "        \n",
    "        self.BD_LSTM_layer = nn.LSTM(\n",
    "            input_size = emb_dim,\n",
    "            hidden_size = lstm_dim,\n",
    "            num_layers = 1,\n",
    "            batch_first = True,\n",
    "            bidirectional = True\n",
    "        )\n",
    "        \n",
    "        self.FNN_1 = nn.Linear(self.lstm_dim*2, intermediate_context_dims[0])\n",
    "        self.FNN_2 = nn.Linear(intermediate_context_dims[0], self.interaction_layer_dim) \n",
    "        self.FNN_3 = nn.Linear(emb_dim, self.interaction_layer_dim) \n",
    "        return\n",
    "    \n",
    "    \n",
    "    # Define network structure\n",
    "    def forward(self, input_x_pos, input_x_neg=None):\n",
    "        \n",
    "         # ------------- main function ------------------- #\n",
    "        def process(x,_type='pos'):\n",
    "            \n",
    "            split_x = torch.split(\n",
    "                x, \n",
    "                split_size_or_sections = 1, \n",
    "                dim=1\n",
    "            )\n",
    "            # ------------\n",
    "            # emb_op should be [ ?, num_domains, emb_dim]\n",
    "            emb_op = []\n",
    "            for i in range(self.num_domains) :\n",
    "                _emb_layer = self.list_Entity_Embed[i]\n",
    "                _x =  torch.squeeze(split_x[i],dim=-1)\n",
    "                emb_op.append(_emb_layer(_x))\n",
    "            \n",
    "            # ------------\n",
    "            # Calculate mean of the embeddings\n",
    "            stacked = torch.squeeze(torch.stack( emb_op, dim = 1),dim=-2)\n",
    "            _mean = torch.mean(stacked, dim=1, keepdim=True, out=None)\n",
    "            padded_seq = torch.cat([_mean, stacked, _mean], dim=1)\n",
    "                      \n",
    "            lstm_out, lstm_hidden = self.BD_LSTM_layer(padded_seq)\n",
    "            # break up lstm output \n",
    "            \n",
    "            n_timesteps =  self.num_domains + 2\n",
    "            _parts = torch.chunk(lstm_out, 2, dim=-1) \n",
    "            \n",
    "            bdLstm_fwd = torch.chunk(_parts[0], n_timesteps, dim=1)\n",
    "            bdLstm_rev = torch.chunk(_parts[1], n_timesteps, dim=1)       \n",
    "            \n",
    "            # ------------\n",
    "            op_list = []\n",
    "            for i in range(1, n_timesteps - 1):\n",
    "                idx = i-1\n",
    "                left_idx = i-1\n",
    "                right_idx = i+1\n",
    "                _left = bdLstm_fwd[left_idx]\n",
    "                _right = bdLstm_rev[right_idx]\n",
    "                # shape should be [?, 2*lstm_dim]\n",
    "                ctx_concat = torch.squeeze(torch.cat([_left, _right],dim=-1),dim=1)\n",
    "                ctx = self.FNN_1(ctx_concat)\n",
    "                ctx = self.FNN_2(ctx)\n",
    "             \n",
    "                # Now, transform the input embedding\n",
    "                xformed_inp = self.FNN_3(emb_op[idx])\n",
    "                \n",
    "                # Calculate dot product\n",
    "                _dim = self.interaction_layer_dim\n",
    "                dot_product = torch.bmm(\n",
    "                    ctx.view([-1,1,_dim]) ,  xformed_inp.view([-1,_dim,1])\n",
    "                )\n",
    "                _op = torch.squeeze(dot_product,dim=-1)\n",
    "                op_list.append(_op)\n",
    "               \n",
    "                \n",
    "            cur_op = torch.squeeze(torch.stack( op_list, dim=1), dim=-1 )\n",
    "            cur_op = torch.sum( cur_op, dim=-1, keepdim=True)\n",
    "            if _type == 'neg':\n",
    "                cur_op = -cur_op\n",
    "            \n",
    "            cur_op = torch.sigmoid(cur_op)         \n",
    "            return cur_op\n",
    "        \n",
    "        # ------------\n",
    "        \n",
    "        if input_x_neg is None:\n",
    "            return process(input_x_pos)\n",
    "            # Mean so that gradients can be calculated\n",
    "        else:\n",
    "            pos = torch.log(process(input_x_pos))\n",
    "            list_neg_x = torch.chunk(\n",
    "                input_x_neg, \n",
    "                self.num_neg_samples, \n",
    "                dim = 1\n",
    "            )\n",
    "            \n",
    "            list_neg_x = [ torch.squeeze(_,dim=1) for _ in list_neg_x]\n",
    "            ns = [] \n",
    "            \n",
    "            for _neg_x in list_neg_x:    \n",
    "                tmp = torch.log(process(_neg_x,'neg'))\n",
    "                ns.append(tmp)\n",
    "                \n",
    "            ns = torch.stack(ns,dim=1)\n",
    "            neg = torch.mean(ns)\n",
    "            \n",
    "            # -----------------\n",
    "            # This is the objective function \n",
    "            # -----------------\n",
    "            res = pos + neg\n",
    "            # maximize P, means minimize -P\n",
    "            return -res\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entity_embeddings():\n",
    "    global CONFIG\n",
    "    global DIR\n",
    "    global DATA_DIR\n",
    "    global modelData_SaveDir\n",
    "\n",
    "    embedding_dims = CONFIG[DIR]['entity_embedding_dims']\n",
    "    eEmb_num_epochs = CONFIG[DIR]['eEmb_num_epochs']\n",
    "\n",
    "    # ============== ENTITY EMBEDDING ================ #\n",
    "    # -------------------------------------------------\n",
    "    # Check if files exist , if not generate embeddings\n",
    "    # -------------------------------------------------\n",
    "    training_data_file = CONFIG['train_data_file']\n",
    "    files_exist = len(glob.glob(os.path.join(modelData_SaveDir, 'init_embedding**.npy'))) > 0\n",
    "    if not files_exist or Refresh_Embeddings:\n",
    "        src_DIR = os.path.join(DATA_DIR, DIR)\n",
    "        get_embeddings.get_initial_entity_embeddings(\n",
    "            training_data_file,\n",
    "            modelData_SaveDir,\n",
    "            src_DIR,\n",
    "            embedding_dims,\n",
    "            eEmb_num_epochs\n",
    "        )\n",
    "\n",
    "    # ----- Read in domain_embedding weights ------- #\n",
    "\n",
    "    domain_emb_wt = []\n",
    "    file_list = sorted(glob.glob(\n",
    "        os.path.join(modelData_SaveDir, 'init_embedding**{}.npy'.format(embedding_dims))))\n",
    "    for npy_file in file_list:\n",
    "        _tmp_ = np.load(npy_file)\n",
    "        domain_emb_wt.append(_tmp_)\n",
    "\n",
    "    return domain_emb_wt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.net = None\n",
    "        return\n",
    "        \n",
    "        \n",
    "    def set_hyperparams(\n",
    "        self,\n",
    "        emb_dim,\n",
    "        domain_emb_wt,\n",
    "        domain_dims,\n",
    "        lstm_dim,\n",
    "        interaction_layer_dim=32,\n",
    "        intermediate_context_dims=[64],\n",
    "        num_neg_samples = 15,\n",
    "        model_signature = None,\n",
    "        save_dir = None,\n",
    "        num_epochs=5,\n",
    "        batch_size=256\n",
    "    ):\n",
    "        \n",
    "        self.domain_emb_wt = domain_emb_wt\n",
    "        self.emb_dim=emb_dim\n",
    "        self.domain_dims=domain_dims\n",
    "        self.lstm_dim=lstm_dim\n",
    "        self.interaction_layer_dim=interaction_layer_dim\n",
    "        self.intermediate_context_dims = intermediate_context_dims\n",
    "        self.num_neg_samples=num_neg_samples\n",
    "        self.model_signature=model_signature\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.save_dir = save_dir\n",
    "        self.model_signature = model_signature\n",
    "        self.model_file_name = self.model_signature + \".pkl\"\n",
    "        self.model_weights_path = os.path.join(self.save_dir, self.model_file_name)\n",
    "        self.log_interval = 100\n",
    "        \n",
    "    def build_model(self):\n",
    "        \n",
    "        self.net = Net()\n",
    "        \n",
    "        self.net.setup_model( \n",
    "            emb_dim = self.emb_dim,\n",
    "            domain_dims = self.domain_dims,\n",
    "            domain_emb_wt =self.domain_emb_wt,\n",
    "            lstm_dim = self.lstm_dim,\n",
    "            interaction_layer_dim = self.interaction_layer_dim,\n",
    "            intermediate_context_dims = self.intermediate_context_dims,\n",
    "            num_neg_samples = self.num_neg_samples\n",
    "        )\n",
    "        print(self.net)\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.0025)\n",
    "        self.criterion = custom_loss\n",
    "        \n",
    "        # ================== \n",
    "    \n",
    "    def train_model(self, train_x_pos, train_x_neg):\n",
    "        \n",
    "        data_len = train_x_pos.shape[0]\n",
    "        num_batches = data_len // self.batch_size\n",
    "        bs = self.batch_size\n",
    "        log_interval = 100\n",
    "        \n",
    "        for epoch in range(self.num_epochs):\n",
    "    \n",
    "            # Shuffle\n",
    "            ind_list = list(range(train_x_neg.shape[0]))\n",
    "            shuffle(ind_list)\n",
    "            _train_x_pos = train_x_pos[ind_list, :, :]\n",
    "            _train_x_neg = train_x_neg[ind_list, : , : , :]\n",
    "    \n",
    "            for batch_idx in range(num_batches+1):\n",
    "\n",
    "                _x_pos = _train_x_pos[batch_idx*bs:(batch_idx+1)*bs]\n",
    "                _x_neg = _train_x_neg[batch_idx*bs:(batch_idx+1)*bs]\n",
    "\n",
    "                # feed tensor\n",
    "                _x_pos = torch.LongTensor(_x_pos)\n",
    "                _x_neg = torch.LongTensor(_x_neg)\n",
    "\n",
    "                # ----- #\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.net(_x_pos,_x_neg)\n",
    "                loss = self.criterion(output, None)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                # ----- #\n",
    "                if batch_idx % log_interval == 0:\n",
    "                     print('Train ::  Epoch: {}, Batch {}, Loss {:4f}'.format(epoch, batch_idx,loss))\n",
    "                        \n",
    "        # save model\n",
    "        torch.save(self.net.state_dict(), self.model_weights_path)\n",
    "        return\n",
    "\n",
    "    \n",
    "    def load_model(self):\n",
    "            \n",
    "        self.build_model()\n",
    "        self.net.load_state_dict(torch.load(self.model_weights_path))\n",
    "        self.net.eval()\n",
    "        return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ #\n",
    "train_x_pos, train_x_neg = get_training_data()\n",
    "domain_dim_vals= list(domain_dims.values())\n",
    "model_signature = 'model_003'\n",
    "domain_emb_wt = get_entity_embeddings()\n",
    "domain_emb_wt = [torch.FloatTensor(_) for _ in domain_emb_wt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = train_x_pos [:100]\n",
    "# x = torch.LongTensor(x)\n",
    "# x.shape\n",
    "# output = net(x)\n",
    "# loss = custom_loss(output, None)\n",
    "# loss.backward()\n",
    "# x_neg = train_x_neg[:100]\n",
    "# x_neg = torch.LongTensor(x_neg)\n",
    "# x_neg.shape\n",
    "# output = net(x,x_neg)\n",
    "# loss = custom_loss(output, None)\n",
    "# loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (BD_LSTM_layer): LSTM(256, 64, batch_first=True, bidirectional=True)\n",
      "  (FNN_1): Linear(in_features=128, out_features=48, bias=True)\n",
      "  (FNN_2): Linear(in_features=48, out_features=32, bias=True)\n",
      "  (FNN_3): Linear(in_features=256, out_features=32, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_obj = model()\n",
    "model_obj.set_hyperparams(\n",
    "        emb_dim = 256,\n",
    "        domain_dims = domain_dim_vals,\n",
    "        domain_emb_wt = domain_emb_wt,\n",
    "        lstm_dim=64,\n",
    "        interaction_layer_dim=32,\n",
    "        intermediate_context_dims=[48],\n",
    "        num_epochs=10,\n",
    "        batch_size=512,\n",
    "        model_signature=model_signature,\n",
    "        save_dir=modelData_SaveDir\n",
    "    )\n",
    "\n",
    "model_obj.build_model()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ::  Epoch: 0, Batch 0, Loss 1.546731\n",
      "Train ::  Epoch: 0, Batch 100, Loss 1.014935\n",
      "Train ::  Epoch: 0, Batch 200, Loss 0.727491\n",
      "Train ::  Epoch: 1, Batch 0, Loss 0.700407\n",
      "Train ::  Epoch: 1, Batch 100, Loss 0.648676\n",
      "Train ::  Epoch: 1, Batch 200, Loss 0.697568\n",
      "Train ::  Epoch: 2, Batch 0, Loss 0.548770\n",
      "Train ::  Epoch: 2, Batch 100, Loss 0.587405\n",
      "Train ::  Epoch: 2, Batch 200, Loss 0.502573\n",
      "Train ::  Epoch: 3, Batch 0, Loss 0.513768\n",
      "Train ::  Epoch: 3, Batch 100, Loss 0.458863\n",
      "Train ::  Epoch: 3, Batch 200, Loss 0.457643\n",
      "Train ::  Epoch: 4, Batch 0, Loss 0.526480\n",
      "Train ::  Epoch: 4, Batch 100, Loss 0.401883\n",
      "Train ::  Epoch: 4, Batch 200, Loss 0.373648\n",
      "Train ::  Epoch: 5, Batch 0, Loss 0.390318\n",
      "Train ::  Epoch: 5, Batch 100, Loss 0.433326\n",
      "Train ::  Epoch: 5, Batch 200, Loss 0.356539\n",
      "Train ::  Epoch: 6, Batch 0, Loss 0.400473\n",
      "Train ::  Epoch: 6, Batch 100, Loss 0.357641\n",
      "Train ::  Epoch: 6, Batch 200, Loss 0.496884\n",
      "Train ::  Epoch: 7, Batch 0, Loss 0.359514\n",
      "Train ::  Epoch: 7, Batch 100, Loss 0.325254\n",
      "Train ::  Epoch: 7, Batch 200, Loss 0.281122\n",
      "Train ::  Epoch: 8, Batch 0, Loss 0.271679\n",
      "Train ::  Epoch: 8, Batch 100, Loss 0.306229\n",
      "Train ::  Epoch: 8, Batch 200, Loss 0.266908\n",
      "Train ::  Epoch: 9, Batch 0, Loss 0.282642\n",
      "Train ::  Epoch: 9, Batch 100, Loss 0.257636\n",
      "Train ::  Epoch: 9, Batch 200, Loss 0.249423\n"
     ]
    }
   ],
   "source": [
    "model_obj.train_model(train_x_pos, train_x_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_obj.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
