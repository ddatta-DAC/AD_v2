{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True 0 Tesla P100-PCIE-16GB\n",
      "True 0 Tesla P100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from random import shuffle\n",
    "from torch.nn.parameter import Parameter\n",
    "print( torch.cuda.is_available(), torch.cuda.current_device(),torch.cuda.get_device_name(0))\n",
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda:2\"\n",
    "else:\n",
    "    dev = \"cpu\"\n",
    "\n",
    "device = torch.device(dev)\n",
    "print( torch.cuda.is_available(), torch.cuda.current_device(),torch.cuda.get_device_name(0))\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import os\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from collections import OrderedDict\n",
    "import pickle\n",
    "\n",
    "import multiprocessing as mp\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "try:\n",
    "    from . import utils_1\n",
    "except:\n",
    "    import utils_1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Set up config\n"
     ]
    }
   ],
   "source": [
    "CONFIG_FILE = 'config_1.yaml'\n",
    "DIR = None\n",
    "OP_DIR = None\n",
    "modelData_SaveDir = None\n",
    "DATA_DIR = None\n",
    "num_jobs = None\n",
    "CONFIG = None\n",
    "Refresh_Embeddings = None\n",
    "logger = None\n",
    "domain_dims = None\n",
    "train_data_file = None\n",
    "id_col = 'PanjivaRecordID'\n",
    "# ------ #\n",
    "\n",
    "def get_domain_dims(dd_file_path):\n",
    "    with open(dd_file_path, 'rb') as fh:\n",
    "        domain_dims = pickle.load(fh)\n",
    "    _tmpDF = pd.DataFrame.from_dict(domain_dims, orient='index')\n",
    "    _tmpDF = _tmpDF.reset_index()\n",
    "    _tmpDF = _tmpDF.rename(columns={'index': 'domain'})\n",
    "    _tmpDF = _tmpDF.sort_values(by=['domain'])\n",
    "    res = {k: v for k, v in zip(_tmpDF['domain'], _tmpDF[0])}\n",
    "    return res\n",
    "\n",
    "\n",
    "def setup_config(_DIR=None):\n",
    "    global CONFIG_FILE\n",
    "    global DATA_DIR\n",
    "    global modelData_SaveDir\n",
    "    global OP_DIR\n",
    "    global DIR\n",
    "    global num_jobs\n",
    "    global Refresh_Embeddings\n",
    "    global logger\n",
    "    global CONFIG\n",
    "    global domain_dims\n",
    "    global train_data_file\n",
    "    \n",
    "    with open(CONFIG_FILE) as f:\n",
    "        CONFIG = yaml.safe_load(f)\n",
    "    if _DIR is None:\n",
    "        DIR = CONFIG['DIR']\n",
    "    else:\n",
    "        DIR = _DIR\n",
    "\n",
    "    DATA_DIR = os.path.join(CONFIG['DATA_DIR'])\n",
    "    modelData_SaveDir = os.path.join(\n",
    "        CONFIG['model_data_save_dir'],\n",
    "        DIR\n",
    "    )\n",
    "    train_data_file = CONFIG['train_data_file']\n",
    "    \n",
    "    if not os.path.exists(CONFIG['OP_DIR']):\n",
    "        os.mkdir(CONFIG['OP_DIR'])\n",
    "    OP_DIR = os.path.join(CONFIG['OP_DIR'], DIR)\n",
    "    if not os.path.exists(OP_DIR):\n",
    "        os.mkdir(OP_DIR)\n",
    "\n",
    "    Refresh_Embeddings = CONFIG[DIR]['Refresh_Embeddings']\n",
    "    cpu_count = mp.cpu_count()\n",
    "    num_jobs = min(cpu_count, CONFIG['num_jobs'])\n",
    "\n",
    "    if not os.path.exists(CONFIG['model_data_save_dir']):\n",
    "        os.mkdir(CONFIG['model_data_save_dir'])\n",
    "\n",
    "    if not os.path.exists(modelData_SaveDir):\n",
    "        os.mkdir(modelData_SaveDir)\n",
    "    \n",
    "    domain_dims_file = os.path.join(DATA_DIR, DIR, \"domain_dims.pkl\")\n",
    "    domain_dims = get_domain_dims(domain_dims_file)\n",
    "    print(' Set up config')\n",
    "    return\n",
    "\n",
    "setup_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_coocc_matrix(df, col_1, col_2):\n",
    "    set_elements_1 = set(list(df[col_1]))\n",
    "    set_elements_2 = set(list(df[col_2]))\n",
    "    count_1 = len(set_elements_1)\n",
    "    count_2 = len(set_elements_2)\n",
    "    coocc = np.zeros([count_1, count_2])\n",
    "    df = df[[col_1, col_2]]\n",
    "    new_df = df.groupby([col_1, col_2]).size().reset_index(name='count')\n",
    "\n",
    "    for _, row in new_df.iterrows():\n",
    "        i = row[col_1]\n",
    "        j = row[col_2]\n",
    "        coocc[i][j] = row['count']\n",
    "\n",
    "    print('Col 1 & 2', col_1, col_2, coocc.shape, '>>', (count_1, count_2))\n",
    "    return coocc\n",
    "\n",
    "\n",
    "\n",
    "def get_coOccMatrix_dict(df, id_col):\n",
    "    columns = list(df.columns)\n",
    "    columns.remove(id_col)\n",
    "    columns = list(sorted(columns))\n",
    "    columnWise_coOccMatrix_dict = {}\n",
    "\n",
    "    for i in range(len(columns)):\n",
    "        for j in range(i + 1, len(columns)):\n",
    "            col_1 = columns[i]\n",
    "            col_2 = columns[j]\n",
    "            key = col_1 + '_+_' + col_2\n",
    "            res = create_coocc_matrix(df, col_1, col_2)\n",
    "            columnWise_coOccMatrix_dict[key] = res\n",
    "    columnWise_coOccMatrix_dict = OrderedDict(columnWise_coOccMatrix_dict)\n",
    "    return columnWise_coOccMatrix_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Carrier', 'ConsigneePanjivaID', 'HSCode', 'PortOfLading', 'PortOfUnlading', 'ShipmentDestination', 'ShipmentOrigin', 'ShipperPanjivaID']\n"
     ]
    }
   ],
   "source": [
    "src_DIR = os.path.join(DATA_DIR, DIR)\n",
    "training_data_file = CONFIG['train_data_file']\n",
    "train_df = pd.read_csv(os.path.join(src_DIR, train_data_file))\n",
    "feature_cols = list(train_df.columns)\n",
    "feature_cols = list(feature_cols)\n",
    "feature_cols.remove(id_col)\n",
    "domains = sorted(feature_cols)\n",
    "print(feature_cols)\n",
    "\n",
    "model_data_save_dir = modelData_SaveDir\n",
    "\n",
    "data = train_df[feature_cols].values\n",
    "# ------------------------------- #\n",
    "coOcc_dict_file = os.path.join(model_data_save_dir, \"coOccMatrix_dict.pkl\")\n",
    "X_ij_file = os.path.join(model_data_save_dir, \"X_ij.npy\")\n",
    "domain_dims_file = os.path.join(src_DIR, \"domain_dims.pkl\")\n",
    "domain_dims = get_domain_dims(domain_dims_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Carrier': 548,\n",
       " 'ConsigneePanjivaID': 5113,\n",
       " 'HSCode': 95,\n",
       " 'PortOfLading': 238,\n",
       " 'PortOfUnlading': 64,\n",
       " 'ShipmentDestination': 113,\n",
       " 'ShipmentOrigin': 116,\n",
       " 'ShipperPanjivaID': 6193}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domain_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----\n",
    "# Check if pairwise co-occurrence dictionary exists\n",
    "# -----\n",
    "if os.path.exists(coOcc_dict_file):\n",
    "    with open(coOcc_dict_file, 'rb') as fh:\n",
    "        coOccMatrix_dict = pickle.load(fh)\n",
    "else:\n",
    "    coOccMatrix_dict = get_coOccMatrix_dict(train_df, id_col='PanjivaRecordID')\n",
    "    with open(coOcc_dict_file, \"wb\") as fh:\n",
    "        pickle.dump(coOccMatrix_dict, fh, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "# ----------------\n",
    "# Ensure X_ij \n",
    "# ----------------\n",
    "if os.path.exists(X_ij_file):\n",
    "    with open(X_ij_file, 'rb') as fh:\n",
    "        X_ij = np.load(fh)\n",
    "\n",
    "else:\n",
    "   \n",
    "    nd = len(feature_cols)\n",
    "    X_ij = np.zeros([data.shape[0], nd, nd])\n",
    "    print( X_ij.shape )\n",
    "\n",
    "    for i in range(nd):\n",
    "        for j in range(nd):\n",
    "            if i == j :\n",
    "                for d in range(data.shape[0]):\n",
    "                    X_ij[d][i][j] = 0\n",
    "            else:\n",
    "                if i < j: \n",
    "                    _i =i\n",
    "                    _j =j\n",
    "                else : \n",
    "                    _i =j\n",
    "                    _j =i\n",
    "                key = feature_cols[_i] + '_+_' + feature_cols[_j]\n",
    "                \n",
    "                for d in range(data.shape[0]):\n",
    "                    e1 = data[d][_i]\n",
    "                    e2 = data[d][_j]\n",
    "                    X_ij[d][i][j] = coOccMatrix_dict[key][e1][e2]\n",
    "                    \n",
    "    X_ij = np.asarray(X_ij,np.int32)\n",
    "    with open(X_ij_file, \"wb\") as fh:\n",
    "        np.save(fh, X_ij)\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "nd = len(feature_cols)\n",
    "X_ij_max = np.zeros([nd,nd])\n",
    "for i in range(nd):\n",
    "    for j in range(nd):\n",
    "        if i==j : continue\n",
    "        if i < j: \n",
    "            _i =i\n",
    "            _j =j\n",
    "        else : \n",
    "            _i =j\n",
    "            _j =i\n",
    "        key = feature_cols[_i] + '_+_' + feature_cols[_j]\n",
    "        X_ij_max[i][j] = np.max(coOccMatrix_dict[key])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ij_max = X_ij_max+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =================================\n",
    "# Co-occurrence based embedding model\n",
    "# Projecting GloVe to multivariate categorical \n",
    "# =================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate doamin wise MI\n",
    "import math \n",
    "\n",
    "def MI(dff, x, y):\n",
    "    \n",
    "    df = dff.copy()\n",
    "    f_x = 'f_x'\n",
    "    f_y = 'f_y'\n",
    "    f_xy = \"f_xy\"\n",
    "    df[f_x] = df.groupby(x)[x].transform('count')/len(df)\n",
    "    df[f_y] = df.groupby(y)[y].transform('count')/len(df)\n",
    "    \n",
    "    l = len(df)\n",
    "    df_1 = df.groupby([x,y]).size().reset_index(name=f_xy)\n",
    "    df_1[f_xy] = df_1[f_xy]/l\n",
    "    \n",
    "    df_1 = df_1.sort_values(by=[x,y])\n",
    "    \n",
    "    df_2 = df.drop_duplicates([x,y])\n",
    "    df_2 = df_2.sort_values(by=[x,y])\n",
    "    df_2 = df_2[[x,y,f_x,f_y]]\n",
    "    \n",
    "    df_3 = df_1.merge(df_2, how='inner', on =[x,y])\n",
    "    def calc(row):\n",
    "        return row[f_xy]*np.log(row[f_xy]/(row[f_x]*row[f_y]))\n",
    "    \n",
    "    df_3['m'] = df_3.apply(calc,axis=1)\n",
    "    return np.sum(df_3['m'])\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_domains = len(domain_dims)\n",
    "MI_domain_ij = np.ones([num_domains,num_domains])\n",
    "\n",
    "\n",
    "domains_list = list(domain_dims.keys())\n",
    "for i in range(num_domains):\n",
    "    for j in range(i+1,num_domains):\n",
    "        MI_domain_ij[i][j] = MI(train_df, domains_list[i], domains_list[j])\n",
    "        MI_domain_ij[j][i] = MI_domain_ij[i][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / (e_x.sum()/2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "MI_domain_ij_scaled = scaler.fit_transform(MI_domain_ij_scaled) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== \n",
    "# Custom Loss function \n",
    "# ======================\n",
    "\n",
    "# y : shape [ ?, d, d]\n",
    "def custom_loss(y_pred, y_true):\n",
    "    \n",
    "    # X_ij shape should be [ d,d ]\n",
    "    global X_ij_max\n",
    "    global MI_domain_ij_scaled\n",
    "    _MI = torch.FloatTensor(MI_domain_ij_scaled)\n",
    "    _X_ij_max = torch.FloatTensor(X_ij_max)\n",
    "    a = 0.9\n",
    "    epsilon = 0.000001\n",
    "\n",
    "    e1 = torch.pow(y_pred - torch.log(y_true + epsilon) , 2)\n",
    "    \n",
    "    _xij_m = _X_ij_max.repeat(y_pred.size()[0], 1, 1)\n",
    "    z = y_true / _xij_m \n",
    "    \n",
    "    s1 = torch.clamp(torch.pow(z,a), 0.05, 1.0)\n",
    "    \n",
    "    loss =  s1 * _MI * e1  \n",
    "    sample_loss = torch.sum(loss,keepdim = False, dim=-1)\n",
    "    sample_loss = torch.sum(sample_loss,keepdim = False, dim=-1)\n",
    "    sample_loss = sample_loss/2\n",
    "    return torch.mean(\n",
    "        sample_loss\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        emb_dim,\n",
    "        domain_dims\n",
    "    ):\n",
    "        super(Net, self).__init__()\n",
    "        self.num_domains = len(domain_dims)\n",
    "        self.domain_dims = domain_dims\n",
    "        self.emb_dim = emb_dim\n",
    "        \n",
    "        self.list_W_m = []\n",
    "        self.list_W_c = []\n",
    "        self.list_B_m = []\n",
    "        self.list_B_c = []\n",
    "        \n",
    "        \n",
    "        for d_idx in range(self.num_domains):\n",
    "            e = nn.Embedding(num_embeddings= domain_dims[d_idx], embedding_dim=emb_dim)\n",
    "            e.weight = Parameter(torch.Tensor(torch.empty(self.domain_dims[d_idx], emb_dim).uniform_(-1, 1)))\n",
    "            self.register_parameter('e_'+str(d_idx), e.weight)\n",
    "            self.list_W_m.append(e)\n",
    "            \n",
    "            e1 = nn.Embedding(num_embeddings= domain_dims[d_idx], embedding_dim=emb_dim)\n",
    "            e1.weight = Parameter(torch.Tensor(torch.empty(self.domain_dims[d_idx], emb_dim).uniform_(-1, 1)))\n",
    "            self.register_parameter('e1_'+str(d_idx), e1.weight)\n",
    "            self.list_W_c.append(e1)\n",
    "            \n",
    "            b = nn.Embedding(num_embeddings= domain_dims[d_idx], embedding_dim=1)\n",
    "            b.weight = Parameter(torch.Tensor(torch.empty(domain_dims[d_idx], 1).uniform_(-1, 1)))\n",
    "            self.register_parameter('b_'+str(d_idx), b.weight)\n",
    "            self.list_B_m.append(b)\n",
    "            \n",
    "            b1 = nn.Embedding(num_embeddings=domain_dims[d_idx], embedding_dim=1)\n",
    "            b1._weight = Parameter(torch.Tensor(torch.empty(domain_dims[d_idx], 1).uniform_(-1, 1)))\n",
    "            self.register_parameter('b1_'+str(d_idx), b1.weight)\n",
    "            self.list_B_c.append(b1) \n",
    "            \n",
    "    \n",
    "    # --------------------------------------\n",
    "    # Define network structure\n",
    "    # x : [? , dims]\n",
    "    # --------------------------------------\n",
    "    def forward(self, x):\n",
    "        split_x = torch.chunk(\n",
    "            x, \n",
    "            chunks = self.num_domains, \n",
    "            dim = 1\n",
    "        )\n",
    "        \n",
    "        nd = self.num_domains\n",
    "        res = []\n",
    "        for m_idx in range(nd):\n",
    "            _zero = split_x[m_idx]*0\n",
    "            _zero = _zero.type(torch.FloatTensor).view([-1,1,1])\n",
    "            \n",
    "            \n",
    "            w_i = self.list_W_m[m_idx](split_x[m_idx])\n",
    "            b_i = self.list_B_m[m_idx](split_x[m_idx])\n",
    "            \n",
    "            for c_idx in range(nd):\n",
    "                if m_idx == c_idx : \n",
    "                    res.append(_zero)\n",
    "                else:\n",
    "                    w_j = self.list_W_c[c_idx](split_x[c_idx])\n",
    "                    b_j = self.list_B_c[c_idx](split_x[c_idx])\n",
    "\n",
    "                    s = torch.bmm(\n",
    "                        w_i.view(-1, 1, self.emb_dim), \n",
    "                        w_j.view(-1, self.emb_dim, 1)\n",
    "                    ) \n",
    "                    s = s + b_i + b_j\n",
    "                    \n",
    "                    res.append(s)\n",
    "\n",
    "        # Reshape from  list [ [?,1] ...[?,d*d] ] to  [?, d ,d]\n",
    "      \n",
    "        res = torch.stack(\n",
    "            res,\n",
    "            dim=1\n",
    "        )\n",
    "        res = torch.squeeze(res,dim=-1)\n",
    "        res = torch.squeeze(res,dim=-1)\n",
    "        \n",
    "        res = res.view([-1, nd, nd])\n",
    "        return res\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 64\n",
    "domain_dims_vals = list( domain_dims.values() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(emb_dim,domain_dims_vals)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.005)\n",
    "criterion = custom_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(net.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_df[feature_cols].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140382, 8, 8)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ij.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "bs = 128\n",
    "log_interval = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = train_x.shape[0]//bs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ::  Epoch: 0, Batch 0, Loss 490.315155\n",
      "Train ::  Epoch: 0, Batch 100, Loss 39.855339\n",
      "Train ::  Epoch: 0, Batch 200, Loss 9.579609\n",
      "Train ::  Epoch: 0, Batch 300, Loss 4.738073\n",
      "Train ::  Epoch: 0, Batch 400, Loss 3.205403\n",
      "Train ::  Epoch: 0, Batch 500, Loss 2.321314\n",
      "Train ::  Epoch: 0, Batch 600, Loss 1.873317\n",
      "Train ::  Epoch: 0, Batch 700, Loss 1.536324\n",
      "Train ::  Epoch: 0, Batch 800, Loss 1.169533\n",
      "Train ::  Epoch: 0, Batch 900, Loss 1.079208\n",
      "Train ::  Epoch: 0, Batch 1000, Loss 0.741467\n",
      "Train ::  Epoch: 1, Batch 0, Loss 0.664297\n",
      "Train ::  Epoch: 1, Batch 100, Loss 0.590185\n",
      "Train ::  Epoch: 1, Batch 200, Loss 0.578701\n",
      "Train ::  Epoch: 1, Batch 300, Loss 0.490084\n",
      "Train ::  Epoch: 1, Batch 400, Loss 0.581943\n",
      "Train ::  Epoch: 1, Batch 500, Loss 0.537322\n",
      "Train ::  Epoch: 1, Batch 600, Loss 0.427503\n",
      "Train ::  Epoch: 1, Batch 700, Loss 0.402560\n",
      "Train ::  Epoch: 1, Batch 800, Loss 0.425102\n",
      "Train ::  Epoch: 1, Batch 900, Loss 0.346960\n",
      "Train ::  Epoch: 1, Batch 1000, Loss 0.294175\n",
      "Train ::  Epoch: 2, Batch 0, Loss 0.260795\n",
      "Train ::  Epoch: 2, Batch 100, Loss 0.264238\n",
      "Train ::  Epoch: 2, Batch 200, Loss 0.260098\n",
      "Train ::  Epoch: 2, Batch 300, Loss 0.219364\n",
      "Train ::  Epoch: 2, Batch 400, Loss 0.272885\n",
      "Train ::  Epoch: 2, Batch 500, Loss 0.188982\n",
      "Train ::  Epoch: 2, Batch 600, Loss 0.247764\n",
      "Train ::  Epoch: 2, Batch 700, Loss 0.268779\n",
      "Train ::  Epoch: 2, Batch 800, Loss 0.176766\n",
      "Train ::  Epoch: 2, Batch 900, Loss 0.173783\n",
      "Train ::  Epoch: 2, Batch 1000, Loss 0.196034\n",
      "Train ::  Epoch: 3, Batch 0, Loss 0.194389\n",
      "Train ::  Epoch: 3, Batch 100, Loss 0.191197\n",
      "Train ::  Epoch: 3, Batch 200, Loss 0.219712\n",
      "Train ::  Epoch: 3, Batch 300, Loss 0.190533\n",
      "Train ::  Epoch: 3, Batch 400, Loss 0.182446\n",
      "Train ::  Epoch: 3, Batch 500, Loss 0.195870\n",
      "Train ::  Epoch: 3, Batch 600, Loss 0.166024\n",
      "Train ::  Epoch: 3, Batch 700, Loss 0.145616\n",
      "Train ::  Epoch: 3, Batch 800, Loss 0.162696\n",
      "Train ::  Epoch: 3, Batch 900, Loss 0.155498\n",
      "Train ::  Epoch: 3, Batch 1000, Loss 0.181750\n",
      "Train ::  Epoch: 4, Batch 0, Loss 0.154570\n",
      "Train ::  Epoch: 4, Batch 100, Loss 0.143520\n",
      "Train ::  Epoch: 4, Batch 200, Loss 0.166705\n",
      "Train ::  Epoch: 4, Batch 300, Loss 0.169735\n",
      "Train ::  Epoch: 4, Batch 400, Loss 0.159272\n",
      "Train ::  Epoch: 4, Batch 500, Loss 0.168159\n",
      "Train ::  Epoch: 4, Batch 600, Loss 0.196311\n",
      "Train ::  Epoch: 4, Batch 700, Loss 0.196726\n",
      "Train ::  Epoch: 4, Batch 800, Loss 0.143006\n",
      "Train ::  Epoch: 4, Batch 900, Loss 0.169415\n",
      "Train ::  Epoch: 4, Batch 1000, Loss 0.165082\n",
      "Train ::  Epoch: 5, Batch 0, Loss 0.163278\n",
      "Train ::  Epoch: 5, Batch 100, Loss 0.150971\n",
      "Train ::  Epoch: 5, Batch 200, Loss 0.109928\n",
      "Train ::  Epoch: 5, Batch 300, Loss 0.114383\n",
      "Train ::  Epoch: 5, Batch 400, Loss 0.126042\n",
      "Train ::  Epoch: 5, Batch 500, Loss 0.155765\n",
      "Train ::  Epoch: 5, Batch 600, Loss 0.142287\n",
      "Train ::  Epoch: 5, Batch 700, Loss 0.148266\n",
      "Train ::  Epoch: 5, Batch 800, Loss 0.149357\n",
      "Train ::  Epoch: 5, Batch 900, Loss 0.161339\n",
      "Train ::  Epoch: 5, Batch 1000, Loss 0.150758\n",
      "Train ::  Epoch: 6, Batch 0, Loss 0.109221\n",
      "Train ::  Epoch: 6, Batch 100, Loss 0.121555\n",
      "Train ::  Epoch: 6, Batch 200, Loss 0.126314\n",
      "Train ::  Epoch: 6, Batch 300, Loss 0.127088\n",
      "Train ::  Epoch: 6, Batch 400, Loss 0.130905\n",
      "Train ::  Epoch: 6, Batch 500, Loss 0.109436\n",
      "Train ::  Epoch: 6, Batch 600, Loss 0.126005\n",
      "Train ::  Epoch: 6, Batch 700, Loss 0.128649\n",
      "Train ::  Epoch: 6, Batch 800, Loss 0.103226\n",
      "Train ::  Epoch: 6, Batch 900, Loss 0.127992\n",
      "Train ::  Epoch: 6, Batch 1000, Loss 0.128846\n",
      "Train ::  Epoch: 7, Batch 0, Loss 0.097633\n",
      "Train ::  Epoch: 7, Batch 100, Loss 0.124447\n",
      "Train ::  Epoch: 7, Batch 200, Loss 0.106111\n",
      "Train ::  Epoch: 7, Batch 300, Loss 0.095056\n",
      "Train ::  Epoch: 7, Batch 400, Loss 0.079156\n",
      "Train ::  Epoch: 7, Batch 500, Loss 0.104373\n",
      "Train ::  Epoch: 7, Batch 600, Loss 0.109614\n",
      "Train ::  Epoch: 7, Batch 700, Loss 0.121853\n",
      "Train ::  Epoch: 7, Batch 800, Loss 0.143215\n",
      "Train ::  Epoch: 7, Batch 900, Loss 0.137836\n",
      "Train ::  Epoch: 7, Batch 1000, Loss 0.099375\n",
      "Train ::  Epoch: 8, Batch 0, Loss 0.126087\n",
      "Train ::  Epoch: 8, Batch 100, Loss 0.104287\n",
      "Train ::  Epoch: 8, Batch 200, Loss 0.102289\n",
      "Train ::  Epoch: 8, Batch 300, Loss 0.103133\n",
      "Train ::  Epoch: 8, Batch 400, Loss 0.096897\n",
      "Train ::  Epoch: 8, Batch 500, Loss 0.089566\n",
      "Train ::  Epoch: 8, Batch 600, Loss 0.117735\n",
      "Train ::  Epoch: 8, Batch 700, Loss 0.087430\n",
      "Train ::  Epoch: 8, Batch 800, Loss 0.119454\n",
      "Train ::  Epoch: 8, Batch 900, Loss 0.120181\n",
      "Train ::  Epoch: 8, Batch 1000, Loss 0.101177\n",
      "Train ::  Epoch: 9, Batch 0, Loss 0.077578\n",
      "Train ::  Epoch: 9, Batch 100, Loss 0.102149\n",
      "Train ::  Epoch: 9, Batch 200, Loss 0.114720\n",
      "Train ::  Epoch: 9, Batch 300, Loss 0.104292\n",
      "Train ::  Epoch: 9, Batch 400, Loss 0.089361\n",
      "Train ::  Epoch: 9, Batch 500, Loss 0.107467\n",
      "Train ::  Epoch: 9, Batch 600, Loss 0.078952\n",
      "Train ::  Epoch: 9, Batch 700, Loss 0.090239\n",
      "Train ::  Epoch: 9, Batch 800, Loss 0.088252\n",
      "Train ::  Epoch: 9, Batch 900, Loss 0.080410\n",
      "Train ::  Epoch: 9, Batch 1000, Loss 0.109811\n",
      "Train ::  Epoch: 10, Batch 0, Loss 0.079119\n",
      "Train ::  Epoch: 10, Batch 100, Loss 0.087376\n",
      "Train ::  Epoch: 10, Batch 200, Loss 0.076002\n",
      "Train ::  Epoch: 10, Batch 300, Loss 0.084566\n",
      "Train ::  Epoch: 10, Batch 400, Loss 0.070458\n",
      "Train ::  Epoch: 10, Batch 500, Loss 0.087762\n",
      "Train ::  Epoch: 10, Batch 600, Loss 0.081288\n",
      "Train ::  Epoch: 10, Batch 700, Loss 0.089601\n",
      "Train ::  Epoch: 10, Batch 800, Loss 0.075185\n",
      "Train ::  Epoch: 10, Batch 900, Loss 0.089547\n",
      "Train ::  Epoch: 10, Batch 1000, Loss 0.083357\n",
      "Train ::  Epoch: 11, Batch 0, Loss 0.082429\n",
      "Train ::  Epoch: 11, Batch 100, Loss 0.108226\n",
      "Train ::  Epoch: 11, Batch 200, Loss 0.079729\n",
      "Train ::  Epoch: 11, Batch 300, Loss 0.067988\n",
      "Train ::  Epoch: 11, Batch 400, Loss 0.066261\n",
      "Train ::  Epoch: 11, Batch 500, Loss 0.085127\n",
      "Train ::  Epoch: 11, Batch 600, Loss 0.100162\n",
      "Train ::  Epoch: 11, Batch 700, Loss 0.093337\n",
      "Train ::  Epoch: 11, Batch 800, Loss 0.088025\n",
      "Train ::  Epoch: 11, Batch 900, Loss 0.078194\n",
      "Train ::  Epoch: 11, Batch 1000, Loss 0.090906\n",
      "Train ::  Epoch: 12, Batch 0, Loss 0.072504\n",
      "Train ::  Epoch: 12, Batch 100, Loss 0.086807\n",
      "Train ::  Epoch: 12, Batch 200, Loss 0.080721\n",
      "Train ::  Epoch: 12, Batch 300, Loss 0.066690\n",
      "Train ::  Epoch: 12, Batch 400, Loss 0.076900\n",
      "Train ::  Epoch: 12, Batch 500, Loss 0.071302\n",
      "Train ::  Epoch: 12, Batch 600, Loss 0.077664\n",
      "Train ::  Epoch: 12, Batch 700, Loss 0.066760\n",
      "Train ::  Epoch: 12, Batch 800, Loss 0.081547\n",
      "Train ::  Epoch: 12, Batch 900, Loss 0.072598\n",
      "Train ::  Epoch: 12, Batch 1000, Loss 0.095615\n",
      "Train ::  Epoch: 13, Batch 0, Loss 0.067323\n",
      "Train ::  Epoch: 13, Batch 100, Loss 0.074083\n",
      "Train ::  Epoch: 13, Batch 200, Loss 0.054694\n",
      "Train ::  Epoch: 13, Batch 300, Loss 0.065034\n",
      "Train ::  Epoch: 13, Batch 400, Loss 0.065507\n",
      "Train ::  Epoch: 13, Batch 500, Loss 0.071611\n",
      "Train ::  Epoch: 13, Batch 600, Loss 0.072595\n",
      "Train ::  Epoch: 13, Batch 700, Loss 0.070636\n",
      "Train ::  Epoch: 13, Batch 800, Loss 0.079159\n",
      "Train ::  Epoch: 13, Batch 900, Loss 0.068363\n",
      "Train ::  Epoch: 13, Batch 1000, Loss 0.083134\n",
      "Train ::  Epoch: 14, Batch 0, Loss 0.070466\n",
      "Train ::  Epoch: 14, Batch 100, Loss 0.057883\n",
      "Train ::  Epoch: 14, Batch 200, Loss 0.074526\n",
      "Train ::  Epoch: 14, Batch 300, Loss 0.069769\n",
      "Train ::  Epoch: 14, Batch 400, Loss 0.076522\n",
      "Train ::  Epoch: 14, Batch 500, Loss 0.070693\n",
      "Train ::  Epoch: 14, Batch 600, Loss 0.079780\n",
      "Train ::  Epoch: 14, Batch 700, Loss 0.053040\n",
      "Train ::  Epoch: 14, Batch 800, Loss 0.089474\n",
      "Train ::  Epoch: 14, Batch 900, Loss 0.075762\n",
      "Train ::  Epoch: 14, Batch 1000, Loss 0.090329\n",
      "Train ::  Epoch: 15, Batch 0, Loss 0.059200\n",
      "Train ::  Epoch: 15, Batch 100, Loss 0.073928\n",
      "Train ::  Epoch: 15, Batch 200, Loss 0.073185\n",
      "Train ::  Epoch: 15, Batch 300, Loss 0.063047\n",
      "Train ::  Epoch: 15, Batch 400, Loss 0.060432\n",
      "Train ::  Epoch: 15, Batch 500, Loss 0.064013\n",
      "Train ::  Epoch: 15, Batch 600, Loss 0.069858\n",
      "Train ::  Epoch: 15, Batch 700, Loss 0.084180\n",
      "Train ::  Epoch: 15, Batch 800, Loss 0.057117\n",
      "Train ::  Epoch: 15, Batch 900, Loss 0.074767\n",
      "Train ::  Epoch: 15, Batch 1000, Loss 0.073056\n",
      "Train ::  Epoch: 16, Batch 0, Loss 0.074553\n",
      "Train ::  Epoch: 16, Batch 100, Loss 0.060989\n",
      "Train ::  Epoch: 16, Batch 200, Loss 0.072047\n",
      "Train ::  Epoch: 16, Batch 300, Loss 0.062917\n",
      "Train ::  Epoch: 16, Batch 400, Loss 0.074490\n",
      "Train ::  Epoch: 16, Batch 500, Loss 0.062779\n",
      "Train ::  Epoch: 16, Batch 600, Loss 0.049905\n",
      "Train ::  Epoch: 16, Batch 700, Loss 0.058240\n",
      "Train ::  Epoch: 16, Batch 800, Loss 0.073393\n",
      "Train ::  Epoch: 16, Batch 900, Loss 0.060642\n",
      "Train ::  Epoch: 16, Batch 1000, Loss 0.071400\n",
      "Train ::  Epoch: 17, Batch 0, Loss 0.062923\n",
      "Train ::  Epoch: 17, Batch 100, Loss 0.087705\n",
      "Train ::  Epoch: 17, Batch 200, Loss 0.059250\n",
      "Train ::  Epoch: 17, Batch 300, Loss 0.049031\n",
      "Train ::  Epoch: 17, Batch 400, Loss 0.049464\n",
      "Train ::  Epoch: 17, Batch 500, Loss 0.057932\n",
      "Train ::  Epoch: 17, Batch 600, Loss 0.075181\n",
      "Train ::  Epoch: 17, Batch 700, Loss 0.061393\n",
      "Train ::  Epoch: 17, Batch 800, Loss 0.065107\n",
      "Train ::  Epoch: 17, Batch 900, Loss 0.075517\n",
      "Train ::  Epoch: 17, Batch 1000, Loss 0.058781\n",
      "Train ::  Epoch: 18, Batch 0, Loss 0.064052\n",
      "Train ::  Epoch: 18, Batch 100, Loss 0.059733\n",
      "Train ::  Epoch: 18, Batch 200, Loss 0.055889\n",
      "Train ::  Epoch: 18, Batch 300, Loss 0.053794\n",
      "Train ::  Epoch: 18, Batch 400, Loss 0.051808\n",
      "Train ::  Epoch: 18, Batch 500, Loss 0.077939\n",
      "Train ::  Epoch: 18, Batch 600, Loss 0.076702\n",
      "Train ::  Epoch: 18, Batch 700, Loss 0.061571\n",
      "Train ::  Epoch: 18, Batch 800, Loss 0.065591\n",
      "Train ::  Epoch: 18, Batch 900, Loss 0.063947\n",
      "Train ::  Epoch: 18, Batch 1000, Loss 0.044902\n",
      "Train ::  Epoch: 19, Batch 0, Loss 0.061246\n",
      "Train ::  Epoch: 19, Batch 100, Loss 0.058280\n",
      "Train ::  Epoch: 19, Batch 200, Loss 0.045750\n",
      "Train ::  Epoch: 19, Batch 300, Loss 0.064630\n",
      "Train ::  Epoch: 19, Batch 400, Loss 0.056250\n",
      "Train ::  Epoch: 19, Batch 500, Loss 0.064956\n",
      "Train ::  Epoch: 19, Batch 600, Loss 0.064001\n",
      "Train ::  Epoch: 19, Batch 700, Loss 0.061428\n",
      "Train ::  Epoch: 19, Batch 800, Loss 0.056704\n",
      "Train ::  Epoch: 19, Batch 900, Loss 0.057924\n",
      "Train ::  Epoch: 19, Batch 1000, Loss 0.061852\n",
      "Train ::  Epoch: 20, Batch 0, Loss 0.059387\n",
      "Train ::  Epoch: 20, Batch 100, Loss 0.055273\n",
      "Train ::  Epoch: 20, Batch 200, Loss 0.046348\n",
      "Train ::  Epoch: 20, Batch 300, Loss 0.064157\n",
      "Train ::  Epoch: 20, Batch 400, Loss 0.078927\n",
      "Train ::  Epoch: 20, Batch 500, Loss 0.045514\n",
      "Train ::  Epoch: 20, Batch 600, Loss 0.049283\n",
      "Train ::  Epoch: 20, Batch 700, Loss 0.054887\n",
      "Train ::  Epoch: 20, Batch 800, Loss 0.061144\n",
      "Train ::  Epoch: 20, Batch 900, Loss 0.049228\n",
      "Train ::  Epoch: 20, Batch 1000, Loss 0.048082\n",
      "Train ::  Epoch: 21, Batch 0, Loss 0.063929\n",
      "Train ::  Epoch: 21, Batch 100, Loss 0.064924\n",
      "Train ::  Epoch: 21, Batch 200, Loss 0.047078\n",
      "Train ::  Epoch: 21, Batch 300, Loss 0.044106\n",
      "Train ::  Epoch: 21, Batch 400, Loss 0.057148\n",
      "Train ::  Epoch: 21, Batch 500, Loss 0.048541\n",
      "Train ::  Epoch: 21, Batch 600, Loss 0.068181\n",
      "Train ::  Epoch: 21, Batch 700, Loss 0.048242\n",
      "Train ::  Epoch: 21, Batch 800, Loss 0.044252\n",
      "Train ::  Epoch: 21, Batch 900, Loss 0.039011\n",
      "Train ::  Epoch: 21, Batch 1000, Loss 0.056172\n",
      "Train ::  Epoch: 22, Batch 0, Loss 0.050466\n",
      "Train ::  Epoch: 22, Batch 100, Loss 0.042794\n",
      "Train ::  Epoch: 22, Batch 200, Loss 0.051462\n",
      "Train ::  Epoch: 22, Batch 300, Loss 0.044535\n",
      "Train ::  Epoch: 22, Batch 400, Loss 0.056998\n",
      "Train ::  Epoch: 22, Batch 500, Loss 0.043195\n",
      "Train ::  Epoch: 22, Batch 600, Loss 0.053840\n",
      "Train ::  Epoch: 22, Batch 700, Loss 0.050220\n",
      "Train ::  Epoch: 22, Batch 800, Loss 0.050506\n",
      "Train ::  Epoch: 22, Batch 900, Loss 0.045736\n",
      "Train ::  Epoch: 22, Batch 1000, Loss 0.052339\n",
      "Train ::  Epoch: 23, Batch 0, Loss 0.046909\n",
      "Train ::  Epoch: 23, Batch 100, Loss 0.047362\n",
      "Train ::  Epoch: 23, Batch 200, Loss 0.050888\n",
      "Train ::  Epoch: 23, Batch 300, Loss 0.049346\n",
      "Train ::  Epoch: 23, Batch 400, Loss 0.059645\n",
      "Train ::  Epoch: 23, Batch 500, Loss 0.051097\n",
      "Train ::  Epoch: 23, Batch 600, Loss 0.039811\n",
      "Train ::  Epoch: 23, Batch 700, Loss 0.047681\n",
      "Train ::  Epoch: 23, Batch 800, Loss 0.049743\n",
      "Train ::  Epoch: 23, Batch 900, Loss 0.064557\n",
      "Train ::  Epoch: 23, Batch 1000, Loss 0.052456\n",
      "Train ::  Epoch: 24, Batch 0, Loss 0.039500\n",
      "Train ::  Epoch: 24, Batch 100, Loss 0.040459\n",
      "Train ::  Epoch: 24, Batch 200, Loss 0.048754\n",
      "Train ::  Epoch: 24, Batch 300, Loss 0.059440\n",
      "Train ::  Epoch: 24, Batch 400, Loss 0.044789\n",
      "Train ::  Epoch: 24, Batch 500, Loss 0.053015\n",
      "Train ::  Epoch: 24, Batch 600, Loss 0.048768\n",
      "Train ::  Epoch: 24, Batch 700, Loss 0.050163\n",
      "Train ::  Epoch: 24, Batch 800, Loss 0.052718\n",
      "Train ::  Epoch: 24, Batch 900, Loss 0.061527\n",
      "Train ::  Epoch: 24, Batch 1000, Loss 0.053921\n",
      "Train ::  Epoch: 25, Batch 0, Loss 0.042957\n",
      "Train ::  Epoch: 25, Batch 100, Loss 0.045545\n",
      "Train ::  Epoch: 25, Batch 200, Loss 0.043609\n",
      "Train ::  Epoch: 25, Batch 300, Loss 0.033888\n",
      "Train ::  Epoch: 25, Batch 400, Loss 0.043681\n",
      "Train ::  Epoch: 25, Batch 500, Loss 0.049510\n",
      "Train ::  Epoch: 25, Batch 600, Loss 0.047830\n",
      "Train ::  Epoch: 25, Batch 700, Loss 0.049165\n",
      "Train ::  Epoch: 25, Batch 800, Loss 0.052871\n",
      "Train ::  Epoch: 25, Batch 900, Loss 0.058832\n",
      "Train ::  Epoch: 25, Batch 1000, Loss 0.051859\n",
      "Train ::  Epoch: 26, Batch 0, Loss 0.051206\n",
      "Train ::  Epoch: 26, Batch 100, Loss 0.040611\n",
      "Train ::  Epoch: 26, Batch 200, Loss 0.051037\n",
      "Train ::  Epoch: 26, Batch 300, Loss 0.055498\n",
      "Train ::  Epoch: 26, Batch 400, Loss 0.044211\n",
      "Train ::  Epoch: 26, Batch 500, Loss 0.046869\n",
      "Train ::  Epoch: 26, Batch 600, Loss 0.047417\n",
      "Train ::  Epoch: 26, Batch 700, Loss 0.046204\n",
      "Train ::  Epoch: 26, Batch 800, Loss 0.044193\n",
      "Train ::  Epoch: 26, Batch 900, Loss 0.047206\n",
      "Train ::  Epoch: 26, Batch 1000, Loss 0.033425\n",
      "Train ::  Epoch: 27, Batch 0, Loss 0.042300\n",
      "Train ::  Epoch: 27, Batch 100, Loss 0.049418\n",
      "Train ::  Epoch: 27, Batch 200, Loss 0.049097\n",
      "Train ::  Epoch: 27, Batch 300, Loss 0.042384\n",
      "Train ::  Epoch: 27, Batch 400, Loss 0.053815\n",
      "Train ::  Epoch: 27, Batch 500, Loss 0.043623\n",
      "Train ::  Epoch: 27, Batch 600, Loss 0.040142\n",
      "Train ::  Epoch: 27, Batch 700, Loss 0.036978\n",
      "Train ::  Epoch: 27, Batch 800, Loss 0.058954\n",
      "Train ::  Epoch: 27, Batch 900, Loss 0.045054\n",
      "Train ::  Epoch: 27, Batch 1000, Loss 0.043922\n",
      "Train ::  Epoch: 28, Batch 0, Loss 0.042561\n",
      "Train ::  Epoch: 28, Batch 100, Loss 0.044073\n",
      "Train ::  Epoch: 28, Batch 200, Loss 0.037228\n",
      "Train ::  Epoch: 28, Batch 300, Loss 0.038025\n",
      "Train ::  Epoch: 28, Batch 400, Loss 0.048283\n",
      "Train ::  Epoch: 28, Batch 500, Loss 0.040964\n",
      "Train ::  Epoch: 28, Batch 600, Loss 0.039177\n",
      "Train ::  Epoch: 28, Batch 700, Loss 0.049360\n",
      "Train ::  Epoch: 28, Batch 800, Loss 0.041781\n",
      "Train ::  Epoch: 28, Batch 900, Loss 0.041396\n",
      "Train ::  Epoch: 28, Batch 1000, Loss 0.042861\n",
      "Train ::  Epoch: 29, Batch 0, Loss 0.044868\n",
      "Train ::  Epoch: 29, Batch 100, Loss 0.039038\n",
      "Train ::  Epoch: 29, Batch 200, Loss 0.036567\n",
      "Train ::  Epoch: 29, Batch 300, Loss 0.031995\n",
      "Train ::  Epoch: 29, Batch 400, Loss 0.040895\n",
      "Train ::  Epoch: 29, Batch 500, Loss 0.041896\n",
      "Train ::  Epoch: 29, Batch 600, Loss 0.041342\n",
      "Train ::  Epoch: 29, Batch 700, Loss 0.046212\n",
      "Train ::  Epoch: 29, Batch 800, Loss 0.045272\n",
      "Train ::  Epoch: 29, Batch 900, Loss 0.047253\n",
      "Train ::  Epoch: 29, Batch 1000, Loss 0.041763\n",
      "Train ::  Epoch: 30, Batch 0, Loss 0.035952\n",
      "Train ::  Epoch: 30, Batch 100, Loss 0.047621\n",
      "Train ::  Epoch: 30, Batch 200, Loss 0.032659\n",
      "Train ::  Epoch: 30, Batch 300, Loss 0.031456\n",
      "Train ::  Epoch: 30, Batch 400, Loss 0.045903\n",
      "Train ::  Epoch: 30, Batch 500, Loss 0.047822\n",
      "Train ::  Epoch: 30, Batch 600, Loss 0.039305\n",
      "Train ::  Epoch: 30, Batch 700, Loss 0.047580\n",
      "Train ::  Epoch: 30, Batch 800, Loss 0.043109\n",
      "Train ::  Epoch: 30, Batch 900, Loss 0.057310\n",
      "Train ::  Epoch: 30, Batch 1000, Loss 0.042313\n",
      "Train ::  Epoch: 31, Batch 0, Loss 0.031024\n",
      "Train ::  Epoch: 31, Batch 100, Loss 0.043495\n",
      "Train ::  Epoch: 31, Batch 200, Loss 0.040805\n",
      "Train ::  Epoch: 31, Batch 300, Loss 0.038366\n",
      "Train ::  Epoch: 31, Batch 400, Loss 0.042673\n",
      "Train ::  Epoch: 31, Batch 500, Loss 0.043185\n",
      "Train ::  Epoch: 31, Batch 600, Loss 0.052371\n",
      "Train ::  Epoch: 31, Batch 700, Loss 0.040470\n",
      "Train ::  Epoch: 31, Batch 800, Loss 0.036270\n",
      "Train ::  Epoch: 31, Batch 900, Loss 0.044063\n",
      "Train ::  Epoch: 31, Batch 1000, Loss 0.043293\n",
      "Train ::  Epoch: 32, Batch 0, Loss 0.041000\n",
      "Train ::  Epoch: 32, Batch 100, Loss 0.030472\n",
      "Train ::  Epoch: 32, Batch 200, Loss 0.042076\n",
      "Train ::  Epoch: 32, Batch 300, Loss 0.033040\n",
      "Train ::  Epoch: 32, Batch 400, Loss 0.031990\n",
      "Train ::  Epoch: 32, Batch 500, Loss 0.033015\n",
      "Train ::  Epoch: 32, Batch 600, Loss 0.037374\n",
      "Train ::  Epoch: 32, Batch 700, Loss 0.033533\n",
      "Train ::  Epoch: 32, Batch 800, Loss 0.033324\n",
      "Train ::  Epoch: 32, Batch 900, Loss 0.036713\n",
      "Train ::  Epoch: 32, Batch 1000, Loss 0.044254\n",
      "Train ::  Epoch: 33, Batch 0, Loss 0.036679\n",
      "Train ::  Epoch: 33, Batch 100, Loss 0.031606\n",
      "Train ::  Epoch: 33, Batch 200, Loss 0.034846\n",
      "Train ::  Epoch: 33, Batch 300, Loss 0.041880\n",
      "Train ::  Epoch: 33, Batch 400, Loss 0.042137\n",
      "Train ::  Epoch: 33, Batch 500, Loss 0.043528\n",
      "Train ::  Epoch: 33, Batch 600, Loss 0.048003\n",
      "Train ::  Epoch: 33, Batch 700, Loss 0.048738\n",
      "Train ::  Epoch: 33, Batch 800, Loss 0.046248\n",
      "Train ::  Epoch: 33, Batch 900, Loss 0.053780\n",
      "Train ::  Epoch: 33, Batch 1000, Loss 0.050726\n",
      "Train ::  Epoch: 34, Batch 0, Loss 0.028182\n",
      "Train ::  Epoch: 34, Batch 100, Loss 0.037469\n",
      "Train ::  Epoch: 34, Batch 200, Loss 0.040479\n",
      "Train ::  Epoch: 34, Batch 300, Loss 0.040432\n",
      "Train ::  Epoch: 34, Batch 400, Loss 0.033231\n",
      "Train ::  Epoch: 34, Batch 500, Loss 0.037262\n",
      "Train ::  Epoch: 34, Batch 600, Loss 0.038583\n",
      "Train ::  Epoch: 34, Batch 700, Loss 0.037152\n",
      "Train ::  Epoch: 34, Batch 800, Loss 0.031193\n",
      "Train ::  Epoch: 34, Batch 900, Loss 0.048627\n",
      "Train ::  Epoch: 34, Batch 1000, Loss 0.042077\n",
      "Train ::  Epoch: 35, Batch 0, Loss 0.036251\n",
      "Train ::  Epoch: 35, Batch 100, Loss 0.035717\n",
      "Train ::  Epoch: 35, Batch 200, Loss 0.039931\n",
      "Train ::  Epoch: 35, Batch 300, Loss 0.037246\n",
      "Train ::  Epoch: 35, Batch 400, Loss 0.029516\n",
      "Train ::  Epoch: 35, Batch 500, Loss 0.034266\n",
      "Train ::  Epoch: 35, Batch 600, Loss 0.044102\n",
      "Train ::  Epoch: 35, Batch 700, Loss 0.041653\n",
      "Train ::  Epoch: 35, Batch 800, Loss 0.037456\n",
      "Train ::  Epoch: 35, Batch 900, Loss 0.036864\n",
      "Train ::  Epoch: 35, Batch 1000, Loss 0.049532\n",
      "Train ::  Epoch: 36, Batch 0, Loss 0.030458\n",
      "Train ::  Epoch: 36, Batch 100, Loss 0.040484\n",
      "Train ::  Epoch: 36, Batch 200, Loss 0.035346\n",
      "Train ::  Epoch: 36, Batch 300, Loss 0.037987\n",
      "Train ::  Epoch: 36, Batch 400, Loss 0.037673\n",
      "Train ::  Epoch: 36, Batch 500, Loss 0.038178\n",
      "Train ::  Epoch: 36, Batch 600, Loss 0.041526\n",
      "Train ::  Epoch: 36, Batch 700, Loss 0.036761\n",
      "Train ::  Epoch: 36, Batch 800, Loss 0.034897\n",
      "Train ::  Epoch: 36, Batch 900, Loss 0.036148\n",
      "Train ::  Epoch: 36, Batch 1000, Loss 0.044066\n",
      "Train ::  Epoch: 37, Batch 0, Loss 0.032668\n",
      "Train ::  Epoch: 37, Batch 100, Loss 0.037477\n",
      "Train ::  Epoch: 37, Batch 200, Loss 0.029219\n",
      "Train ::  Epoch: 37, Batch 300, Loss 0.040905\n",
      "Train ::  Epoch: 37, Batch 400, Loss 0.042853\n",
      "Train ::  Epoch: 37, Batch 500, Loss 0.033542\n",
      "Train ::  Epoch: 37, Batch 600, Loss 0.038190\n",
      "Train ::  Epoch: 37, Batch 700, Loss 0.045562\n",
      "Train ::  Epoch: 37, Batch 800, Loss 0.042269\n",
      "Train ::  Epoch: 37, Batch 900, Loss 0.041412\n",
      "Train ::  Epoch: 37, Batch 1000, Loss 0.045586\n",
      "Train ::  Epoch: 38, Batch 0, Loss 0.049577\n",
      "Train ::  Epoch: 38, Batch 100, Loss 0.033620\n",
      "Train ::  Epoch: 38, Batch 200, Loss 0.034650\n",
      "Train ::  Epoch: 38, Batch 300, Loss 0.037499\n",
      "Train ::  Epoch: 38, Batch 400, Loss 0.046324\n",
      "Train ::  Epoch: 38, Batch 500, Loss 0.031082\n",
      "Train ::  Epoch: 38, Batch 600, Loss 0.044026\n",
      "Train ::  Epoch: 38, Batch 700, Loss 0.035217\n",
      "Train ::  Epoch: 38, Batch 800, Loss 0.031659\n",
      "Train ::  Epoch: 38, Batch 900, Loss 0.037169\n",
      "Train ::  Epoch: 38, Batch 1000, Loss 0.037140\n",
      "Train ::  Epoch: 39, Batch 0, Loss 0.030236\n",
      "Train ::  Epoch: 39, Batch 100, Loss 0.036759\n",
      "Train ::  Epoch: 39, Batch 200, Loss 0.032300\n",
      "Train ::  Epoch: 39, Batch 300, Loss 0.040023\n",
      "Train ::  Epoch: 39, Batch 400, Loss 0.033985\n",
      "Train ::  Epoch: 39, Batch 500, Loss 0.040142\n",
      "Train ::  Epoch: 39, Batch 600, Loss 0.035365\n",
      "Train ::  Epoch: 39, Batch 700, Loss 0.032867\n",
      "Train ::  Epoch: 39, Batch 800, Loss 0.033010\n",
      "Train ::  Epoch: 39, Batch 900, Loss 0.040218\n",
      "Train ::  Epoch: 39, Batch 1000, Loss 0.043568\n",
      "Train ::  Epoch: 40, Batch 0, Loss 0.030487\n",
      "Train ::  Epoch: 40, Batch 100, Loss 0.033166\n",
      "Train ::  Epoch: 40, Batch 200, Loss 0.039703\n",
      "Train ::  Epoch: 40, Batch 300, Loss 0.033594\n",
      "Train ::  Epoch: 40, Batch 400, Loss 0.039302\n",
      "Train ::  Epoch: 40, Batch 500, Loss 0.041266\n",
      "Train ::  Epoch: 40, Batch 600, Loss 0.031702\n",
      "Train ::  Epoch: 40, Batch 700, Loss 0.040481\n",
      "Train ::  Epoch: 40, Batch 800, Loss 0.040415\n",
      "Train ::  Epoch: 40, Batch 900, Loss 0.039694\n",
      "Train ::  Epoch: 40, Batch 1000, Loss 0.035335\n",
      "Train ::  Epoch: 41, Batch 0, Loss 0.032611\n",
      "Train ::  Epoch: 41, Batch 100, Loss 0.034368\n",
      "Train ::  Epoch: 41, Batch 200, Loss 0.031919\n",
      "Train ::  Epoch: 41, Batch 300, Loss 0.033763\n",
      "Train ::  Epoch: 41, Batch 400, Loss 0.039305\n",
      "Train ::  Epoch: 41, Batch 500, Loss 0.037877\n",
      "Train ::  Epoch: 41, Batch 600, Loss 0.038628\n",
      "Train ::  Epoch: 41, Batch 700, Loss 0.038973\n",
      "Train ::  Epoch: 41, Batch 800, Loss 0.040124\n",
      "Train ::  Epoch: 41, Batch 900, Loss 0.034745\n",
      "Train ::  Epoch: 41, Batch 1000, Loss 0.031230\n",
      "Train ::  Epoch: 42, Batch 0, Loss 0.035461\n",
      "Train ::  Epoch: 42, Batch 100, Loss 0.039241\n",
      "Train ::  Epoch: 42, Batch 200, Loss 0.031307\n",
      "Train ::  Epoch: 42, Batch 300, Loss 0.045286\n",
      "Train ::  Epoch: 42, Batch 400, Loss 0.031577\n",
      "Train ::  Epoch: 42, Batch 500, Loss 0.044470\n",
      "Train ::  Epoch: 42, Batch 600, Loss 0.034583\n",
      "Train ::  Epoch: 42, Batch 700, Loss 0.032359\n",
      "Train ::  Epoch: 42, Batch 800, Loss 0.032155\n",
      "Train ::  Epoch: 42, Batch 900, Loss 0.039364\n",
      "Train ::  Epoch: 42, Batch 1000, Loss 0.033887\n",
      "Train ::  Epoch: 43, Batch 0, Loss 0.030588\n",
      "Train ::  Epoch: 43, Batch 100, Loss 0.031707\n",
      "Train ::  Epoch: 43, Batch 200, Loss 0.037666\n",
      "Train ::  Epoch: 43, Batch 300, Loss 0.038229\n",
      "Train ::  Epoch: 43, Batch 400, Loss 0.036026\n",
      "Train ::  Epoch: 43, Batch 500, Loss 0.032969\n",
      "Train ::  Epoch: 43, Batch 600, Loss 0.033684\n",
      "Train ::  Epoch: 43, Batch 700, Loss 0.039674\n",
      "Train ::  Epoch: 43, Batch 800, Loss 0.033612\n",
      "Train ::  Epoch: 43, Batch 900, Loss 0.041397\n",
      "Train ::  Epoch: 43, Batch 1000, Loss 0.051178\n",
      "Train ::  Epoch: 44, Batch 0, Loss 0.037349\n",
      "Train ::  Epoch: 44, Batch 100, Loss 0.030633\n",
      "Train ::  Epoch: 44, Batch 200, Loss 0.042261\n",
      "Train ::  Epoch: 44, Batch 300, Loss 0.036769\n",
      "Train ::  Epoch: 44, Batch 400, Loss 0.041129\n",
      "Train ::  Epoch: 44, Batch 500, Loss 0.042241\n",
      "Train ::  Epoch: 44, Batch 600, Loss 0.039467\n",
      "Train ::  Epoch: 44, Batch 700, Loss 0.034455\n",
      "Train ::  Epoch: 44, Batch 800, Loss 0.031903\n",
      "Train ::  Epoch: 44, Batch 900, Loss 0.037664\n",
      "Train ::  Epoch: 44, Batch 1000, Loss 0.038596\n",
      "Train ::  Epoch: 45, Batch 0, Loss 0.038175\n",
      "Train ::  Epoch: 45, Batch 100, Loss 0.035282\n",
      "Train ::  Epoch: 45, Batch 200, Loss 0.038915\n",
      "Train ::  Epoch: 45, Batch 300, Loss 0.027341\n",
      "Train ::  Epoch: 45, Batch 400, Loss 0.037047\n",
      "Train ::  Epoch: 45, Batch 500, Loss 0.029394\n",
      "Train ::  Epoch: 45, Batch 600, Loss 0.036161\n",
      "Train ::  Epoch: 45, Batch 700, Loss 0.032913\n",
      "Train ::  Epoch: 45, Batch 800, Loss 0.042718\n",
      "Train ::  Epoch: 45, Batch 900, Loss 0.037734\n",
      "Train ::  Epoch: 45, Batch 1000, Loss 0.036769\n",
      "Train ::  Epoch: 46, Batch 0, Loss 0.030372\n",
      "Train ::  Epoch: 46, Batch 100, Loss 0.030986\n",
      "Train ::  Epoch: 46, Batch 200, Loss 0.035556\n",
      "Train ::  Epoch: 46, Batch 300, Loss 0.031313\n",
      "Train ::  Epoch: 46, Batch 400, Loss 0.050076\n",
      "Train ::  Epoch: 46, Batch 500, Loss 0.032485\n",
      "Train ::  Epoch: 46, Batch 600, Loss 0.028754\n",
      "Train ::  Epoch: 46, Batch 700, Loss 0.034315\n",
      "Train ::  Epoch: 46, Batch 800, Loss 0.038608\n",
      "Train ::  Epoch: 46, Batch 900, Loss 0.035211\n",
      "Train ::  Epoch: 46, Batch 1000, Loss 0.032002\n",
      "Train ::  Epoch: 47, Batch 0, Loss 0.032144\n",
      "Train ::  Epoch: 47, Batch 100, Loss 0.035978\n",
      "Train ::  Epoch: 47, Batch 200, Loss 0.033517\n",
      "Train ::  Epoch: 47, Batch 300, Loss 0.033136\n",
      "Train ::  Epoch: 47, Batch 400, Loss 0.036492\n",
      "Train ::  Epoch: 47, Batch 500, Loss 0.032416\n",
      "Train ::  Epoch: 47, Batch 600, Loss 0.035973\n",
      "Train ::  Epoch: 47, Batch 700, Loss 0.027624\n",
      "Train ::  Epoch: 47, Batch 800, Loss 0.033799\n",
      "Train ::  Epoch: 47, Batch 900, Loss 0.033959\n",
      "Train ::  Epoch: 47, Batch 1000, Loss 0.056309\n",
      "Train ::  Epoch: 48, Batch 0, Loss 0.032460\n",
      "Train ::  Epoch: 48, Batch 100, Loss 0.030145\n",
      "Train ::  Epoch: 48, Batch 200, Loss 0.027532\n",
      "Train ::  Epoch: 48, Batch 300, Loss 0.039557\n",
      "Train ::  Epoch: 48, Batch 400, Loss 0.037467\n",
      "Train ::  Epoch: 48, Batch 500, Loss 0.040753\n",
      "Train ::  Epoch: 48, Batch 600, Loss 0.033438\n",
      "Train ::  Epoch: 48, Batch 700, Loss 0.034329\n",
      "Train ::  Epoch: 48, Batch 800, Loss 0.032958\n",
      "Train ::  Epoch: 48, Batch 900, Loss 0.040123\n",
      "Train ::  Epoch: 48, Batch 1000, Loss 0.032639\n",
      "Train ::  Epoch: 49, Batch 0, Loss 0.034094\n",
      "Train ::  Epoch: 49, Batch 100, Loss 0.041920\n",
      "Train ::  Epoch: 49, Batch 200, Loss 0.029101\n",
      "Train ::  Epoch: 49, Batch 300, Loss 0.036404\n",
      "Train ::  Epoch: 49, Batch 400, Loss 0.035020\n",
      "Train ::  Epoch: 49, Batch 500, Loss 0.031663\n",
      "Train ::  Epoch: 49, Batch 600, Loss 0.036179\n",
      "Train ::  Epoch: 49, Batch 700, Loss 0.036797\n",
      "Train ::  Epoch: 49, Batch 800, Loss 0.033486\n",
      "Train ::  Epoch: 49, Batch 900, Loss 0.031485\n",
      "Train ::  Epoch: 49, Batch 1000, Loss 0.044786\n",
      "Train ::  Epoch: 50, Batch 0, Loss 0.028266\n",
      "Train ::  Epoch: 50, Batch 100, Loss 0.028322\n",
      "Train ::  Epoch: 50, Batch 200, Loss 0.028783\n",
      "Train ::  Epoch: 50, Batch 300, Loss 0.027213\n",
      "Train ::  Epoch: 50, Batch 400, Loss 0.031972\n",
      "Train ::  Epoch: 50, Batch 500, Loss 0.028063\n",
      "Train ::  Epoch: 50, Batch 600, Loss 0.030256\n",
      "Train ::  Epoch: 50, Batch 700, Loss 0.032608\n",
      "Train ::  Epoch: 50, Batch 800, Loss 0.038409\n",
      "Train ::  Epoch: 50, Batch 900, Loss 0.032788\n",
      "Train ::  Epoch: 50, Batch 1000, Loss 0.045327\n",
      "Train ::  Epoch: 51, Batch 0, Loss 0.039866\n",
      "Train ::  Epoch: 51, Batch 100, Loss 0.039778\n",
      "Train ::  Epoch: 51, Batch 200, Loss 0.026247\n",
      "Train ::  Epoch: 51, Batch 300, Loss 0.034307\n",
      "Train ::  Epoch: 51, Batch 400, Loss 0.039184\n",
      "Train ::  Epoch: 51, Batch 500, Loss 0.030181\n",
      "Train ::  Epoch: 51, Batch 600, Loss 0.036791\n",
      "Train ::  Epoch: 51, Batch 700, Loss 0.034244\n",
      "Train ::  Epoch: 51, Batch 800, Loss 0.029415\n",
      "Train ::  Epoch: 51, Batch 900, Loss 0.034119\n",
      "Train ::  Epoch: 51, Batch 1000, Loss 0.028726\n",
      "Train ::  Epoch: 52, Batch 0, Loss 0.028935\n",
      "Train ::  Epoch: 52, Batch 100, Loss 0.037438\n",
      "Train ::  Epoch: 52, Batch 200, Loss 0.026153\n",
      "Train ::  Epoch: 52, Batch 300, Loss 0.029367\n",
      "Train ::  Epoch: 52, Batch 400, Loss 0.032310\n",
      "Train ::  Epoch: 52, Batch 500, Loss 0.030440\n",
      "Train ::  Epoch: 52, Batch 600, Loss 0.036294\n",
      "Train ::  Epoch: 52, Batch 700, Loss 0.034565\n",
      "Train ::  Epoch: 52, Batch 800, Loss 0.031687\n",
      "Train ::  Epoch: 52, Batch 900, Loss 0.039277\n",
      "Train ::  Epoch: 52, Batch 1000, Loss 0.042939\n",
      "Train ::  Epoch: 53, Batch 0, Loss 0.037484\n",
      "Train ::  Epoch: 53, Batch 100, Loss 0.034275\n",
      "Train ::  Epoch: 53, Batch 200, Loss 0.037424\n",
      "Train ::  Epoch: 53, Batch 300, Loss 0.033238\n",
      "Train ::  Epoch: 53, Batch 400, Loss 0.035337\n",
      "Train ::  Epoch: 53, Batch 500, Loss 0.030124\n",
      "Train ::  Epoch: 53, Batch 600, Loss 0.036887\n",
      "Train ::  Epoch: 53, Batch 700, Loss 0.038846\n",
      "Train ::  Epoch: 53, Batch 800, Loss 0.028969\n",
      "Train ::  Epoch: 53, Batch 900, Loss 0.037164\n",
      "Train ::  Epoch: 53, Batch 1000, Loss 0.039540\n",
      "Train ::  Epoch: 54, Batch 0, Loss 0.030846\n",
      "Train ::  Epoch: 54, Batch 100, Loss 0.028056\n",
      "Train ::  Epoch: 54, Batch 200, Loss 0.039349\n",
      "Train ::  Epoch: 54, Batch 300, Loss 0.026457\n",
      "Train ::  Epoch: 54, Batch 400, Loss 0.034075\n",
      "Train ::  Epoch: 54, Batch 500, Loss 0.031653\n",
      "Train ::  Epoch: 54, Batch 600, Loss 0.032376\n",
      "Train ::  Epoch: 54, Batch 700, Loss 0.031772\n",
      "Train ::  Epoch: 54, Batch 800, Loss 0.038174\n",
      "Train ::  Epoch: 54, Batch 900, Loss 0.038589\n",
      "Train ::  Epoch: 54, Batch 1000, Loss 0.032160\n",
      "Train ::  Epoch: 55, Batch 0, Loss 0.026586\n",
      "Train ::  Epoch: 55, Batch 100, Loss 0.034054\n",
      "Train ::  Epoch: 55, Batch 200, Loss 0.032488\n",
      "Train ::  Epoch: 55, Batch 300, Loss 0.042740\n",
      "Train ::  Epoch: 55, Batch 400, Loss 0.032997\n",
      "Train ::  Epoch: 55, Batch 500, Loss 0.033800\n",
      "Train ::  Epoch: 55, Batch 600, Loss 0.039426\n",
      "Train ::  Epoch: 55, Batch 700, Loss 0.035590\n",
      "Train ::  Epoch: 55, Batch 800, Loss 0.039954\n",
      "Train ::  Epoch: 55, Batch 900, Loss 0.028537\n",
      "Train ::  Epoch: 55, Batch 1000, Loss 0.032853\n",
      "Train ::  Epoch: 56, Batch 0, Loss 0.036687\n",
      "Train ::  Epoch: 56, Batch 100, Loss 0.033192\n",
      "Train ::  Epoch: 56, Batch 200, Loss 0.031045\n",
      "Train ::  Epoch: 56, Batch 300, Loss 0.026177\n",
      "Train ::  Epoch: 56, Batch 400, Loss 0.032951\n",
      "Train ::  Epoch: 56, Batch 500, Loss 0.030464\n",
      "Train ::  Epoch: 56, Batch 600, Loss 0.032555\n",
      "Train ::  Epoch: 56, Batch 700, Loss 0.033810\n",
      "Train ::  Epoch: 56, Batch 800, Loss 0.035322\n",
      "Train ::  Epoch: 56, Batch 900, Loss 0.038159\n",
      "Train ::  Epoch: 56, Batch 1000, Loss 0.033197\n",
      "Train ::  Epoch: 57, Batch 0, Loss 0.032826\n",
      "Train ::  Epoch: 57, Batch 100, Loss 0.032815\n",
      "Train ::  Epoch: 57, Batch 200, Loss 0.028168\n",
      "Train ::  Epoch: 57, Batch 300, Loss 0.030111\n",
      "Train ::  Epoch: 57, Batch 400, Loss 0.034349\n",
      "Train ::  Epoch: 57, Batch 500, Loss 0.041306\n",
      "Train ::  Epoch: 57, Batch 600, Loss 0.031247\n",
      "Train ::  Epoch: 57, Batch 700, Loss 0.037430\n",
      "Train ::  Epoch: 57, Batch 800, Loss 0.029145\n",
      "Train ::  Epoch: 57, Batch 900, Loss 0.030576\n",
      "Train ::  Epoch: 57, Batch 1000, Loss 0.029718\n",
      "Train ::  Epoch: 58, Batch 0, Loss 0.028160\n",
      "Train ::  Epoch: 58, Batch 100, Loss 0.036404\n",
      "Train ::  Epoch: 58, Batch 200, Loss 0.030942\n",
      "Train ::  Epoch: 58, Batch 300, Loss 0.043945\n",
      "Train ::  Epoch: 58, Batch 400, Loss 0.031628\n",
      "Train ::  Epoch: 58, Batch 500, Loss 0.030119\n",
      "Train ::  Epoch: 58, Batch 600, Loss 0.029399\n",
      "Train ::  Epoch: 58, Batch 700, Loss 0.033919\n",
      "Train ::  Epoch: 58, Batch 800, Loss 0.035011\n",
      "Train ::  Epoch: 58, Batch 900, Loss 0.026267\n",
      "Train ::  Epoch: 58, Batch 1000, Loss 0.043624\n",
      "Train ::  Epoch: 59, Batch 0, Loss 0.031726\n",
      "Train ::  Epoch: 59, Batch 100, Loss 0.028916\n",
      "Train ::  Epoch: 59, Batch 200, Loss 0.035552\n",
      "Train ::  Epoch: 59, Batch 300, Loss 0.036760\n",
      "Train ::  Epoch: 59, Batch 400, Loss 0.034130\n",
      "Train ::  Epoch: 59, Batch 500, Loss 0.052065\n",
      "Train ::  Epoch: 59, Batch 600, Loss 0.036160\n",
      "Train ::  Epoch: 59, Batch 700, Loss 0.032211\n",
      "Train ::  Epoch: 59, Batch 800, Loss 0.037444\n",
      "Train ::  Epoch: 59, Batch 900, Loss 0.038340\n",
      "Train ::  Epoch: 59, Batch 1000, Loss 0.032904\n",
      "Train ::  Epoch: 60, Batch 0, Loss 0.029880\n",
      "Train ::  Epoch: 60, Batch 100, Loss 0.025193\n",
      "Train ::  Epoch: 60, Batch 200, Loss 0.024911\n",
      "Train ::  Epoch: 60, Batch 300, Loss 0.025525\n",
      "Train ::  Epoch: 60, Batch 400, Loss 0.027023\n",
      "Train ::  Epoch: 60, Batch 500, Loss 0.031824\n",
      "Train ::  Epoch: 60, Batch 600, Loss 0.039450\n",
      "Train ::  Epoch: 60, Batch 700, Loss 0.037925\n",
      "Train ::  Epoch: 60, Batch 800, Loss 0.030300\n",
      "Train ::  Epoch: 60, Batch 900, Loss 0.034673\n",
      "Train ::  Epoch: 60, Batch 1000, Loss 0.032491\n",
      "Train ::  Epoch: 61, Batch 0, Loss 0.024394\n",
      "Train ::  Epoch: 61, Batch 100, Loss 0.033569\n",
      "Train ::  Epoch: 61, Batch 200, Loss 0.032494\n",
      "Train ::  Epoch: 61, Batch 300, Loss 0.030150\n",
      "Train ::  Epoch: 61, Batch 400, Loss 0.032362\n",
      "Train ::  Epoch: 61, Batch 500, Loss 0.029328\n",
      "Train ::  Epoch: 61, Batch 600, Loss 0.038893\n",
      "Train ::  Epoch: 61, Batch 700, Loss 0.034128\n",
      "Train ::  Epoch: 61, Batch 800, Loss 0.032061\n",
      "Train ::  Epoch: 61, Batch 900, Loss 0.031039\n",
      "Train ::  Epoch: 61, Batch 1000, Loss 0.029132\n",
      "Train ::  Epoch: 62, Batch 0, Loss 0.032001\n",
      "Train ::  Epoch: 62, Batch 100, Loss 0.034020\n",
      "Train ::  Epoch: 62, Batch 200, Loss 0.033281\n",
      "Train ::  Epoch: 62, Batch 300, Loss 0.032436\n",
      "Train ::  Epoch: 62, Batch 400, Loss 0.043292\n",
      "Train ::  Epoch: 62, Batch 500, Loss 0.029938\n",
      "Train ::  Epoch: 62, Batch 600, Loss 0.027191\n",
      "Train ::  Epoch: 62, Batch 700, Loss 0.033106\n",
      "Train ::  Epoch: 62, Batch 800, Loss 0.027616\n",
      "Train ::  Epoch: 62, Batch 900, Loss 0.032065\n",
      "Train ::  Epoch: 62, Batch 1000, Loss 0.039834\n",
      "Train ::  Epoch: 63, Batch 0, Loss 0.030827\n",
      "Train ::  Epoch: 63, Batch 100, Loss 0.034558\n",
      "Train ::  Epoch: 63, Batch 200, Loss 0.033976\n",
      "Train ::  Epoch: 63, Batch 300, Loss 0.027675\n",
      "Train ::  Epoch: 63, Batch 400, Loss 0.032224\n",
      "Train ::  Epoch: 63, Batch 500, Loss 0.040918\n",
      "Train ::  Epoch: 63, Batch 600, Loss 0.032417\n",
      "Train ::  Epoch: 63, Batch 700, Loss 0.032077\n",
      "Train ::  Epoch: 63, Batch 800, Loss 0.025762\n",
      "Train ::  Epoch: 63, Batch 900, Loss 0.033482\n",
      "Train ::  Epoch: 63, Batch 1000, Loss 0.033738\n",
      "Train ::  Epoch: 64, Batch 0, Loss 0.030793\n",
      "Train ::  Epoch: 64, Batch 100, Loss 0.030147\n",
      "Train ::  Epoch: 64, Batch 200, Loss 0.026741\n",
      "Train ::  Epoch: 64, Batch 300, Loss 0.040222\n",
      "Train ::  Epoch: 64, Batch 400, Loss 0.026714\n",
      "Train ::  Epoch: 64, Batch 500, Loss 0.029547\n",
      "Train ::  Epoch: 64, Batch 600, Loss 0.030363\n",
      "Train ::  Epoch: 64, Batch 700, Loss 0.038861\n",
      "Train ::  Epoch: 64, Batch 800, Loss 0.034632\n",
      "Train ::  Epoch: 64, Batch 900, Loss 0.031302\n",
      "Train ::  Epoch: 64, Batch 1000, Loss 0.034042\n",
      "Train ::  Epoch: 65, Batch 0, Loss 0.026057\n",
      "Train ::  Epoch: 65, Batch 100, Loss 0.029971\n",
      "Train ::  Epoch: 65, Batch 200, Loss 0.031526\n",
      "Train ::  Epoch: 65, Batch 300, Loss 0.031814\n",
      "Train ::  Epoch: 65, Batch 400, Loss 0.034045\n",
      "Train ::  Epoch: 65, Batch 500, Loss 0.035832\n",
      "Train ::  Epoch: 65, Batch 600, Loss 0.039752\n",
      "Train ::  Epoch: 65, Batch 700, Loss 0.038162\n",
      "Train ::  Epoch: 65, Batch 800, Loss 0.032633\n",
      "Train ::  Epoch: 65, Batch 900, Loss 0.038709\n",
      "Train ::  Epoch: 65, Batch 1000, Loss 0.037203\n",
      "Train ::  Epoch: 66, Batch 0, Loss 0.026570\n",
      "Train ::  Epoch: 66, Batch 100, Loss 0.026413\n",
      "Train ::  Epoch: 66, Batch 200, Loss 0.027693\n",
      "Train ::  Epoch: 66, Batch 300, Loss 0.027627\n",
      "Train ::  Epoch: 66, Batch 400, Loss 0.028198\n",
      "Train ::  Epoch: 66, Batch 500, Loss 0.028631\n",
      "Train ::  Epoch: 66, Batch 600, Loss 0.033059\n",
      "Train ::  Epoch: 66, Batch 700, Loss 0.038071\n",
      "Train ::  Epoch: 66, Batch 800, Loss 0.036589\n",
      "Train ::  Epoch: 66, Batch 900, Loss 0.027036\n",
      "Train ::  Epoch: 66, Batch 1000, Loss 0.035101\n",
      "Train ::  Epoch: 67, Batch 0, Loss 0.029870\n",
      "Train ::  Epoch: 67, Batch 100, Loss 0.028688\n",
      "Train ::  Epoch: 67, Batch 200, Loss 0.030498\n",
      "Train ::  Epoch: 67, Batch 300, Loss 0.031937\n",
      "Train ::  Epoch: 67, Batch 400, Loss 0.029348\n",
      "Train ::  Epoch: 67, Batch 500, Loss 0.038885\n",
      "Train ::  Epoch: 67, Batch 600, Loss 0.026077\n",
      "Train ::  Epoch: 67, Batch 700, Loss 0.027723\n",
      "Train ::  Epoch: 67, Batch 800, Loss 0.030746\n",
      "Train ::  Epoch: 67, Batch 900, Loss 0.038610\n",
      "Train ::  Epoch: 67, Batch 1000, Loss 0.036657\n",
      "Train ::  Epoch: 68, Batch 0, Loss 0.033972\n",
      "Train ::  Epoch: 68, Batch 100, Loss 0.037421\n",
      "Train ::  Epoch: 68, Batch 200, Loss 0.035885\n",
      "Train ::  Epoch: 68, Batch 300, Loss 0.032773\n",
      "Train ::  Epoch: 68, Batch 400, Loss 0.034069\n",
      "Train ::  Epoch: 68, Batch 500, Loss 0.036166\n",
      "Train ::  Epoch: 68, Batch 600, Loss 0.025842\n",
      "Train ::  Epoch: 68, Batch 700, Loss 0.047006\n",
      "Train ::  Epoch: 68, Batch 800, Loss 0.023855\n",
      "Train ::  Epoch: 68, Batch 900, Loss 0.030904\n",
      "Train ::  Epoch: 68, Batch 1000, Loss 0.033051\n",
      "Train ::  Epoch: 69, Batch 0, Loss 0.024996\n",
      "Train ::  Epoch: 69, Batch 100, Loss 0.022519\n",
      "Train ::  Epoch: 69, Batch 200, Loss 0.035143\n",
      "Train ::  Epoch: 69, Batch 300, Loss 0.026463\n",
      "Train ::  Epoch: 69, Batch 400, Loss 0.036206\n",
      "Train ::  Epoch: 69, Batch 500, Loss 0.028539\n",
      "Train ::  Epoch: 69, Batch 600, Loss 0.028251\n",
      "Train ::  Epoch: 69, Batch 700, Loss 0.027293\n",
      "Train ::  Epoch: 69, Batch 800, Loss 0.037279\n",
      "Train ::  Epoch: 69, Batch 900, Loss 0.031754\n",
      "Train ::  Epoch: 69, Batch 1000, Loss 0.033181\n",
      "Train ::  Epoch: 70, Batch 0, Loss 0.032774\n",
      "Train ::  Epoch: 70, Batch 100, Loss 0.027187\n",
      "Train ::  Epoch: 70, Batch 200, Loss 0.036223\n",
      "Train ::  Epoch: 70, Batch 300, Loss 0.028793\n",
      "Train ::  Epoch: 70, Batch 400, Loss 0.035126\n",
      "Train ::  Epoch: 70, Batch 500, Loss 0.030357\n",
      "Train ::  Epoch: 70, Batch 600, Loss 0.028743\n",
      "Train ::  Epoch: 70, Batch 700, Loss 0.034131\n",
      "Train ::  Epoch: 70, Batch 800, Loss 0.026050\n",
      "Train ::  Epoch: 70, Batch 900, Loss 0.032080\n",
      "Train ::  Epoch: 70, Batch 1000, Loss 0.037143\n",
      "Train ::  Epoch: 71, Batch 0, Loss 0.023730\n",
      "Train ::  Epoch: 71, Batch 100, Loss 0.024633\n",
      "Train ::  Epoch: 71, Batch 200, Loss 0.025877\n",
      "Train ::  Epoch: 71, Batch 300, Loss 0.027998\n",
      "Train ::  Epoch: 71, Batch 400, Loss 0.032116\n",
      "Train ::  Epoch: 71, Batch 500, Loss 0.042137\n",
      "Train ::  Epoch: 71, Batch 600, Loss 0.037723\n",
      "Train ::  Epoch: 71, Batch 700, Loss 0.024979\n",
      "Train ::  Epoch: 71, Batch 800, Loss 0.031931\n",
      "Train ::  Epoch: 71, Batch 900, Loss 0.039174\n",
      "Train ::  Epoch: 71, Batch 1000, Loss 0.035626\n",
      "Train ::  Epoch: 72, Batch 0, Loss 0.031369\n",
      "Train ::  Epoch: 72, Batch 100, Loss 0.035960\n",
      "Train ::  Epoch: 72, Batch 200, Loss 0.034140\n",
      "Train ::  Epoch: 72, Batch 300, Loss 0.033829\n",
      "Train ::  Epoch: 72, Batch 400, Loss 0.031880\n",
      "Train ::  Epoch: 72, Batch 500, Loss 0.031698\n",
      "Train ::  Epoch: 72, Batch 600, Loss 0.034413\n",
      "Train ::  Epoch: 72, Batch 700, Loss 0.029744\n",
      "Train ::  Epoch: 72, Batch 800, Loss 0.026357\n",
      "Train ::  Epoch: 72, Batch 900, Loss 0.028417\n",
      "Train ::  Epoch: 72, Batch 1000, Loss 0.027016\n",
      "Train ::  Epoch: 73, Batch 0, Loss 0.037536\n",
      "Train ::  Epoch: 73, Batch 100, Loss 0.027814\n",
      "Train ::  Epoch: 73, Batch 200, Loss 0.022801\n",
      "Train ::  Epoch: 73, Batch 300, Loss 0.024263\n",
      "Train ::  Epoch: 73, Batch 400, Loss 0.023978\n",
      "Train ::  Epoch: 73, Batch 500, Loss 0.040007\n",
      "Train ::  Epoch: 73, Batch 600, Loss 0.029663\n",
      "Train ::  Epoch: 73, Batch 700, Loss 0.036066\n",
      "Train ::  Epoch: 73, Batch 800, Loss 0.029509\n",
      "Train ::  Epoch: 73, Batch 900, Loss 0.040100\n",
      "Train ::  Epoch: 73, Batch 1000, Loss 0.043361\n",
      "Train ::  Epoch: 74, Batch 0, Loss 0.033579\n",
      "Train ::  Epoch: 74, Batch 100, Loss 0.033878\n",
      "Train ::  Epoch: 74, Batch 200, Loss 0.026564\n",
      "Train ::  Epoch: 74, Batch 300, Loss 0.031003\n",
      "Train ::  Epoch: 74, Batch 400, Loss 0.030591\n",
      "Train ::  Epoch: 74, Batch 500, Loss 0.032538\n",
      "Train ::  Epoch: 74, Batch 600, Loss 0.025534\n",
      "Train ::  Epoch: 74, Batch 700, Loss 0.030751\n",
      "Train ::  Epoch: 74, Batch 800, Loss 0.039420\n",
      "Train ::  Epoch: 74, Batch 900, Loss 0.030557\n",
      "Train ::  Epoch: 74, Batch 1000, Loss 0.024781\n",
      "Train ::  Epoch: 75, Batch 0, Loss 0.032614\n",
      "Train ::  Epoch: 75, Batch 100, Loss 0.038754\n",
      "Train ::  Epoch: 75, Batch 200, Loss 0.039958\n",
      "Train ::  Epoch: 75, Batch 300, Loss 0.027547\n",
      "Train ::  Epoch: 75, Batch 400, Loss 0.032751\n",
      "Train ::  Epoch: 75, Batch 500, Loss 0.030634\n",
      "Train ::  Epoch: 75, Batch 600, Loss 0.032884\n",
      "Train ::  Epoch: 75, Batch 700, Loss 0.037775\n",
      "Train ::  Epoch: 75, Batch 800, Loss 0.033945\n",
      "Train ::  Epoch: 75, Batch 900, Loss 0.024305\n",
      "Train ::  Epoch: 75, Batch 1000, Loss 0.032573\n",
      "Train ::  Epoch: 76, Batch 0, Loss 0.031608\n",
      "Train ::  Epoch: 76, Batch 100, Loss 0.027509\n",
      "Train ::  Epoch: 76, Batch 200, Loss 0.026317\n",
      "Train ::  Epoch: 76, Batch 300, Loss 0.021644\n",
      "Train ::  Epoch: 76, Batch 400, Loss 0.017805\n",
      "Train ::  Epoch: 76, Batch 500, Loss 0.026128\n",
      "Train ::  Epoch: 76, Batch 600, Loss 0.027990\n",
      "Train ::  Epoch: 76, Batch 700, Loss 0.035856\n",
      "Train ::  Epoch: 76, Batch 800, Loss 0.030142\n",
      "Train ::  Epoch: 76, Batch 900, Loss 0.038716\n",
      "Train ::  Epoch: 76, Batch 1000, Loss 0.024197\n",
      "Train ::  Epoch: 77, Batch 0, Loss 0.037570\n",
      "Train ::  Epoch: 77, Batch 100, Loss 0.024599\n",
      "Train ::  Epoch: 77, Batch 200, Loss 0.025530\n",
      "Train ::  Epoch: 77, Batch 300, Loss 0.024515\n",
      "Train ::  Epoch: 77, Batch 400, Loss 0.029237\n",
      "Train ::  Epoch: 77, Batch 500, Loss 0.026881\n",
      "Train ::  Epoch: 77, Batch 600, Loss 0.035992\n",
      "Train ::  Epoch: 77, Batch 700, Loss 0.037709\n",
      "Train ::  Epoch: 77, Batch 800, Loss 0.023966\n",
      "Train ::  Epoch: 77, Batch 900, Loss 0.032428\n",
      "Train ::  Epoch: 77, Batch 1000, Loss 0.035453\n",
      "Train ::  Epoch: 78, Batch 0, Loss 0.030238\n",
      "Train ::  Epoch: 78, Batch 100, Loss 0.033566\n",
      "Train ::  Epoch: 78, Batch 200, Loss 0.030651\n",
      "Train ::  Epoch: 78, Batch 300, Loss 0.032566\n",
      "Train ::  Epoch: 78, Batch 400, Loss 0.030644\n",
      "Train ::  Epoch: 78, Batch 500, Loss 0.030953\n",
      "Train ::  Epoch: 78, Batch 600, Loss 0.039851\n",
      "Train ::  Epoch: 78, Batch 700, Loss 0.035894\n",
      "Train ::  Epoch: 78, Batch 800, Loss 0.029901\n",
      "Train ::  Epoch: 78, Batch 900, Loss 0.025026\n",
      "Train ::  Epoch: 78, Batch 1000, Loss 0.039030\n",
      "Train ::  Epoch: 79, Batch 0, Loss 0.023036\n",
      "Train ::  Epoch: 79, Batch 100, Loss 0.031841\n",
      "Train ::  Epoch: 79, Batch 200, Loss 0.035133\n",
      "Train ::  Epoch: 79, Batch 300, Loss 0.028698\n",
      "Train ::  Epoch: 79, Batch 400, Loss 0.021891\n",
      "Train ::  Epoch: 79, Batch 500, Loss 0.032683\n",
      "Train ::  Epoch: 79, Batch 600, Loss 0.036910\n",
      "Train ::  Epoch: 79, Batch 700, Loss 0.032106\n",
      "Train ::  Epoch: 79, Batch 800, Loss 0.028493\n",
      "Train ::  Epoch: 79, Batch 900, Loss 0.025602\n",
      "Train ::  Epoch: 79, Batch 1000, Loss 0.030598\n",
      "Train ::  Epoch: 80, Batch 0, Loss 0.030559\n",
      "Train ::  Epoch: 80, Batch 100, Loss 0.034883\n",
      "Train ::  Epoch: 80, Batch 200, Loss 0.033327\n",
      "Train ::  Epoch: 80, Batch 300, Loss 0.032040\n",
      "Train ::  Epoch: 80, Batch 400, Loss 0.027071\n",
      "Train ::  Epoch: 80, Batch 500, Loss 0.031775\n",
      "Train ::  Epoch: 80, Batch 600, Loss 0.034838\n",
      "Train ::  Epoch: 80, Batch 700, Loss 0.032723\n",
      "Train ::  Epoch: 80, Batch 800, Loss 0.029878\n",
      "Train ::  Epoch: 80, Batch 900, Loss 0.034528\n",
      "Train ::  Epoch: 80, Batch 1000, Loss 0.026734\n",
      "Train ::  Epoch: 81, Batch 0, Loss 0.026784\n",
      "Train ::  Epoch: 81, Batch 100, Loss 0.026284\n",
      "Train ::  Epoch: 81, Batch 200, Loss 0.028278\n",
      "Train ::  Epoch: 81, Batch 300, Loss 0.027763\n",
      "Train ::  Epoch: 81, Batch 400, Loss 0.030275\n",
      "Train ::  Epoch: 81, Batch 500, Loss 0.028345\n",
      "Train ::  Epoch: 81, Batch 600, Loss 0.028534\n",
      "Train ::  Epoch: 81, Batch 700, Loss 0.026827\n",
      "Train ::  Epoch: 81, Batch 800, Loss 0.031150\n",
      "Train ::  Epoch: 81, Batch 900, Loss 0.022580\n",
      "Train ::  Epoch: 81, Batch 1000, Loss 0.027439\n",
      "Train ::  Epoch: 82, Batch 0, Loss 0.026849\n",
      "Train ::  Epoch: 82, Batch 100, Loss 0.030152\n",
      "Train ::  Epoch: 82, Batch 200, Loss 0.030819\n",
      "Train ::  Epoch: 82, Batch 300, Loss 0.033482\n",
      "Train ::  Epoch: 82, Batch 400, Loss 0.035829\n",
      "Train ::  Epoch: 82, Batch 500, Loss 0.036220\n",
      "Train ::  Epoch: 82, Batch 600, Loss 0.030988\n",
      "Train ::  Epoch: 82, Batch 700, Loss 0.027836\n",
      "Train ::  Epoch: 82, Batch 800, Loss 0.058114\n",
      "Train ::  Epoch: 82, Batch 900, Loss 0.025899\n",
      "Train ::  Epoch: 82, Batch 1000, Loss 0.025471\n",
      "Train ::  Epoch: 83, Batch 0, Loss 0.028932\n",
      "Train ::  Epoch: 83, Batch 100, Loss 0.033016\n",
      "Train ::  Epoch: 83, Batch 200, Loss 0.031638\n",
      "Train ::  Epoch: 83, Batch 300, Loss 0.028084\n",
      "Train ::  Epoch: 83, Batch 400, Loss 0.025170\n",
      "Train ::  Epoch: 83, Batch 500, Loss 0.028372\n",
      "Train ::  Epoch: 83, Batch 600, Loss 0.034405\n",
      "Train ::  Epoch: 83, Batch 700, Loss 0.028108\n",
      "Train ::  Epoch: 83, Batch 800, Loss 0.036577\n",
      "Train ::  Epoch: 83, Batch 900, Loss 0.034980\n",
      "Train ::  Epoch: 83, Batch 1000, Loss 0.036598\n",
      "Train ::  Epoch: 84, Batch 0, Loss 0.027384\n",
      "Train ::  Epoch: 84, Batch 100, Loss 0.030244\n",
      "Train ::  Epoch: 84, Batch 200, Loss 0.027531\n",
      "Train ::  Epoch: 84, Batch 300, Loss 0.030421\n",
      "Train ::  Epoch: 84, Batch 400, Loss 0.028179\n",
      "Train ::  Epoch: 84, Batch 500, Loss 0.034226\n",
      "Train ::  Epoch: 84, Batch 600, Loss 0.026771\n",
      "Train ::  Epoch: 84, Batch 700, Loss 0.032257\n",
      "Train ::  Epoch: 84, Batch 800, Loss 0.031767\n",
      "Train ::  Epoch: 84, Batch 900, Loss 0.027861\n",
      "Train ::  Epoch: 84, Batch 1000, Loss 0.028843\n",
      "Train ::  Epoch: 85, Batch 0, Loss 0.031589\n",
      "Train ::  Epoch: 85, Batch 100, Loss 0.027171\n",
      "Train ::  Epoch: 85, Batch 200, Loss 0.029246\n",
      "Train ::  Epoch: 85, Batch 300, Loss 0.041168\n",
      "Train ::  Epoch: 85, Batch 400, Loss 0.029952\n",
      "Train ::  Epoch: 85, Batch 500, Loss 0.032357\n",
      "Train ::  Epoch: 85, Batch 600, Loss 0.027747\n",
      "Train ::  Epoch: 85, Batch 700, Loss 0.037856\n",
      "Train ::  Epoch: 85, Batch 800, Loss 0.035364\n",
      "Train ::  Epoch: 85, Batch 900, Loss 0.030124\n",
      "Train ::  Epoch: 85, Batch 1000, Loss 0.036650\n",
      "Train ::  Epoch: 86, Batch 0, Loss 0.024924\n",
      "Train ::  Epoch: 86, Batch 100, Loss 0.029620\n",
      "Train ::  Epoch: 86, Batch 200, Loss 0.025569\n",
      "Train ::  Epoch: 86, Batch 300, Loss 0.028538\n",
      "Train ::  Epoch: 86, Batch 400, Loss 0.028202\n",
      "Train ::  Epoch: 86, Batch 500, Loss 0.027301\n",
      "Train ::  Epoch: 86, Batch 600, Loss 0.033050\n",
      "Train ::  Epoch: 86, Batch 700, Loss 0.030441\n",
      "Train ::  Epoch: 86, Batch 800, Loss 0.029933\n",
      "Train ::  Epoch: 86, Batch 900, Loss 0.029071\n",
      "Train ::  Epoch: 86, Batch 1000, Loss 0.033065\n",
      "Train ::  Epoch: 87, Batch 0, Loss 0.025392\n",
      "Train ::  Epoch: 87, Batch 100, Loss 0.029047\n",
      "Train ::  Epoch: 87, Batch 200, Loss 0.024798\n",
      "Train ::  Epoch: 87, Batch 300, Loss 0.027235\n",
      "Train ::  Epoch: 87, Batch 400, Loss 0.028390\n",
      "Train ::  Epoch: 87, Batch 500, Loss 0.032994\n",
      "Train ::  Epoch: 87, Batch 600, Loss 0.030788\n",
      "Train ::  Epoch: 87, Batch 700, Loss 0.036768\n",
      "Train ::  Epoch: 87, Batch 800, Loss 0.034736\n",
      "Train ::  Epoch: 87, Batch 900, Loss 0.033473\n",
      "Train ::  Epoch: 87, Batch 1000, Loss 0.027198\n",
      "Train ::  Epoch: 88, Batch 0, Loss 0.031883\n",
      "Train ::  Epoch: 88, Batch 100, Loss 0.026140\n",
      "Train ::  Epoch: 88, Batch 200, Loss 0.027260\n",
      "Train ::  Epoch: 88, Batch 300, Loss 0.027820\n",
      "Train ::  Epoch: 88, Batch 400, Loss 0.024025\n",
      "Train ::  Epoch: 88, Batch 500, Loss 0.031907\n",
      "Train ::  Epoch: 88, Batch 600, Loss 0.029595\n",
      "Train ::  Epoch: 88, Batch 700, Loss 0.026074\n",
      "Train ::  Epoch: 88, Batch 800, Loss 0.027583\n",
      "Train ::  Epoch: 88, Batch 900, Loss 0.034165\n",
      "Train ::  Epoch: 88, Batch 1000, Loss 0.028125\n",
      "Train ::  Epoch: 89, Batch 0, Loss 0.028479\n",
      "Train ::  Epoch: 89, Batch 100, Loss 0.033251\n",
      "Train ::  Epoch: 89, Batch 200, Loss 0.030636\n",
      "Train ::  Epoch: 89, Batch 300, Loss 0.022813\n",
      "Train ::  Epoch: 89, Batch 400, Loss 0.029183\n",
      "Train ::  Epoch: 89, Batch 500, Loss 0.029381\n",
      "Train ::  Epoch: 89, Batch 600, Loss 0.029905\n",
      "Train ::  Epoch: 89, Batch 700, Loss 0.027292\n",
      "Train ::  Epoch: 89, Batch 800, Loss 0.027040\n",
      "Train ::  Epoch: 89, Batch 900, Loss 0.032503\n",
      "Train ::  Epoch: 89, Batch 1000, Loss 0.034281\n",
      "Train ::  Epoch: 90, Batch 0, Loss 0.032484\n",
      "Train ::  Epoch: 90, Batch 100, Loss 0.022885\n",
      "Train ::  Epoch: 90, Batch 200, Loss 0.030694\n",
      "Train ::  Epoch: 90, Batch 300, Loss 0.024608\n",
      "Train ::  Epoch: 90, Batch 400, Loss 0.029649\n",
      "Train ::  Epoch: 90, Batch 500, Loss 0.028844\n",
      "Train ::  Epoch: 90, Batch 600, Loss 0.031993\n",
      "Train ::  Epoch: 90, Batch 700, Loss 0.028601\n",
      "Train ::  Epoch: 90, Batch 800, Loss 0.032588\n",
      "Train ::  Epoch: 90, Batch 900, Loss 0.036626\n",
      "Train ::  Epoch: 90, Batch 1000, Loss 0.027610\n",
      "Train ::  Epoch: 91, Batch 0, Loss 0.024866\n",
      "Train ::  Epoch: 91, Batch 100, Loss 0.027452\n",
      "Train ::  Epoch: 91, Batch 200, Loss 0.023838\n",
      "Train ::  Epoch: 91, Batch 300, Loss 0.027401\n",
      "Train ::  Epoch: 91, Batch 400, Loss 0.029955\n",
      "Train ::  Epoch: 91, Batch 500, Loss 0.026492\n",
      "Train ::  Epoch: 91, Batch 600, Loss 0.033402\n",
      "Train ::  Epoch: 91, Batch 700, Loss 0.029991\n",
      "Train ::  Epoch: 91, Batch 800, Loss 0.038974\n",
      "Train ::  Epoch: 91, Batch 900, Loss 0.034410\n",
      "Train ::  Epoch: 91, Batch 1000, Loss 0.030886\n",
      "Train ::  Epoch: 92, Batch 0, Loss 0.030200\n",
      "Train ::  Epoch: 92, Batch 100, Loss 0.029245\n",
      "Train ::  Epoch: 92, Batch 200, Loss 0.027994\n",
      "Train ::  Epoch: 92, Batch 300, Loss 0.030456\n",
      "Train ::  Epoch: 92, Batch 400, Loss 0.028117\n",
      "Train ::  Epoch: 92, Batch 500, Loss 0.022162\n",
      "Train ::  Epoch: 92, Batch 600, Loss 0.027765\n",
      "Train ::  Epoch: 92, Batch 700, Loss 0.029715\n",
      "Train ::  Epoch: 92, Batch 800, Loss 0.027756\n",
      "Train ::  Epoch: 92, Batch 900, Loss 0.029269\n",
      "Train ::  Epoch: 92, Batch 1000, Loss 0.026653\n",
      "Train ::  Epoch: 93, Batch 0, Loss 0.034743\n",
      "Train ::  Epoch: 93, Batch 100, Loss 0.031832\n",
      "Train ::  Epoch: 93, Batch 200, Loss 0.030489\n",
      "Train ::  Epoch: 93, Batch 300, Loss 0.025859\n",
      "Train ::  Epoch: 93, Batch 400, Loss 0.028812\n",
      "Train ::  Epoch: 93, Batch 500, Loss 0.023647\n",
      "Train ::  Epoch: 93, Batch 600, Loss 0.033642\n",
      "Train ::  Epoch: 93, Batch 700, Loss 0.027786\n",
      "Train ::  Epoch: 93, Batch 800, Loss 0.033104\n",
      "Train ::  Epoch: 93, Batch 900, Loss 0.030159\n",
      "Train ::  Epoch: 93, Batch 1000, Loss 0.027391\n",
      "Train ::  Epoch: 94, Batch 0, Loss 0.027914\n",
      "Train ::  Epoch: 94, Batch 100, Loss 0.022993\n",
      "Train ::  Epoch: 94, Batch 200, Loss 0.024682\n",
      "Train ::  Epoch: 94, Batch 300, Loss 0.025780\n",
      "Train ::  Epoch: 94, Batch 400, Loss 0.025293\n",
      "Train ::  Epoch: 94, Batch 500, Loss 0.029345\n",
      "Train ::  Epoch: 94, Batch 600, Loss 0.032710\n",
      "Train ::  Epoch: 94, Batch 700, Loss 0.026641\n",
      "Train ::  Epoch: 94, Batch 800, Loss 0.029645\n",
      "Train ::  Epoch: 94, Batch 900, Loss 0.026673\n",
      "Train ::  Epoch: 94, Batch 1000, Loss 0.029517\n",
      "Train ::  Epoch: 95, Batch 0, Loss 0.028939\n",
      "Train ::  Epoch: 95, Batch 100, Loss 0.027607\n",
      "Train ::  Epoch: 95, Batch 200, Loss 0.024013\n",
      "Train ::  Epoch: 95, Batch 300, Loss 0.033355\n",
      "Train ::  Epoch: 95, Batch 400, Loss 0.022744\n",
      "Train ::  Epoch: 95, Batch 500, Loss 0.033316\n",
      "Train ::  Epoch: 95, Batch 600, Loss 0.032543\n",
      "Train ::  Epoch: 95, Batch 700, Loss 0.031828\n",
      "Train ::  Epoch: 95, Batch 800, Loss 0.029878\n",
      "Train ::  Epoch: 95, Batch 900, Loss 0.029076\n",
      "Train ::  Epoch: 95, Batch 1000, Loss 0.031331\n",
      "Train ::  Epoch: 96, Batch 0, Loss 0.029778\n",
      "Train ::  Epoch: 96, Batch 100, Loss 0.025399\n",
      "Train ::  Epoch: 96, Batch 200, Loss 0.026599\n",
      "Train ::  Epoch: 96, Batch 300, Loss 0.029208\n",
      "Train ::  Epoch: 96, Batch 400, Loss 0.029578\n",
      "Train ::  Epoch: 96, Batch 500, Loss 0.023372\n",
      "Train ::  Epoch: 96, Batch 600, Loss 0.022264\n",
      "Train ::  Epoch: 96, Batch 700, Loss 0.035805\n",
      "Train ::  Epoch: 96, Batch 800, Loss 0.030626\n",
      "Train ::  Epoch: 96, Batch 900, Loss 0.028971\n",
      "Train ::  Epoch: 96, Batch 1000, Loss 0.024441\n",
      "Train ::  Epoch: 97, Batch 0, Loss 0.026105\n",
      "Train ::  Epoch: 97, Batch 100, Loss 0.023585\n",
      "Train ::  Epoch: 97, Batch 200, Loss 0.026020\n",
      "Train ::  Epoch: 97, Batch 300, Loss 0.033847\n",
      "Train ::  Epoch: 97, Batch 400, Loss 0.038944\n",
      "Train ::  Epoch: 97, Batch 500, Loss 0.044752\n",
      "Train ::  Epoch: 97, Batch 600, Loss 0.032713\n",
      "Train ::  Epoch: 97, Batch 700, Loss 0.035538\n",
      "Train ::  Epoch: 97, Batch 800, Loss 0.032841\n",
      "Train ::  Epoch: 97, Batch 900, Loss 0.035485\n",
      "Train ::  Epoch: 97, Batch 1000, Loss 0.031108\n",
      "Train ::  Epoch: 98, Batch 0, Loss 0.026855\n",
      "Train ::  Epoch: 98, Batch 100, Loss 0.024240\n",
      "Train ::  Epoch: 98, Batch 200, Loss 0.025568\n",
      "Train ::  Epoch: 98, Batch 300, Loss 0.027439\n",
      "Train ::  Epoch: 98, Batch 400, Loss 0.028125\n",
      "Train ::  Epoch: 98, Batch 500, Loss 0.042870\n",
      "Train ::  Epoch: 98, Batch 600, Loss 0.031894\n",
      "Train ::  Epoch: 98, Batch 700, Loss 0.024474\n",
      "Train ::  Epoch: 98, Batch 800, Loss 0.032399\n",
      "Train ::  Epoch: 98, Batch 900, Loss 0.034433\n",
      "Train ::  Epoch: 98, Batch 1000, Loss 0.033310\n",
      "Train ::  Epoch: 99, Batch 0, Loss 0.023111\n",
      "Train ::  Epoch: 99, Batch 100, Loss 0.028214\n",
      "Train ::  Epoch: 99, Batch 200, Loss 0.026732\n",
      "Train ::  Epoch: 99, Batch 300, Loss 0.024514\n",
      "Train ::  Epoch: 99, Batch 400, Loss 0.032033\n",
      "Train ::  Epoch: 99, Batch 500, Loss 0.025263\n",
      "Train ::  Epoch: 99, Batch 600, Loss 0.030627\n",
      "Train ::  Epoch: 99, Batch 700, Loss 0.030441\n",
      "Train ::  Epoch: 99, Batch 800, Loss 0.031486\n",
      "Train ::  Epoch: 99, Batch 900, Loss 0.026819\n",
      "Train ::  Epoch: 99, Batch 1000, Loss 0.023055\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Shuffle\n",
    "    ind_list = list(range(train_x.shape[0]))\n",
    "    shuffle(ind_list)\n",
    "    _train_x = train_x[ind_list,:]\n",
    "    _y = X_ij[ind_list,:,:]\n",
    "    _y = _y+1\n",
    "    \n",
    "    for batch_idx in range(num_batches+1):\n",
    "        _x_pos = _train_x[batch_idx*bs:(batch_idx+1)*bs]\n",
    "        _y_true = _y[batch_idx*bs:(batch_idx+1)*bs]\n",
    "        # feed tensor\n",
    "        _x_pos = torch.LongTensor(_x_pos)\n",
    "        _y_true = torch.FloatTensor(_y_true)\n",
    "        # ----- #\n",
    "        optimizer.zero_grad()\n",
    "        output = net(_x_pos)\n",
    "       \n",
    "        loss = criterion(output, _y_true)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # ----- #\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train ::  Epoch: {}, Batch {}, Loss {:4f}'.format(epoch, batch_idx,loss))\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(548, 64)\n",
      "(5113, 64)\n",
      "(95, 64)\n",
      "(238, 64)\n",
      "(64, 64)\n",
      "(113, 64)\n",
      "(116, 64)\n",
      "(6193, 64)\n"
     ]
    }
   ],
   "source": [
    "weight_dict = OrderedDict({})\n",
    "for i in range(len(domain_dims)):\n",
    "    arr = (net.list_W_m[i].weight.detach().numpy() + net.list_W_c[i].weight.detach().numpy())/2\n",
    "    print(arr.shape)\n",
    "    _D = list(domain_dims.items())[i][0]\n",
    "    weight_dict[_D] = arr\n",
    "\n",
    "f_path = os.path.join(modelData_SaveDir,'emb_v2_weight_dict.pkl')\n",
    "with open(f_path,'wb') as fh:\n",
    "    pickle.dump(weight_dict, fh, pickle.HIGHEST_PROTOCOL)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(weight_dict):\n",
    "    hscodeList = [10,20,25,35,45,50,55,75,80,90]\n",
    "\n",
    "    for hscode in hscodeList:\n",
    "        print('-----> ::: ',hscode)\n",
    "        # find the 10 closest  to ShipmentDestination to HSCode in data\n",
    "        df = train_df.loc[train_df['HSCode'] == hscode]\n",
    "        df = df.groupby(['HSCode', 'ShipmentOrigin']).size().reset_index(name='counts')\n",
    "        df = df.sort_values(by=['counts'])\n",
    "       \n",
    "        k_closest = df.tail(10)['ShipmentOrigin'].values\n",
    "        print(k_closest)\n",
    "\n",
    "        # hs_code_vec = wt[0][hscode] + bias[0][hscode]\n",
    "        hs_code_vec = weight_dict['HSCode'][hscode]\n",
    "\n",
    "        shp_dest_vec = []\n",
    "        wt = weight_dict['ShipmentOrigin']\n",
    "        for i in range(wt.shape[0]):\n",
    "            r = wt[i]\n",
    "            shp_dest_vec.append(r)\n",
    "\n",
    "        res = {}\n",
    "        for i in range(wt.shape[0]):\n",
    "            a = np.reshape(shp_dest_vec[i], [1, -1])\n",
    "            b = np.reshape(hs_code_vec, [1, -1])\n",
    "            res[i] = np.dot(a, np.transpose(b))[0][0]\n",
    "\n",
    "        new_df = pd.DataFrame(list(res.items()))\n",
    "        new_df = new_df.sort_values(by=[1])\n",
    "       \n",
    "        new_df = new_df.tail(10)\n",
    "        print(list(new_df[0]))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----> :::  10\n",
      "[46  8 83]\n",
      "[111, 14, 53, 100, 75, 83, 99, 20, 41, 79]\n",
      "-----> :::  20\n",
      "[46 34 21 85 22 64 81  7  8 11]\n",
      "[11, 51, 34, 38, 8, 76, 22, 7, 64, 46]\n",
      "-----> :::  25\n",
      "[ 19 114  30  85  66  51  22  81  40  11]\n",
      "[90, 108, 93, 27, 33, 22, 81, 11, 52, 40]\n",
      "-----> :::  35\n",
      "[66 81 11 30]\n",
      "[41, 32, 69, 56, 4, 99, 79, 33, 3, 0]\n",
      "-----> :::  45\n",
      "[ 7 66 30 93 19 85 81 55 21 11]\n",
      "[49, 46, 29, 24, 55, 114, 43, 81, 21, 11]\n",
      "-----> :::  50\n",
      "[23  7 81 40 30 87 21 22 85 11]\n",
      "[7, 23, 40, 81, 30, 22, 21, 87, 11, 85]\n",
      "-----> :::  55\n",
      "[113 115  22   8  21  81  64  30  75  11]\n",
      "[9, 95, 34, 1, 76, 102, 83, 6, 14, 33]\n",
      "-----> :::  75\n",
      "[ 30  40  76 115  11  81  33]\n",
      "[83, 108, 96, 9, 15, 88, 45, 75, 78, 33]\n",
      "-----> :::  80\n",
      "[43  8 57 40 46 11 92]\n",
      "[41, 36, 96, 102, 98, 34, 74, 47, 75, 92]\n",
      "-----> :::  90\n",
      "[ 80   7 115  85  22 113  38   8  11  66]\n",
      "[98, 47, 87, 90, 45, 39, 38, 109, 113, 66]\n"
     ]
    }
   ],
   "source": [
    "test(weight_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adj_embeddings = {}\n",
    "domains_list = list(domain_dims.keys())\n",
    "MI_ij_dict = {}\n",
    "\n",
    "for i in range(num_domains):\n",
    "    for j in range(i+1, num_domains):\n",
    "        _key = domains_list[i] + '_+_' + domains_list[j]\n",
    "        MI_ij_dict[_key] = MI_domain_ij_scaled[i][j]\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "for i in range(num_domains):\n",
    "    domain_i = domains_list[i]\n",
    "   \n",
    "    adj_embeddings[domain_i] = np.zeros(\n",
    "        weight_dict[domain_i].shape\n",
    "    )\n",
    "\n",
    "    domain_dim = domain_dims[domain_i]\n",
    "    \n",
    "    # -------------\n",
    "    # For each entity in domain i\n",
    "    # -------------\n",
    "    for entity_id in range(domain_dim):\n",
    "        res = 0\n",
    "        # For each entity in domain j != i\n",
    "        for domain_j in domains:\n",
    "            if domain_j == domain_i: \n",
    "               \n",
    "                continue\n",
    "            pair = sorted([domain_i, domain_j])\n",
    "            \n",
    "            key = '_+_'.join(pair)\n",
    "            mi_val = MI_ij_dict[key]\n",
    "            \n",
    "            coOcc_matrix = coOccMatrix_dict[key]\n",
    "            if domain_i == pair[0]:\n",
    "                arr = coOcc_matrix[entity_id, :]\n",
    "            else:\n",
    "                arr = coOcc_matrix[:, entity_id]\n",
    "             \n",
    " \n",
    "            sum_co_occ = max(np.sum(arr), 1)\n",
    "            scale = np.reshape(arr  / sum_co_occ, [-1, 1]) \n",
    "            scale = np.power(scale, 2)\n",
    "            \n",
    "            emb_domain_j = weight_dict[domain_j]\n",
    "            res_j = np.sum(scale  * emb_domain_j, axis=0)\n",
    "            res = res + res_j\n",
    "\n",
    "        res = 0.5 * (res + weight_dict[domain_i][entity_id])\n",
    "        adj_embeddings[domain_i][entity_id] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----> :::  10\n",
      "[46  8 83]\n",
      "[115, 40, 59, 57, 28, 81, 98, 91, 55, 76]\n",
      "-----> :::  25\n",
      "[ 19 114  30  85  66  51  22  81  40  11]\n",
      "[87, 21, 113, 85, 40, 7, 22, 77, 81, 11]\n",
      "-----> :::  35\n",
      "[66 81 11 30]\n",
      "[6, 54, 68, 11, 73, 63, 0, 10, 26, 18]\n",
      "-----> :::  40\n",
      "[98 22 11 66  8 30 46  7 49 64]\n",
      "[39, 87, 7, 11, 85, 23, 64, 77, 49, 113]\n",
      "-----> :::  50\n",
      "[23  7 81 40 30 87 21 22 85 11]\n",
      "[113, 11, 40, 81, 21, 23, 77, 85, 22, 87]\n",
      "-----> :::  55\n",
      "[113 115  22   8  21  81  64  30  75  11]\n",
      "[80, 22, 81, 49, 30, 56, 87, 11, 77, 113]\n",
      "-----> :::  75\n",
      "[ 30  40  76 115  11  81  33]\n",
      "[33, 92, 39, 57, 108, 111, 43, 23, 7, 22]\n",
      "-----> :::  90\n",
      "[ 80   7 115  85  22 113  38   8  11  66]\n",
      "[87, 7, 11, 66, 85, 23, 22, 56, 77, 113]\n"
     ]
    }
   ],
   "source": [
    "test(adj_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['Carrier_+_ConsigneePanjivaID', 'Carrier_+_HSCode', 'Carrier_+_PortOfLading', 'Carrier_+_PortOfUnlading', 'Carrier_+_ShipmentDestination', 'Carrier_+_ShipmentOrigin', 'Carrier_+_ShipperPanjivaID', 'ConsigneePanjivaID_+_HSCode', 'ConsigneePanjivaID_+_PortOfLading', 'ConsigneePanjivaID_+_PortOfUnlading', 'ConsigneePanjivaID_+_ShipmentDestination', 'ConsigneePanjivaID_+_ShipmentOrigin', 'ConsigneePanjivaID_+_ShipperPanjivaID', 'HSCode_+_PortOfLading', 'HSCode_+_PortOfUnlading', 'HSCode_+_ShipmentDestination', 'HSCode_+_ShipmentOrigin', 'HSCode_+_ShipperPanjivaID', 'PortOfLading_+_PortOfUnlading', 'PortOfLading_+_ShipmentDestination', 'PortOfLading_+_ShipmentOrigin', 'PortOfLading_+_ShipperPanjivaID', 'PortOfUnlading_+_ShipmentDestination', 'PortOfUnlading_+_ShipmentOrigin', 'PortOfUnlading_+_ShipperPanjivaID', 'ShipmentDestination_+_ShipmentOrigin', 'ShipmentDestination_+_ShipperPanjivaID', 'ShipmentOrigin_+_ShipperPanjivaID'])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coOccMatrix_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.,  2.,  2., ...,  9.,  2.,  2.],\n",
       "       [ 2.,  8.,  2., ...,  2.,  2.,  2.],\n",
       "       [ 2.,  2.,  2., ...,  2.,  2.,  2.],\n",
       "       ...,\n",
       "       [ 2.,  2.,  2., ...,  2.,  2.,  2.],\n",
       "       [ 2.,  2.,  2., ..., 20.,  2.,  2.],\n",
       "       [ 2.,  2.,  2., ...,  2.,  2.,  2.]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.6036e+04 0.0000e+00 3.8000e+01 ... 0.0000e+00 3.0000e+00 1.0000e+01]\n",
      " [1.0000e+01 0.0000e+00 7.1000e+01 ... 0.0000e+00 1.0000e+00 0.0000e+00]\n",
      " [0.0000e+00 0.0000e+00 0.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      " ...\n",
      " [0.0000e+00 0.0000e+00 0.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      " [0.0000e+00 0.0000e+00 0.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      " [0.0000e+00 0.0000e+00 0.0000e+00 ... 0.0000e+00 0.0000e+00 0.0000e+00]]\n",
      "97.0\n"
     ]
    }
   ],
   "source": [
    "print(coOccMatrix_dict['PortOfUnlading_+_ShipmentDestination'])\n",
    "print(coOccMatrix_dict['PortOfUnlading_+_ShipmentDestination'][48][57])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.35755005"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = weight_dict['HSCode'][48]\n",
    "b = weight_dict['ShipmentOrigin'][57]\n",
    "np.dot(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "X = weight_dict['PortOfLading']\n",
    "X_embedded = TSNE(n_components=2, perplexity=8.0).fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "fig=plt.figure()\n",
    "ax=fig.add_axes([0,0,1,1])\n",
    "\n",
    "ax.scatter(X_embedded[:,0],X_embedded[:,1],color='g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
