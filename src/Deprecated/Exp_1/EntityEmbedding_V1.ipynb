{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ddatta/anaconda3/envs/AD_v2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ddatta/anaconda3/envs/AD_v2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ddatta/anaconda3/envs/AD_v2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ddatta/anaconda3/envs/AD_v2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ddatta/anaconda3/envs/AD_v2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ddatta/anaconda3/envs/AD_v2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/ddatta/anaconda3/envs/AD_v2/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ddatta/anaconda3/envs/AD_v2/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ddatta/anaconda3/envs/AD_v2/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ddatta/anaconda3/envs/AD_v2/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ddatta/anaconda3/envs/AD_v2/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ddatta/anaconda3/envs/AD_v2/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-72dc0a20e498>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import os\n",
    "from keras import Model\n",
    "from keras.layers import Input, Embedding, Dot, Reshape, Add\n",
    "from keras.layers import Lambda\n",
    "import sys\n",
    "import os\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from collections import OrderedDict\n",
    "import pickle\n",
    "import logging\n",
    "import multiprocessing as mp\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "try:\n",
    "    from . import utils_1\n",
    "except:\n",
    "    import utils_1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ij_max = None\n",
    "# =================================\n",
    "# Co-occurrence based embedding model\n",
    "# Projecting GloVe to multivariate categorical \n",
    "# =================================\n",
    "\n",
    "\n",
    "def get_model(\n",
    "    domain_dimesnsions = None,\n",
    "    num_domains = 4,\n",
    "    embed_dim = 16,\n",
    "    _X_ij_max = None\n",
    "):\n",
    "\n",
    "    global X_ij_max\n",
    "    X_ij_max = _X_ij_max\n",
    "    embedding_layer = []\n",
    "    bias_layer = []\n",
    "\n",
    "    input_layer = Input(\n",
    "        shape=(num_domains,)\n",
    "    )\n",
    "\n",
    "    # =======================\n",
    "    # Input record\n",
    "    # =======================\n",
    "    split_input_record = Lambda(\n",
    "        lambda x:\n",
    "        tf.split(\n",
    "            x,\n",
    "            num_or_size_splits=num_domains,\n",
    "            axis=-1\n",
    "        ),\n",
    "        name='split_layer'\n",
    "    )(input_layer)\n",
    "    \n",
    "    for i in range(num_domains):\n",
    "        emb_i = Embedding(\n",
    "            input_dim = domain_dimesnsions[i],\n",
    "            output_dim= embed_dim,\n",
    "            embeddings_initializer='random_uniform',\n",
    "            name='embedding_w_'+str(i)\n",
    "        )(split_input_record[i])\n",
    "        embedding_layer.append(emb_i)\n",
    "\n",
    "        bias_i = Embedding(\n",
    "            input_dim = domain_dimesnsions[i],\n",
    "            output_dim=1,\n",
    "            input_length=1,\n",
    "            embeddings_initializer='random_uniform',\n",
    "            name= 'embedding_b_'+str(i)\n",
    "        )(split_input_record[i])\n",
    "        bias_layer.append(bias_i)\n",
    "\n",
    "    y_pred = []\n",
    "\n",
    "    for i in range(num_domains):\n",
    "        for j in range(i+1,num_domains):\n",
    "            w_i__w_j = Dot(axes=-1)([\n",
    "                embedding_layer[i],\n",
    "                embedding_layer[j]\n",
    "            ])\n",
    "            w_i__w_j = Reshape(target_shape=(1,))(w_i__w_j)\n",
    "            pred_logXij = Add()([w_i__w_j, bias_layer[i],bias_layer[j]])\n",
    "            pred_logXij = Reshape(target_shape=(1,))(pred_logXij)\n",
    "            y_pred.append(pred_logXij)\n",
    "\n",
    "    y_pred_stacked = Lambda(\n",
    "        lambda x:\n",
    "        tf.stack(\n",
    "            x,\n",
    "            axis=1\n",
    "        ),\n",
    "        name='stack_layer'\n",
    "    )(y_pred)\n",
    "\n",
    "    y_pred_final = Lambda(\n",
    "        lambda x:\n",
    "        tf.squeeze(\n",
    "            x,\n",
    "            axis=-1\n",
    "        ),\n",
    "        name='squeeze_layer'\n",
    "    )(y_pred_stacked)\n",
    "\n",
    "    model = Model(\n",
    "        input_layer,\n",
    "        y_pred_final\n",
    "    )\n",
    "    model.compile(\n",
    "        loss = custom_loss_function,\n",
    "        optimizer='adam'\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def custom_loss_function(\n",
    "        y_true,\n",
    "        y_pred\n",
    "):\n",
    "    global X_ij_max\n",
    "    a = 0.75\n",
    "    epsilon = 0.000001\n",
    "\n",
    "    _err1 = K.square(y_pred - K.log(y_true + epsilon))\n",
    "    _scale1 = K.pow(\n",
    "        K.clip(y_true / X_ij_max, 0.0, 1.0),\n",
    "        a\n",
    "    )\n",
    "    loss = _scale1 * _err1\n",
    "    return K.sum(\n",
    "        loss,\n",
    "        axis=-1\n",
    "    )\n",
    "\n",
    "def train_model(\n",
    "        model,\n",
    "        x,\n",
    "        y_true,\n",
    "        file_save_loc,\n",
    "        epochs=100\n",
    "):\n",
    "    model.summary()\n",
    "    model.fit(\n",
    "        x=x,\n",
    "        y=y_true,\n",
    "        batch_size=256,\n",
    "        epochs=epochs,\n",
    "        verbose=1,\n",
    "        shuffle=True\n",
    "    )\n",
    "    save_model(model,file_save_loc)\n",
    "\n",
    "    return model\n",
    "\n",
    "def save_model(model, file_save_loc):\n",
    "\n",
    "    for layer in model.layers:\n",
    "        if 'embedding_w' in layer.name:\n",
    "            f_path = os.path.join( file_save_loc, layer.name + \".npy\")\n",
    "            np.save(f_path, arr=layer.get_weights()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Global variables ===================== #\n",
    "CONFIG_FILE = 'config_1.yaml'\n",
    "# ============================================================ #\n",
    "\n",
    "\n",
    "CONFIG_FILE = 'config_1.yaml'\n",
    "DIR = None\n",
    "OP_DIR = None\n",
    "modelData_SaveDir = None\n",
    "DATA_DIR = None\n",
    "num_jobs = None\n",
    "CONFIG = None\n",
    "Refresh_Embeddings = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_config(_DIR=None):\n",
    "    global CONFIG_FILE\n",
    "    global DATA_DIR\n",
    "    global modelData_SaveDir\n",
    "    global OP_DIR\n",
    "    global DIR\n",
    "    global num_jobs\n",
    "    global Refresh_Embeddings\n",
    "    global CONFIG\n",
    "\n",
    "    with open(CONFIG_FILE) as f:\n",
    "        CONFIG = yaml.safe_load(f)\n",
    "    if _DIR is None:\n",
    "        DIR = CONFIG['DIR']\n",
    "    else:\n",
    "        DIR = _DIR\n",
    "\n",
    "    DATA_DIR = os.path.join(CONFIG['DATA_DIR'])\n",
    "\n",
    "    modelData_SaveDir = os.path.join(\n",
    "        CONFIG['model_data_save_dir'],\n",
    "        DIR\n",
    "    )\n",
    "\n",
    "    if not os.path.exists(CONFIG['OP_DIR']):\n",
    "        os.mkdir(CONFIG['OP_DIR'])\n",
    "    OP_DIR = os.path.join(CONFIG['OP_DIR'], DIR)\n",
    "    if not os.path.exists(OP_DIR):\n",
    "        os.mkdir(OP_DIR)\n",
    "\n",
    "    Refresh_Embeddings = CONFIG[DIR]['Refresh_Embeddings']\n",
    "    cpu_count = mp.cpu_count()\n",
    "    num_jobs = min(cpu_count, CONFIG['num_jobs'])\n",
    "\n",
    "    if not os.path.exists(CONFIG['model_data_save_dir']):\n",
    "        os.mkdir(CONFIG['model_data_save_dir'])\n",
    "\n",
    "    if not os.path.exists(modelData_SaveDir):\n",
    "        os.mkdir(modelData_SaveDir)\n",
    "    \n",
    "    print(' Set up config')\n",
    "    return\n",
    "\n",
    "setup_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_coocc_matrix(df, col_1, col_2):\n",
    "    set_elements_1 = set(list(df[col_1]))\n",
    "    set_elements_2 = set(list(df[col_2]))\n",
    "    count_1 = len(set_elements_1)\n",
    "    count_2 = len(set_elements_2)\n",
    "    coocc = np.zeros([count_1, count_2])\n",
    "    df = df[[col_1, col_2]]\n",
    "    new_df = df.groupby([col_1, col_2]).size().reset_index(name='count')\n",
    "\n",
    "    for _, row in new_df.iterrows():\n",
    "        i = row[col_1]\n",
    "        j = row[col_2]\n",
    "        coocc[i][j] = row['count']\n",
    "\n",
    "    print('Col 1 & 2', col_1, col_2, coocc.shape, '>>', (count_1, count_2))\n",
    "    return coocc\n",
    "\n",
    "\n",
    "\n",
    "def get_coOccMatrix_dict(df, id_col):\n",
    "    columns = list(df.columns)\n",
    "    columns.remove(id_col)\n",
    "    columns = list(sorted(columns))\n",
    "    columnWise_coOccMatrix_dict = {}\n",
    "\n",
    "    for i in range(len(columns)):\n",
    "        for j in range(i + 1, len(columns)):\n",
    "            col_1 = columns[i]\n",
    "            col_2 = columns[j]\n",
    "            key = col_1 + '_+_' + col_2\n",
    "            res = create_coocc_matrix(df, col_1, col_2)\n",
    "            columnWise_coOccMatrix_dict[key] = res\n",
    "    columnWise_coOccMatrix_dict = OrderedDict(columnWise_coOccMatrix_dict)\n",
    "    return columnWise_coOccMatrix_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_entity_embeddings(\n",
    "        train_data_file,\n",
    "        model_data_save_dir,\n",
    "        DATA_DIR,\n",
    "        embedding_dims,\n",
    "        num_epochs,\n",
    "        id_col='PanjivaRecordID'\n",
    "):\n",
    "    train_df = pd.read_csv(os.path.join(DATA_DIR, train_data_file))\n",
    "    feature_cols = list(train_df.columns)\n",
    "    feature_cols = list(feature_cols)\n",
    "    feature_cols.remove(id_col)\n",
    "    domains = sorted(feature_cols)\n",
    "    print(feature_cols)\n",
    "\n",
    "    data = train_df[feature_cols].values\n",
    "    # ------------------------------- #\n",
    "    coOcc_dict_file = os.path.join(model_data_save_dir, \"coOccMatrix_dict.pkl\")\n",
    "    X_ij_file = os.path.join(model_data_save_dir, \"X_ij.npy\")\n",
    "    domain_dims_file = os.path.join(DATA_DIR, \"domain_dims.pkl\")\n",
    "    domain_dims = utils_1.get_domain_dims(domain_dims_file)\n",
    "\n",
    "    # -----\n",
    "    # Check if pairwise co-occurrence dictionary exists\n",
    "    # -----\n",
    "    if os.path.exists(coOcc_dict_file):\n",
    "        with open(coOcc_dict_file, 'rb') as fh:\n",
    "            coOccMatrix_dict = pickle.load(fh)\n",
    "    else:\n",
    "        coOccMatrix_dict = get_coOccMatrix_dict(train_df, id_col='PanjivaRecordID')\n",
    "        with open(coOcc_dict_file, \"wb\") as fh:\n",
    "            pickle.dump(coOccMatrix_dict, fh, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "    # ----------------\n",
    "    # Ensure X_ij is in a flattened format ; i < j\n",
    "    # ----------------\n",
    "    if os.path.exists(X_ij_file):\n",
    "        with open(X_ij_file, 'rb') as fh:\n",
    "            X_ij = np.load(fh)\n",
    "\n",
    "    else:\n",
    "        nd = len(feature_cols)\n",
    "        num_c = nd * (nd - 1) // 2\n",
    "        X_ij = np.zeros([data.shape[0], num_c])\n",
    "        k = 0\n",
    "        for i in range(len(feature_cols)):\n",
    "            for j in range(i + 1, len(feature_cols)):\n",
    "                key = feature_cols[i] + '_+_' + feature_cols[j]\n",
    "                for d in range(data.shape[0]):\n",
    "                    e1 = data[d][i]\n",
    "                    e2 = data[d][j]\n",
    "                    X_ij[d][k] = coOccMatrix_dict[key][e1][e2]\n",
    "                k += 1\n",
    "        X_ij = np.asarray(X_ij,np.int32)\n",
    "        with open(X_ij_file, \"wb\") as fh:\n",
    "            np.save(fh, X_ij)\n",
    "\n",
    "    # -------------------------------- #\n",
    "\n",
    "    # X_ij_max needed for scaling\n",
    "    X_ij_max = []\n",
    "    for k, v in coOccMatrix_dict.items():\n",
    "        X_ij_max.append(np.max(v))\n",
    "\n",
    "    num_domains = len(domain_dims)\n",
    "    print(domain_dims.values())\n",
    "\n",
    "    model = get_model(\n",
    "        domain_dimesnsions=list(domain_dims.values()),\n",
    "        num_domains=num_domains,\n",
    "        embed_dim=embedding_dims,\n",
    "        _X_ij_max=X_ij_max\n",
    "    )\n",
    "\n",
    "    # check if model present !!\n",
    "    _present = len(glob.glob(os.path.join(model_data_save_dir, 'embedding_w_**')))>0\n",
    "    if not _present :\n",
    "        model = train_model(\n",
    "            model,\n",
    "            data,\n",
    "            X_ij,\n",
    "            file_save_loc=model_data_save_dir,\n",
    "            epochs=num_epochs\n",
    "        )\n",
    "    # ----\n",
    "    # Save the embeddings (weights) in a dictionary\n",
    "    # ----\n",
    "    emb_w = {}\n",
    "    for i in range(len(feature_cols)):\n",
    "        dom = feature_cols[i]\n",
    "        f_path = os.path.join(model_data_save_dir, 'embedding_w_{}.npy'.format(i))\n",
    "        w = np.load(f_path)\n",
    "        emb_w[dom] = w\n",
    "\n",
    "    # ================== \n",
    "    # Modifying concept of  GloVe\n",
    "    # emb ( entity = E in D)\n",
    "    #  x = 0\n",
    "    #  For d in {Doamian} - D\n",
    "    #     x += Sum (CoOcc( E, E_d`)/max(CoOcc( E, E_d`)) *  emb ( entity = E ))\n",
    "    #  x = 1/2(emb_old(E) + x)\n",
    "    # ==================\n",
    "\n",
    "    new_embeddings = {}\n",
    "    for domain_i in domains:\n",
    "        new_embeddings[domain_i] = np.zeros(\n",
    "            emb_w[domain_i].shape\n",
    "        )\n",
    "\n",
    "        domain_dim = domain_dims[domain_i]\n",
    "        # For each entity in domain i \n",
    "        for entity_id in range(domain_dim):\n",
    "            res = 0\n",
    "            # For each entity in domain j != i\n",
    "            for domain_j in domains:\n",
    "                if domain_j == domain_i: continue\n",
    "                pair = sorted([domain_i, domain_j])\n",
    "\n",
    "                key = '_+_'.join(pair)\n",
    "                coOcc_matrix = coOccMatrix_dict[key]\n",
    "                if domain_i == pair[0]:\n",
    "                    arr = coOcc_matrix[entity_id, :]\n",
    "                else:\n",
    "                    arr = coOcc_matrix[:, entity_id]\n",
    "\n",
    "                sum_co_occ = max(np.sum(arr), 1)\n",
    "                scale = np.reshape(arr / sum_co_occ, [-1, 1])\n",
    "\n",
    "                emb_domain_j = emb_w[domain_j]\n",
    "                res_j = np.sum(scale * scale * emb_domain_j, axis=0)\n",
    "                res = res + res_j\n",
    "\n",
    "            res = 0.5 * (res + emb_w[domain_i][entity_id])\n",
    "#             res = emb_w[domain_i][entity_id]\n",
    "            new_embeddings[domain_i][entity_id] = res\n",
    "\n",
    "    # Write the embeddings to file \n",
    "    for domain_i in domains:\n",
    "        print(' >> ', domain_i)\n",
    "        file_name = os.path.join(\n",
    "            model_data_save_dir,\n",
    "            'init_embedding_' + domain_i + '_' + str(embedding_dims) + '.npy'\n",
    "        )\n",
    "        np.save(\n",
    "            file=file_name,\n",
    "            arr=new_embeddings[domain_i]\n",
    "        )\n",
    "    \n",
    "    # =================================\n",
    "    # This is only for testing whether the model works\n",
    "    # Usually not called, only for debugging\n",
    "    # =================================\n",
    "    def test():\n",
    "        hscodeList = [10,25,35,40,50,55,75,90]\n",
    "        \n",
    "        for hscode in hscodeList:\n",
    "            print('-----> ::: ',hscode)\n",
    "            # find the 10 closest  to ShipmentDestination to HSCode in data\n",
    "            df = train_df.loc[train_df['HSCode'] == hscode]\n",
    "            df = df.groupby(['HSCode', 'PortOfLading']).size().reset_index(name='counts')\n",
    "            df = df.sort_values(by=['counts'])\n",
    "\n",
    "            k_closest = df.tail(10)['PortOfLading'].values\n",
    "            print(k_closest)\n",
    "\n",
    "            # hs_code_vec = wt[0][hscode] + bias[0][hscode]\n",
    "            hs_code_vec = new_embeddings['HSCode'][hscode]\n",
    "\n",
    "            shp_dest_vec = []\n",
    "            wt = new_embeddings['PortOfLading']\n",
    "            for i in range(wt.shape[0]):\n",
    "                r = wt[i]\n",
    "                shp_dest_vec.append(r)\n",
    "\n",
    "            res = {}\n",
    "            for i in range(wt.shape[0]):\n",
    "                a = np.reshape(shp_dest_vec[i], [1, -1])\n",
    "                b = np.reshape(hs_code_vec, [1, -1])\n",
    "                res[i] = cosine_similarity(a, b)\n",
    "\n",
    "            new_df = pd.DataFrame(list(res.items()))\n",
    "            new_df = new_df.sort_values(by=[1])\n",
    "            new_df = new_df.tail(10)\n",
    "            print(list(new_df[0]))\n",
    "            \n",
    "            \n",
    "            print('----->')\n",
    "    test()\n",
    "    \n",
    "    return new_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CONFIG' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d50527d89cd8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining_data_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_data_file'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msrc_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m embeddings = get_initial_entity_embeddings(\n\u001b[1;32m      5\u001b[0m     \u001b[0mtraining_data_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CONFIG' is not defined"
     ]
    }
   ],
   "source": [
    "training_data_file = CONFIG['train_data_file']\n",
    " \n",
    "src_DIR = os.path.join(DATA_DIR, DIR)\n",
    "embeddings = get_initial_entity_embeddings(\n",
    "    training_data_file,\n",
    "    modelData_SaveDir,\n",
    "    src_DIR,\n",
    "    embedding_dims=256,\n",
    "    num_epochs=40\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
