import pandas as pd
import sys
import os
import numpy as np
import pickle
from itertools import combinations
from joblib import Parallel, delayed
from numpy import random
import hashlib
import os
import sys
import pandas as pd
import numpy as np
import sklearn
import glob
import pickle
import random
from joblib import Parallel, delayed
import yaml
import math
from collections import Counter

sys.path.append('.')
sys.path.append('./..')

try:
    from . import utils_createAnomalies as utils_local
except:
    import utils_createAnomalies as utils_local


def check_nonZeroCoOccurrence(
        dict_domain_entities,
        dict_coOccMatrix
):
    domains = sorted(dict_domain_entities.keys())
    for d_pair in combinations(domains, 2):
        d_pair = sorted(d_pair)
        key = '_+_'.join(d_pair)
        e1 = dict_domain_entities[d_pair[0]]
        e2 = dict_domain_entities[d_pair[1]]
        if dict_coOccMatrix[key][e1][e2] == 0:
            return False
    return True


'''
Find patterns ::
{ A,B } !-> { C } 
'''

def find_conflicting_patterns_aux_1(
        train_df,
        dict_coOccMatrix,
        id_col,
        pattern_size=3,
        count=100,
        min_normal_pattern_count = 5
):
    results = []
    domains = list(sorted(train_df.columns))
    domains.remove(id_col)

    # create set of entity ids for each of the domains
    domain_entitiesSet_dict = {}
    for d in domains:
        domain_entitiesSet_dict[d] = list(set(train_df[d]))

    cur_count = 0
    while cur_count < count:
        domain_set = np.random.choice(
            domains,
            size=pattern_size,
            replace=False
        )
        excluded_domain = np.random.choice(domain_set, size=1)[0]
        pos_set = list(domain_set)
        pos_set.remove(excluded_domain)
        candidate_dict = {}

        _tries1 = 0

        #  Find { E_d1, E_d2, ... E_d3 } such that they co-occur pairwise
        while True:
            for d in pos_set:
                # sample entity
                candidate_dict[d] = np.random.choice(domain_entitiesSet_dict[d], size=1)[0]

            if check_nonZeroCoOccurrence(candidate_dict, dict_coOccMatrix) == True:
                break
            _tries1 += 1

        # ======
        # Find patterns with a minimum "support"
        # ======
        if utils_local.find_pattern_count(candidate_dict, train_df) >= min_normal_pattern_count :
            _tries2 = 0
            max_tries = 1000
            condition_satisfied = False
            cur_domain = excluded_domain

            while not condition_satisfied and _tries2 < max_tries:
                # Select an entity
                cand_e = np.random.choice(domain_entitiesSet_dict[cur_domain], size=1)[0]
                candidate_dict[cur_domain] = cand_e
                # ====
                # Ensure that new candidate has non-zero co-occurrence with others
                for dpair in combinations(list(candidate_dict.keys()), 2):
                    subSet_dict = {}
                    subSet_dict[dpair[0]] = candidate_dict[dpair[0]]
                    subSet_dict[dpair[1]] = candidate_dict[dpair[1]]

                    if not check_nonZeroCoOccurrence(subSet_dict, dict_coOccMatrix):
                        condition_satisfied = False
                        break
                    else:
                        condition_satisfied = True
                # ====
                _tries2 += 1

        results.append(
            candidate_dict
        )
    return results


# ========== #

def generate_anomalies_type_2(
        train_df,
        test_df,
        dict_coOccMatrix,
        id_col='PanjivaRecordID',
        pattern_size=4,
        reqd_anom_perc=10,
        num_jobs=40,
        pattern_duplicate_count=100
):
    # =====================
    # Over estimating a bit, so that overlaps can be compensated for
    # =====================
    dist_pattern_count = int(1.25 * len(test_df) * (reqd_anom_perc / 100) / 100)

    list_results = Parallel(n_jobs=num_jobs)(
        delayed(find_conflicting_patterns_aux_1)(
            train_df,
            test_df,
            dict_coOccMatrix,
            id_col,
            pattern_size=pattern_size,
            count=dist_pattern_count
        ) for _ in range(num_jobs)
    )
    results= []
    for item in list_results:
        results.extend(item)

    # Remove duplicate 'spurious' patterns generated by the previous call
    patterns = utils_local.dedup_list_dictionaries(
        results
    )
    result_df = pd.DataFrame(columns=list(train_df.columns))
    # ===================
    # For each pattern create k samples ,
    # where k = pattern_duplicates
    # ===================

    for pattern in patterns:
        # ========
        # select 2 (partial) domains
        # ========
        _domains_set = np.random.choice(list(pattern.keys()),replace=False,size=2)
        cand = {}
        for d in _domains_set:
            cand [d] = pattern[d]

        # ===
        # Find k records with partial match with pattern
        # ===
        match_df = utils_local.query_df(
            test_df,
            cand
        )
        match_df = match_df.sample( int( 1.1 * pattern_duplicate_count) )

        for _, row in match_df.iterrows():
            row_copy = pd.Series(row,copy=True)
            for d,e in pattern.items():
                row_copy[d] =  e

            result_df = result_df.append(
                row_copy, ignore_index=True
            )

    result_df[id_col] = result_df.apply(
        utils_local.aux_modify_id,
        args=('002',)
    )

    return result_df


# ========================
# CONFIG = set_up_config()
# train_df = pd.read_csv(os.path.join(save_dir, CONFIG['train_data_file']))
# dict_coOccMatrix = get_coOccMatrix_dict(train_df, id_col)
# generate_anomalies_type_4(train_df, dict_coOccMatrix, id_col, pattern_size=pattern_size, count=10)
