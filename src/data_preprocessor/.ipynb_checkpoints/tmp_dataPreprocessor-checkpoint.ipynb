{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "from itertools import combinations\n",
    "from joblib import Parallel, delayed\n",
    "from numpy import random\n",
    "import hashlib\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "from joblib import Parallel, delayed\n",
    "import yaml\n",
    "import math\n",
    "from collections import Counter\n",
    "sys.path.append('.')\n",
    "sys.path.append('./..')\n",
    "\n",
    "try:\n",
    "    import clean_up_test_data\n",
    "except:\n",
    "    from . import clean_up_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "CONFIG_FILE = 'config_preprocessor_v02.yaml'\n",
    "id_col = 'PanjivaRecordID'\n",
    "ns_id_col = 'NegSampleID'\n",
    "term_2_col = 'term_2'\n",
    "term_4_col = 'term_4'\n",
    "num_neg_samples_ape = None\n",
    "use_cols = None\n",
    "freq_bound = None\n",
    "column_value_filters = None\n",
    "num_neg_samples = None\n",
    "save_dir = None\n",
    "cleaned_csv_subdir = None\n",
    "\n",
    "\n",
    "def set_up_config():\n",
    "    global CONFIG_FILE\n",
    "    global use_cols\n",
    "    global freq_bound\n",
    "    global num_neg_samples_ape\n",
    "    global DIR\n",
    "    global save_dir\n",
    "    global column_value_filters\n",
    "    global num_neg_samples\n",
    "    global cleaned_csv_subdir\n",
    "\n",
    "    with open(CONFIG_FILE) as f:\n",
    "        CONFIG = yaml.safe_load(f)\n",
    "\n",
    "    DIR = CONFIG['DIR']\n",
    "    save_dir = os.path.join(\n",
    "        CONFIG['save_dir'],\n",
    "        DIR\n",
    "    )\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "\n",
    "    cleaned_csv_subdir = os.path.join(\n",
    "        save_dir,\n",
    "        CONFIG['cleaned_csv_subdir']\n",
    "    )\n",
    "\n",
    "    if not os.path.exists(cleaned_csv_subdir):\n",
    "        os.mkdir(cleaned_csv_subdir)\n",
    "\n",
    "    use_cols = CONFIG[DIR]['use_cols']\n",
    "    freq_bound = CONFIG[DIR]['low_freq_bound']\n",
    "    column_value_filters = CONFIG[DIR]['column_value_filters']\n",
    "    num_neg_samples_ape = CONFIG[DIR]['num_neg_samples_ape']\n",
    "    num_neg_samples = CONFIG[DIR]['num_neg_samples']\n",
    "\n",
    "    return CONFIG\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Append a hash to speed up processing\n",
    "# ==========================\n",
    "def get_hash_aux(row, id_col):\n",
    "    row_dict = row.to_dict()\n",
    "    del row_dict[id_col]\n",
    "    _str = '_'.join([str(_) for _ in row_dict.values()])\n",
    "    _str = str.encode(_str)\n",
    "    str_hash = hashlib.md5(_str).hexdigest()\n",
    "    return str_hash\n",
    "\n",
    "\n",
    "def add_hash(df, id_col):\n",
    "    df['hash'] = df.apply(\n",
    "        get_hash_aux,\n",
    "        axis=1,\n",
    "        args=(id_col,)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def is_duplicate(ref_df, hash_val):\n",
    "    if len(ref_df.loc[ref_df['hash'] == hash_val]) > 0: return True\n",
    "    return False\n",
    "\n",
    "# modify the id_col\n",
    "def aux_modify_id(value, suffix):\n",
    "    return int(str(value) + str(suffix))\n",
    "\n",
    "def find_pattern_count(domainEntity_dict, ref_df):\n",
    "    global id_col\n",
    "    query_str = []\n",
    "\n",
    "    for _c, _i in domainEntity_dict.items():\n",
    "        query_str.append(' ' + _c + ' == ' + str(_i))\n",
    "    query_str = ' & '.join(query_str)\n",
    "    res_query = ref_df.query(query_str)\n",
    "    return len(res_query)\n",
    "\n",
    "def get_coOccMatrix_dict(df, id_col):\n",
    "    columns = list(df.columns)\n",
    "    columns.remove(id_col)\n",
    "    columns = list(sorted(columns))\n",
    "    columnWise_coOccMatrix_dict = {}\n",
    "\n",
    "    for i in range(len(columns)):\n",
    "        for j in range(i + 1, len(columns)):\n",
    "            col_1 = columns[i]\n",
    "            col_2 = columns[j]\n",
    "            key = col_1 + '_+_' + col_2\n",
    "            res = clean_up_test_data.create_coocc_matrix(df, col_1, col_2)\n",
    "            columnWise_coOccMatrix_dict[key] = res\n",
    "    return columnWise_coOccMatrix_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = set_up_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Col 1 & 2 AdminRegion CountryOfSale (492, 205) >> (492, 205)\n",
      "Col 1 & 2 AdminRegion HSCode (492, 130) >> (492, 130)\n",
      "Col 1 & 2 AdminRegion Province (492, 31) >> (492, 31)\n",
      "Col 1 & 2 AdminRegion ShipmentDestination (492, 206) >> (492, 206)\n",
      "Col 1 & 2 AdminRegion ShipperPanjivaID (492, 14088) >> (492, 14088)\n",
      "Col 1 & 2 AdminRegion TradeType (492, 9) >> (492, 9)\n",
      "Col 1 & 2 AdminRegion TransportMethod (492, 6) >> (492, 6)\n",
      "Col 1 & 2 CountryOfSale HSCode (205, 130) >> (205, 130)\n",
      "Col 1 & 2 CountryOfSale Province (205, 31) >> (205, 31)\n",
      "Col 1 & 2 CountryOfSale ShipmentDestination (205, 206) >> (205, 206)\n",
      "Col 1 & 2 CountryOfSale ShipperPanjivaID (205, 14088) >> (205, 14088)\n",
      "Col 1 & 2 CountryOfSale TradeType (205, 9) >> (205, 9)\n",
      "Col 1 & 2 CountryOfSale TransportMethod (205, 6) >> (205, 6)\n",
      "Col 1 & 2 HSCode Province (130, 31) >> (130, 31)\n",
      "Col 1 & 2 HSCode ShipmentDestination (130, 206) >> (130, 206)\n",
      "Col 1 & 2 HSCode ShipperPanjivaID (130, 14088) >> (130, 14088)\n",
      "Col 1 & 2 HSCode TradeType (130, 9) >> (130, 9)\n",
      "Col 1 & 2 HSCode TransportMethod (130, 6) >> (130, 6)\n",
      "Col 1 & 2 Province ShipmentDestination (31, 206) >> (31, 206)\n",
      "Col 1 & 2 Province ShipperPanjivaID (31, 14088) >> (31, 14088)\n",
      "Col 1 & 2 Province TradeType (31, 9) >> (31, 9)\n",
      "Col 1 & 2 Province TransportMethod (31, 6) >> (31, 6)\n",
      "Col 1 & 2 ShipmentDestination ShipperPanjivaID (206, 14088) >> (206, 14088)\n",
      "Col 1 & 2 ShipmentDestination TradeType (206, 9) >> (206, 9)\n",
      "Col 1 & 2 ShipmentDestination TransportMethod (206, 6) >> (206, 6)\n",
      "Col 1 & 2 ShipperPanjivaID TradeType (14088, 9) >> (14088, 9)\n",
      "Col 1 & 2 ShipperPanjivaID TransportMethod (14088, 6) >> (14088, 6)\n",
      "Col 1 & 2 TradeType TransportMethod (9, 6) >> (9, 6)\n"
     ]
    }
   ],
   "source": [
    " dict_coOccMatrix = get_coOccMatrix_dict(train_df, id_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nonZeroCoOccurrence(\n",
    "    dict_domain_entities, \n",
    "    dict_coOccMatrix\n",
    "):\n",
    "    domains = sorted(dict_domain_entities.keys())\n",
    "    for d_pair in combinations(domains,2):\n",
    "        d_pair = sorted(d_pair)\n",
    "        key = '_+_'.join(d_pair)\n",
    "        if dict_coOccMatrix[\n",
    "            dict_domain_entities[d_pair[0]], \n",
    "            dict_domain_entities[d_pair[1]]\n",
    "        ] == 0 : return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def find_conflicting_patterns(train_df, dict_coOccMatrix, id_col, pattern_size = 4, count = 10):\n",
    "        \n",
    "    domains = list(sorted(train_df.columns))\n",
    "    domains.remove(id_col)\n",
    "    # create set of entity ids for each of the domains\n",
    "    domain_entitiesSet_dict = {}\n",
    "    min_pattern_count =  5\n",
    "    for d in domains:\n",
    "        domain_entitiesSet_dict[d] = list(set(train_df[d]))\n",
    "        \n",
    "    anomalies_df = pd.DataFrame(\n",
    "        columns=list(train_df.columns)\n",
    "    )\n",
    "    max_count = count\n",
    "    max_iterations = 1000\n",
    "    cur_count = 0\n",
    "    while cur_count < max_count :    \n",
    "        iterations = 0\n",
    "        print ( domains )\n",
    "        while iterations < max_iterations:\n",
    "            domain_set = random.choice(domains, size = pattern_size, replace = False)\n",
    "            print ( domain_set )        \n",
    "            excluded_domain = random.choice(domain_set, size=1)[0]\n",
    "            pos_set = list(domain_set)\n",
    "            pos_set.remove(excluded_domain)\n",
    "            candidate_dict = {}\n",
    "            _tries1 = 0 \n",
    "            \n",
    "            while True:\n",
    "                for d in pos_set:\n",
    "                    # sample entity\n",
    "                    candidate_dict[d] = random.choice(domain_entitiesSet_dict[d], size=1)[0]\n",
    "                if check_nonZeroCoOccurrence( candidate_dict, dict_coOccMatrix) == True: continue\n",
    "                _tries1 += 1\n",
    "            print(' Tries (1) :: ', _tries1)\n",
    "            if find_pattern_count(candidate_dict, train_df) >= min_pattern_count:\n",
    "                _tries2 = 0\n",
    "                condition_satisfied = False\n",
    "                while condition_satisfied == False:\n",
    "                    cand_e = random.choice(domain_entitiesSet_dict[excluded_domain], size=1)[0]\n",
    "                    candidate_dict[excluded_domain] = cand_e\n",
    "                    # Ensure that cand_e has non-zero co-occurrence with others\n",
    "                    for dpair in combinations(list(candidate_dict.keys()),2):\n",
    "                        subSet_dict = {}\n",
    "                        subSet_dict[dpair[0]] = candidate_dict[dpair[0]]\n",
    "                        subSet_dict[dpair[1]] = candidate_dict[dpair[1]]\n",
    "                        if check_nonZeroCoOccurrence( subSet_dict, dict_coOccMatrix) == False:\n",
    "                            condition_satisfied = False\n",
    "                            break\n",
    "                        else:\n",
    "                            condition_satisfied = True\n",
    "                    \n",
    "                    _tries2 += 1\n",
    "                print(' Tries (2) :: ', _tries2)        \n",
    "            iterations += 1\n",
    "            \n",
    "        cur_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AdminRegion', 'CountryOfSale', 'HSCode', 'Province', 'ShipmentDestination', 'ShipperPanjivaID', 'TradeType', 'TransportMethod']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "choice() got an unexpected keyword argument 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-f3760c5df1ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfind_conflicting_patterns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_coOccMatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpattern_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-1c3efb5547a6>\u001b[0m in \u001b[0;36mfind_conflicting_patterns\u001b[0;34m(train_df, dict_coOccMatrix, id_col, pattern_size, count)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0mdomains\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0miterations\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_iterations\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mdomain_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdomains\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpattern_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0mdomain_set\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mexcluded_domain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdomain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: choice() got an unexpected keyword argument 'size'"
     ]
    }
   ],
   "source": [
    "find_conflicting_patterns(train_df, dict_coOccMatrix, id_col, pattern_size = 4, count = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CountryOfSale', 'TradeType', 'TransportMethod',\n",
       "       'ShipperPanjivaID'], dtype='<U19')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ['AdminRegion', 'CountryOfSale', 'HSCode', 'Province', 'ShipmentDestination', 'ShipperPanjivaID', 'TradeType', 'TransportMethod']\n",
    "\n",
    "np.random.choice(a,size=4,replace=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
