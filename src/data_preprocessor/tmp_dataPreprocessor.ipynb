{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import glob\n",
    "import pickle\n",
    "import random\n",
    "from joblib import Parallel, delayed\n",
    "import yaml\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# ===================\n",
    "\n",
    "CONFIG_FILE = 'config_preprocessor_v02.yaml'\n",
    "id_col = 'PanjivaRecordID'\n",
    "ns_id_col = 'NegSampleID'\n",
    "term_2_col = 'term_2'\n",
    "term_4_col = 'term_4'\n",
    "num_neg_samples_ape = None\n",
    "use_cols = None\n",
    "freq_bound = None\n",
    "column_value_filters = None\n",
    "num_neg_samples = None\n",
    "save_dir = None\n",
    "cleaned_csv_subdir = None\n",
    "\n",
    "# ====================\n",
    "\n",
    "def get_regex(_type):\n",
    "    global DIR\n",
    "\n",
    "    if DIR == 'us_import':\n",
    "        if _type == 'train':\n",
    "            return '*0[1-6]*2016*.csv'\n",
    "        if _type == 'test':\n",
    "            return '*0[7-9]*2016*.csv'\n",
    "\n",
    "    if DIR == 'china_import':\n",
    "        if _type == 'train':\n",
    "            return '*0[1-6]*2016*.csv'\n",
    "        if _type == 'test':\n",
    "            return '*0[7-9]*2016*.csv'\n",
    "\n",
    "    if DIR == 'china_export':\n",
    "        if _type == 'train':\n",
    "            return '*0[1-4]*2016*.csv'\n",
    "        if _type == 'test':\n",
    "            return '*0[5-6]*2016*.csv'\n",
    "\n",
    "    return '*.csv'\n",
    "\n",
    "\n",
    "\n",
    "def get_files(DIR, _type='all'):\n",
    "    \n",
    "    data_dir = os.path.join(\n",
    "        './../../Data_Raw',\n",
    "        DIR\n",
    "    )\n",
    "    \n",
    "    regex = get_regex(_type)\n",
    "    files = sorted(\n",
    "        glob.glob(\n",
    "            os.path.join(data_dir, regex)\n",
    "        )\n",
    "    )\n",
    "    print('DIR ::', DIR, ' Type ::', _type, 'Files count::', len(files) )\n",
    "    return files\n",
    "\n",
    "def set_up_config():\n",
    "    \n",
    "    global CONFIG_FILE\n",
    "    global use_cols\n",
    "    global freq_bound\n",
    "    global num_neg_samples_ape\n",
    "    global DIR\n",
    "    global save_dir\n",
    "    global column_value_filters\n",
    "    global num_neg_samples\n",
    "    global cleaned_csv_subdir\n",
    "    \n",
    "    with open(CONFIG_FILE) as f:\n",
    "        CONFIG = yaml.safe_load(f)\n",
    "\n",
    "    DIR = CONFIG['DIR']\n",
    "    save_dir = os.path.join(\n",
    "        CONFIG['save_dir'],\n",
    "        DIR\n",
    "    )\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "    \n",
    "    cleaned_csv_subdir = os.path.join(\n",
    "        save_dir, \n",
    "        CONFIG['cleaned_csv_subdir']\n",
    "    )\n",
    "    \n",
    "    if not os.path.exists(cleaned_csv_subdir):\n",
    "        os.mkdir(cleaned_csv_subdir)\n",
    "        \n",
    "    use_cols = CONFIG[DIR]['use_cols']\n",
    "    freq_bound = CONFIG[DIR]['low_freq_bound']\n",
    "    column_value_filters = CONFIG[DIR]['column_value_filters']\n",
    "    num_neg_samples_ape = CONFIG[DIR]['num_neg_samples_ape']\n",
    "    num_neg_samples = CONFIG[DIR]['num_neg_samples']\n",
    "    \n",
    "    return CONFIG\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Replace attribute with id specific to a domain\n",
    "'''\n",
    "def replace_attr_with_id(row, attr, val2id_dict):\n",
    "    val = row[attr]\n",
    "    if val not in val2id_dict.keys():\n",
    "        print(attr, val)\n",
    "        return None\n",
    "    else:\n",
    "        return val2id_dict[val]\n",
    "    \n",
    "'''\n",
    "Converts the train df to ids \n",
    "Returns :\n",
    "col_val2id_dict  { 'col_name': { 'col1_val1': id1,  ... } , ... }\n",
    "'''\n",
    "\n",
    "\n",
    "def convert_to_ids(\n",
    "        df,\n",
    "        save_dir\n",
    "):\n",
    "    global id_col\n",
    "\n",
    "    feature_columns = list(df.columns)\n",
    "    feature_columns.remove(id_col)\n",
    "    \n",
    "    dict_DomainDims = {}\n",
    "    col_val2id_dict = {}\n",
    "\n",
    "    for col in sorted(feature_columns):\n",
    "        vals = list(set(df[col]))\n",
    "\n",
    "        # ----\n",
    "        #   \n",
    "        #   0 : item1 , \n",
    "        #   1 : item2 , \n",
    "        #   ...\n",
    "        # ----\n",
    "        id2val_dict = {\n",
    "            e[0]: e[1]\n",
    "            for e in enumerate(vals, 0)\n",
    "        }\n",
    "\n",
    "        \n",
    "        # ----\n",
    "        #   \n",
    "        #   item1 : 0 , \n",
    "        #   item2 : 0 , \n",
    "        #   ...\n",
    "        # ----\n",
    "        val2id_dict = {\n",
    "            v: k for k, v in id2val_dict.items()\n",
    "        }\n",
    "        col_val2id_dict[col] = val2id_dict\n",
    "\n",
    "        # Replace\n",
    "        df[col] = df.apply(\n",
    "            replace_attr_with_id,\n",
    "            axis=1,\n",
    "            args=(\n",
    "                col,\n",
    "                val2id_dict,\n",
    "            )\n",
    "        )\n",
    "        dict_DomainDims[col] = len(id2val_dict)\n",
    "\n",
    "    print(' Feature columns :: ', feature_columns)\n",
    "    print('dict_DomainDims ', dict_DomainDims)\n",
    "    \n",
    "    # -------------\n",
    "    # Save the domain dimensions \n",
    "    # -------------\n",
    "    \n",
    "    file = 'domain_dims.pkl'\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "    f_path = os.path.join(save_dir, file)\n",
    "\n",
    "    with open(f_path, 'wb') as fh:\n",
    "        pickle.dump(\n",
    "            domain_dims_res,\n",
    "            fh,\n",
    "            pickle.HIGHEST_PROTOCOL\n",
    "        )\n",
    "    return df, col_val2id_dict\n",
    "\n",
    "'''\n",
    "Join the csv files into 1 single Dataframe\n",
    "Removes missing values\n",
    "Input : file_path_list\n",
    "'''\n",
    "def collate(file_list):\n",
    "    global id_col\n",
    "    global use_cols\n",
    "\n",
    "    _master_df = None\n",
    "    for file in file_list:\n",
    "        _df = pd.read_csv(\n",
    "            file,\n",
    "            low_memory=False,\n",
    "            usecols=use_cols\n",
    "        )\n",
    "        \n",
    "        # Drop missing values\n",
    "        _df = _df.dropna()\n",
    "        if _master_df is None:\n",
    "            _master_df = pd.DataFrame(_df)\n",
    "        else:\n",
    "            _master_df = _master_df.append(\n",
    "                _df,\n",
    "                ignore_index=True\n",
    "            )\n",
    "            \n",
    "    feature_cols = list(_master_df.columns)\n",
    "    feature_cols.remove(id_col)\n",
    "    feature_cols = list(sorted(feature_cols))\n",
    "    \n",
    "    all_cols = [id_col]\n",
    "    all_cols.extend(feature_cols)\n",
    "    print(' Columns in the dataframe : ', all_cols)\n",
    "    _master_df = _master_df[all_cols]\n",
    "    return _master_df\n",
    "\n",
    "\n",
    "'''\n",
    "Remove the rows with entities that have very low frequency.\n",
    "'''\n",
    "def remove_low_frequency_values(df):\n",
    "    global id_col\n",
    "    global freq_bound\n",
    "   \n",
    "    freq_column_value_filters = {}\n",
    "\n",
    "    feature_cols = list(df.columns)\n",
    "    feature_cols.remove(id_col)\n",
    "    # ----\n",
    "    # figure out which entities are to be removed\n",
    "    # ----\n",
    "    for c in feature_cols:\n",
    "        values = list(df[c])\n",
    "        freq_column_value_filters[c] = []\n",
    "\n",
    "        obj_counter = Counter(values)\n",
    "        for _item, _count in obj_counter.items():\n",
    "            if _count < freq_bound:\n",
    "                freq_column_value_filters[c].append(_item)\n",
    "    print('Removing :: ')\n",
    "    for c, _items in freq_column_value_filters.items():\n",
    "        print('column : ',c, 'count', len(_items))\n",
    "        \n",
    "    print(' DF length : ', len(df))\n",
    "    for col, val in freq_column_value_filters.items():\n",
    "        df = df.loc[\n",
    "            (~df[col].isin(val))\n",
    "        ]\n",
    "    \n",
    "    return df\n",
    "\n",
    "def HSCode_cleanup( list_df, DIR, config): \n",
    "    hscode_col = 'HSCode'  \n",
    "          \n",
    "    # ----- #\n",
    "    # Expert curated HS codes\n",
    "    hs_code_filter_file = os.path.join(config['hscode_filter_file_loc'], DIR + config['hscode_filter_file_pattern'])\n",
    "    tmp = pd.read_csv(hs_code_filter_file, index_col=None,header=None)\n",
    "    target_codes = list(tmp[0])\n",
    "    \n",
    "    def hsc_proc( _code ):\n",
    "        return str(_code)[:4]\n",
    "    \n",
    "    target_codes = list(sorted([ hsc_proc(_) for _ in target_codes ]))\n",
    "    \n",
    "    def filter_by_ExpertHSCodeList( _code, target_codes):\n",
    "        if _code[:2] in target_codes or _code[:4] in target_codes:\n",
    "            return _code \n",
    "        return None\n",
    "    \n",
    "    # ------ #\n",
    "    # Correct the formats of HSCodes :\n",
    "    # eg. in china_export add in the preceeding 0\n",
    "    # ------ #\n",
    "    def add_preceeding_zero(_code):\n",
    "        _code = _code.strip()\n",
    "        if len(_code) > 6 :\n",
    "            _code = _code[:6]\n",
    "        elif len(_code) == 5 :\n",
    "            _code = '0' + _code\n",
    "        return _code\n",
    "    \n",
    "    list_processed_df = []\n",
    "    for df in list_df:\n",
    "        df = df.dropna()\n",
    "        df[hscode_col] = df[hscode_col].astype(str)\n",
    "        if DIR == 'china_export':\n",
    "            df[hscode_col] = df[hscode_col].apply(add_preceeding_zero)\n",
    "            \n",
    "        df[hscode_col] = df[hscode_col].apply(\n",
    "            filter_by_ExpertHSCodeList, \n",
    "            args=(target_codes,)\n",
    "        )\n",
    "        df = df.dropna()\n",
    "        list_processed_df.append(df)\n",
    "    # --------- #\n",
    "    \n",
    "    return list_processed_df\n",
    "\n",
    "\n",
    "'''\n",
    "Apply :: column_value_filters\n",
    "Remove values which are garbage & known to us\n",
    "'''\n",
    "def apply_value_filters( list_df ):\n",
    "    global column_value_filters\n",
    "    \n",
    "    if type(column_value_filters) != bool:\n",
    "        list_processed_df = [] \n",
    "        for df in list_df :\n",
    "            for col, val in column_value_filters.items():\n",
    "                df = train_master_df.loc[\n",
    "                    (~train_master_df[col].isin(val))\n",
    "                ]\n",
    "            list_processed_df.append(df)\n",
    "        return list_processed_df\n",
    "    return list_df\n",
    "\n",
    "def clean_train_data():\n",
    "    global DIR\n",
    "    global CONFIG \n",
    "    \n",
    "    files = get_files(DIR, 'train')\n",
    "    list_df = [pd.read_csv( _file, usecols = use_cols, low_memory=False) for _file in files]\n",
    "    list_file_name = [ _.split('/')[-1] for _ in  files]\n",
    "    list_df = HSCode_cleanup (list_df , DIR, CONFIG)\n",
    "    \n",
    "    list_df_1 = apply_value_filters(list_df)   \n",
    "    master_df = None\n",
    "    for df in list_df_1:\n",
    "        if master_df is None:\n",
    "            master_df = pd.DataFrame(df, copy=True)\n",
    "        else:\n",
    "            master_df = master_df.append(\n",
    "                df, \n",
    "                ignore_index=True\n",
    "            )\n",
    "            \n",
    "    master_df = remove_low_frequency_values(master_df)\n",
    "        \n",
    "    return  master_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def setup_testing_data(\n",
    "    test_df, \n",
    "    train_df, \n",
    "    col_val2id_dict\n",
    "):\n",
    "    \n",
    "    global id_col\n",
    "    # Replace with None if ids are not in train_set\n",
    "    print('----')\n",
    "    feature_cols = list(test_df.columns)\n",
    "    feature_cols.remove(id_col)\n",
    "\n",
    "    for col in feature_cols:\n",
    "        valid_items = list(col_val2id_dict[col].keys())\n",
    "        test_df = test_df.loc[test_df[col].isin(valid_items)]\n",
    "\n",
    "    print(' Length of testing data', len(test_df))\n",
    "\n",
    "    # First convert to to ids\n",
    "    for col in feature_cols:\n",
    "        val2id_dict = col_val2id_dict[col]\n",
    "        test_df[col] = test_df.apply(\n",
    "            replace_attr_with_id,\n",
    "            axis=1,\n",
    "            args=(\n",
    "                col,\n",
    "                val2id_dict,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    '''\n",
    "    Remove duplicates :\n",
    "    Paralleleize the process \n",
    "    '''\n",
    "    \n",
    "    def aux_validate( target_df, train_df):\n",
    "        tmp_df = pd.DataFrame(columns=list(test_df.columns)) \n",
    "        for i, row in test_df.iterrows():\n",
    "            if validate(row, train_df):\n",
    "                tmp_df = tmp_df.append(row, ignore_index=True)\n",
    "        return tmp_df\n",
    "    \n",
    "            \n",
    "            \n",
    "    num_chunks = 20\n",
    "    chunk_len = int(len(test_df) / (num_chunks - 1))\n",
    "\n",
    "    list_df_chunks = np.split(\n",
    "        test_df.head(chunk_len * (num_chunks - 1)), num_chunks - 1\n",
    "    )\n",
    "\n",
    "    end_len = len(test_df) - chunk_len * (num_chunks - 1)\n",
    "    list_df_chunks.append(test_df.tail(end_len))\n",
    "\n",
    "    print(' Deduplication of test set w.r.t. train :: Length of chunks ' ,list_df_chunks[_l])\n",
    "\n",
    "    list_dedup_df = Parallel(n_jobs=num_chunks)(\n",
    "        delayed( aux_validate )(target_df, train_df)\n",
    "        for target_df in list_df_chunks\n",
    "    )\n",
    "    \n",
    "\n",
    "    new_test_df = None\n",
    "    for _df in list_dedup_df:\n",
    "        if new_test_df is None:\n",
    "            new_test_df = _df\n",
    "        else:\n",
    "            new_test_df = new_test_df.append(_df, ignore_index=True)\n",
    "\n",
    "    print(' After deduplication :: ', len(new_test_df))\n",
    "\n",
    "    return new_test_df\n",
    "\n",
    "def create_train_test_sets():\n",
    "    global use_cols\n",
    "    global DIR\n",
    "    global save_dir\n",
    "    global column_value_filters\n",
    "    global CONFIG\n",
    "    \n",
    "   \n",
    "\n",
    "    # combine test data into 1 file :\n",
    "    test_files = get_files(DIR, 'test')\n",
    "    test_master_df = collate(test_files)\n",
    "    print('size of  Test set ', len(test_master_df))\n",
    "    \n",
    "    train_df = clean_train_data()\n",
    "    '''\n",
    "    test data preprocessing\n",
    "    '''\n",
    "\n",
    "    train_df, col_val2id_dict = convert_to_ids(\n",
    "        train_df,\n",
    "        save_dir\n",
    "    )\n",
    "\n",
    "    test_df = setup_testing_data(\n",
    "        test_master_df,\n",
    "        train_df,\n",
    "        col_val2id_dict\n",
    "    )\n",
    "\n",
    "    test_df.to_csv(os.path.join(save_dir, 'test_data.csv'), index=False)\n",
    "    train_df.to_csv(os.path.join(save_dir, 'train_data.csv'), index=False)\n",
    "   \n",
    "    return\n",
    "\n",
    "\n",
    "CONFIG = set_up_config()\n",
    "\n",
    "create_train_test_sets()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
