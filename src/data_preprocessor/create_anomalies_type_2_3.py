import pandas as pd
import sys
import os
import numpy as np
import pickle
from itertools import combinations
from joblib import Parallel, delayed
from numpy import random
import hashlib
import os
import sys
import pandas as pd
import numpy as np
import sklearn
import glob
import pickle
import random
from joblib import Parallel, delayed
import yaml
import math
from collections import Counter

sys.path.append('.')
sys.path.append('./..')

try:
    from . import utils_createAnomalies as utils_local
except:
    import utils_createAnomalies as utils_local





def get_coOccMatrix_dict(df, id_col):
    columns = list(df.columns)
    columns.remove(id_col)
    columns = list(sorted(columns))
    columnWise_coOccMatrix_dict = {}

    for i in range(len(columns)):
        for j in range(i + 1, len(columns)):
            col_1 = columns[i]
            col_2 = columns[j]
            key = col_1 + '_+_' + col_2
            res = utils_local.create_coocc_matrix(df, col_1, col_2)
            columnWise_coOccMatrix_dict[key] = res

    return columnWise_coOccMatrix_dict


def check_nonZeroCoOccurrence(
        dict_domain_entities,
        dict_coOccMatrix
):
    domains = sorted(dict_domain_entities.keys())
    for d_pair in combinations(domains, 2):
        d_pair = sorted(d_pair)
        key = '_+_'.join(d_pair)
        e1 = dict_domain_entities[d_pair[0]]
        e2 = dict_domain_entities[d_pair[1]]
        if dict_coOccMatrix[key][e1][e2] == 0:
            return False
    return True


def find_conflicting_patterns_aux_1(
        train_df,
        dict_coOccMatrix,
        id_col,
        pattern_size=4,
        count=100
):
    results = []
    domains = list(sorted(train_df.columns))
    domains.remove(id_col)
    # create set of entity ids for each of the domains
    domain_entitiesSet_dict = {}
    min_pattern_count = 5
    for d in domains:
        domain_entitiesSet_dict[d] = list(set(train_df[d]))

    cur_count = 0
    while cur_count < count:
        domain_set = np.random.choice(domains, size=pattern_size, replace=False)
        excluded_domain = np.random.choice(domain_set, size=1)[0]
        pos_set = list(domain_set)
        pos_set.remove(excluded_domain)
        candidate_dict = {}

        _tries1 = 0
        while True:
            for d in pos_set:
                # sample entity
                candidate_dict[d] = np.random.choice(domain_entitiesSet_dict[d], size=1)[0]

            if check_nonZeroCoOccurrence(candidate_dict, dict_coOccMatrix) == True:
                break
            _tries1 += 1

        if utils_local.find_pattern_count(candidate_dict, train_df) >= min_pattern_count:
            _tries2 = 0
            condition_satisfied = False
            while not condition_satisfied:
                # Select an entity
                cand_e = np.random.choice(domain_entitiesSet_dict[excluded_domain], size=1)[0]
                candidate_dict[excluded_domain] = cand_e
                # ====
                # Ensure that new candidate has non-zero co-occurrence with others
                for dpair in combinations(list(candidate_dict.keys()), 2):
                    subSet_dict = {}
                    subSet_dict[dpair[0]] = candidate_dict[dpair[0]]
                    subSet_dict[dpair[1]] = candidate_dict[dpair[1]]

                    if not check_nonZeroCoOccurrence(subSet_dict, dict_coOccMatrix):
                        condition_satisfied = False
                        break
                    else:
                        condition_satisfied = True
                # ====
                _tries2 += 1

        results.append(
            candidate_dict
        )
    return results


# ========== #

def generate_anomalies_type_4(
        train_df,
        test_df,
        dict_coOccMatrix,
        id_col='PanjivaRecordID',
        pattern_size=4,
        reqd_anom_perc=10,
        num_jobs=40,
        pattern_duplicates=100
):
    # =====================
    # Over estimating a bit, so that overlaps can be compensated for
    # =====================
    dist_pattern_count = int(1.25 * len(test_df) * (reqd_anom_perc / 100) / 100)

    list_results = Parallel(n_jobs=num_jobs)(
        delayed(find_conflicting_patterns_aux_1)(
            train_df, test_df, dict_coOccMatrix, id_col, pattern_size=4, count=dist_pattern_count
        ) for _ in range(num_jobs)
    )
    results= []
    for item in list_results:
        results.extend(item)

    # Remove duplicate 'spurious' patterns generated by the previous call
    patterns = utils_local.dedup_list_dictionaries(
        results
    )

    # ===================
    # For each pattern create k samples ,
    # where k = pattern_duplicates
    # ===================

    for pattern in patterns:

        # select 2 (partial) domains
        _domains_set = np.random.choice(list(pattern.keys()),replace=False,size=2)
        cand = {}
        for d in _domains_set:
            cand [d] = pattern[d]


        # ===
        # Find k records with partial match with pattern
        # ===
        match_df = utils_local.query_df(
            test_df,
            cand
        )
        match_df = match_df.sample( int( 1.1 * pattern_duplicates) )
        new_df = pd.DataFrame(columns= list())
        for _, row in match_df.iterrows():
            row_copy = pd.Series(row,copy=True)
            for d,e in pattern.items():
                row_copy[d] =  e

            new_df = new_df.append(
                row_copy, ignore_index=True
            )
    return



# CONFIG = set_up_config()
# train_df = pd.read_csv(os.path.join(save_dir, CONFIG['train_data_file']))
# dict_coOccMatrix = get_coOccMatrix_dict(train_df, id_col)
# generate_anomalies_type_4(train_df, dict_coOccMatrix, id_col, pattern_size=pattern_size, count=10)
