{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 40 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "Cuda available :: True Cuda current device :: 0 Tesla P100-PCIE-16GB\n",
      " Graph Agreement Module \n",
      "Encoder Layer ::\n",
      "Linear(in_features=1024, out_features=512, bias=True)\n",
      "Linear(in_features=512, out_features=512, bias=True)\n",
      "Linear(in_features=512, out_features=256, bias=True)\n",
      "Predictor Layer :: Linear(in_features=256, out_features=1, bias=True)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'clf_net_v1' object has no attribute 'encoder'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-62ab9cc21481>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0mgam_encoder_dimensions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0mclf_inp_emb_dimension\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m     \u001b[0mclf_layer_dimensions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0;36m96\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m48\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m     )\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-62ab9cc21481>\u001b[0m in \u001b[0;36msetup_Net\u001b[0;34m(self, node_emb_dimension, num_domains, gnet_output_dimensions, matrix_pretrained_node_embeddings, gam_record_input_dimension, gam_encoder_dimensions, clf_inp_emb_dimension, clf_layer_dimensions)\u001b[0m\n\u001b[1;32m    324\u001b[0m         self.clf_net = clf_net(\n\u001b[1;32m    325\u001b[0m             \u001b[0mclf_inp_emb_dimension\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m             \u001b[0mclf_layer_dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m         )\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/AD_v2/AD_v2/src/GAM/clf_net.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inp_emb_dimension, layer_dimensions, dropout)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0minp_emb_dimension\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mlayer_dimensions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mdropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         )\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/AD_v2/AD_v2/src/GAM/clf_net.py\u001b[0m in \u001b[0;36msetup_Net\u001b[0;34m(self, inp_emb_dimension, layer_dimensions, dropout)\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0mop_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_dimensions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_dim\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_parameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mlp_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0minp_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tmp_venv/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 518\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'clf_net_v1' object has no attribute 'encoder'"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# ---------------\n",
    "# Author : Debanjan Datta\n",
    "# Email : ddatta@vt.edu\n",
    "# ---------------\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "pandarallel.initialize()\n",
    "sys.path.append('./../..')\n",
    "sys.path.append('./..')\n",
    "\n",
    "import argparse\n",
    "import multiprocessing\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import RandomSampler, SequentialSampler\n",
    "import yaml\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "from torch import tensor\n",
    "from gam_module import gam_net\n",
    "from gam_module import gam_loss\n",
    "from clf_net import clf_net_v1 as clf_net\n",
    "from clf_net import clf_loss_v1 as clf_loss\n",
    "from record_node import graph_net_v1 as graph_net\n",
    "from torch_data_loader import pair_Dataset\n",
    "from torch_data_loader import type1_Dataset\n",
    "from torch import FloatTensor as FT\n",
    "from torch import LongTensor as LT\n",
    "\n",
    "\n",
    "try:\n",
    "    print('Cuda available ::', torch.cuda.is_available(), 'Cuda current device ::', torch.cuda.current_device(),\n",
    "          torch.cuda.get_device_name(0))\n",
    "    if torch.cuda.is_available():\n",
    "        dev = \"cuda:0\"\n",
    "    else:\n",
    "        dev = \"cpu\"\n",
    "    device = torch.device(dev)\n",
    "except:\n",
    "    print('No CUDA')\n",
    "\n",
    "config_file = 'config.yaml'\n",
    "CONFIG = None\n",
    "DATA_SOURCE_DIR_1 = None\n",
    "DATA_SOURCE_DIR_2 = None\n",
    "model_use_data_DIR = None\n",
    "DIR = None\n",
    "domain_dims = None\n",
    "score_col = 'score'\n",
    "fraud_col = 'fraud'\n",
    "anomaly_col = 'anomaly'\n",
    "id_col = 'PanjivaRecordID'\n",
    "label_col = 'y'\n",
    "true_label_col = 'y_true'\n",
    "feature_col_list = []\n",
    "serial_mapping_df = None\n",
    "is_labelled_col = 'labelled'\n",
    "matrix_node_emb_path = None\n",
    "\n",
    "# =================================================\n",
    "\n",
    "def setup_config(_DIR):\n",
    "    global CONFIG\n",
    "    global config_file\n",
    "    global DATA_SOURCE_DIR_1\n",
    "    global DATA_SOURCE_DIR_2\n",
    "    global DIR\n",
    "    global model_use_data_DIR\n",
    "    global domain_dims\n",
    "    global feature_col_list\n",
    "    global serial_mapping_df\n",
    "    global serialized_feature_col_list\n",
    "    global matrix_node_emb_path\n",
    "    \n",
    "    if _DIR is not None:\n",
    "        DIR = _DIR\n",
    "\n",
    "    with open(config_file) as f:\n",
    "        CONFIG = yaml.safe_load(f)\n",
    "\n",
    "    DATA_SOURCE_DIR_1 = CONFIG['DATA_SOURCE_DIR_1']\n",
    "    DATA_SOURCE_DIR_2 = CONFIG['DATA_SOURCE_DIR_2']\n",
    "\n",
    "    DATA_SOURCE_DIR_1 = os.path.join(DATA_SOURCE_DIR_1, DIR)\n",
    "    DATA_SOURCE_DIR_2 = os.path.join(DATA_SOURCE_DIR_1, DIR)\n",
    "\n",
    "    model_use_data_DIR = CONFIG['model_use_data_DIR']\n",
    "    if not os.path.exists(model_use_data_DIR): os.mkdir(model_use_data_DIR)\n",
    "    model_use_data_DIR = os.path.join(model_use_data_DIR, DIR)\n",
    "    if not os.path.exists(model_use_data_DIR): os.mkdir(model_use_data_DIR)\n",
    "\n",
    "    with open(\n",
    "            os.path.join(\n",
    "                DATA_SOURCE_DIR_1,\n",
    "                'domain_dims.pkl'\n",
    "            ), 'rb') as fh:\n",
    "        domain_dims = pickle.load(fh)\n",
    "\n",
    "    feature_col_list = list(sorted(domain_dims.keys()))\n",
    "    serialized_feature_col_list = ['_' + _ for _ in feature_col_list]\n",
    "    serial_mapping_df_path = os.path.join(\n",
    "        CONFIG['serial_mapping_df_loc'],\n",
    "        DIR,\n",
    "        CONFIG['serial_mapping_df_name']\n",
    "    )\n",
    "    serial_mapping_df = pd.read_csv(serial_mapping_df_path, index_col=None)\n",
    "    matrix_node_emb_path = os.path.join(CONFIG['matrix_node_emb_loc'], DIR, CONFIG['matrix_node_emb_file'])\n",
    "    \n",
    "    \n",
    "    return\n",
    "\n",
    "DIR = 'us_import2'\n",
    "setup_config(DIR)\n",
    "\n",
    "\n",
    "\n",
    "def set_ground_truth_labels(df):\n",
    "    global true_label_col\n",
    "    global fraud_col\n",
    "\n",
    "    def aux_true_label(row):\n",
    "        if row[fraud_col]:\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "    df[true_label_col] = df.parallel_apply(aux_true_label, axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "# -----\n",
    "# Return part of dataframe , where instances are labelled\n",
    "# -----\n",
    "def extract_labelled_df(df):\n",
    "    global is_labelled_col\n",
    "    res = pd.DataFrame(\n",
    "        df.loc[df[is_labelled_col] == True],\n",
    "        copy=True\n",
    "    )\n",
    "    return res\n",
    "\n",
    "\n",
    "def extract_unlabelled_df(df):\n",
    "    global is_labelled_col\n",
    "    res = pd.DataFrame(\n",
    "        df.loc[df[is_labelled_col] == False],\n",
    "        copy=True\n",
    "    )\n",
    "    return res\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# Get o/p from the AD system\n",
    "# -----------------------\n",
    "def read_scored_data():\n",
    "    global score_col\n",
    "    global DATA_SOURCE_DIR_2\n",
    "    global label_col\n",
    "\n",
    "    df = pd.read_csv(\n",
    "        os.path.join(DATA_SOURCE_DIR_2, 'scored_test_data.csv'), index_col=None\n",
    "    )\n",
    "    df = df.sort_values(by=[score_col])\n",
    "    df[label_col] = 0\n",
    "    df = set_ground_truth_labels(df)\n",
    "    return df\n",
    "\n",
    "def read_matrix_node_emb():\n",
    "    global matrix_node_emb_path\n",
    "    emb = np.load(matrix_node_emb_path)\n",
    "    return emb\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def set_label_in_top_perc(df, perc):\n",
    "    global score_col\n",
    "    global true_label_col\n",
    "    global id_col\n",
    "\n",
    "    df = df.sort_values(by=[score_col])\n",
    "    if perc > 1:\n",
    "        perc = perc / 100\n",
    "    count = len(df) * perc\n",
    "\n",
    "    cand = list(df.head(count)[id_col])\n",
    "    df.loc[df[id_col].isin(cand), label_col] = df.loc[df[id_col].isin(cand), true_label_col]\n",
    "    df.loc[df[id_col].isin(cand), is_labelled_col] = True\n",
    "    return df\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# Return the id list of new samples to be aded to labelled set.\n",
    "# Ensure balance in labelled and unlabelled samples\n",
    "# --------------------------\n",
    "def find_most_confident_samples (\n",
    "        U_df ,\n",
    "        y_probs ,  # np.array [?, 2]\n",
    "        y_pred_label, # np.array [?,]\n",
    "        confidence_lb = 0.2,\n",
    "        max_count = None\n",
    "):\n",
    "    global label_col\n",
    "    global id_col\n",
    "    if max_count is None:\n",
    "        max_count = 0.10 * len(U_df)\n",
    "\n",
    "    # Assuming binary classification\n",
    "    y_pred = label_col\n",
    "    U_df['diff'] = abs( y_probs[:,0]-  y_probs[:,1])\n",
    "    U_df[y_pred] = y_pred_label\n",
    "    valid_flag = 'valid'\n",
    "    U_df[valid_flag] = False\n",
    "    U_df_0 = U_df.loc[U_df[y_pred] == 0]\n",
    "    U_df_1 = U_df.loc[U_df[y_pred] == 1]\n",
    "\n",
    "    U_df_0 = U_df_0.sort_values(by=['diff'],ascending = False)\n",
    "    U_df_1 = U_df_1.sort_values(by=['diff'],ascending = False)\n",
    "\n",
    "    def aux_1(val):\n",
    "        if val > confidence_lb :\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    U_df_0[valid_flag] = U_df_0['diff'].parallel_apply(aux_1)\n",
    "    U_df_1[valid_flag] = U_df_1['diff'].parallel_apply(aux_1)\n",
    "\n",
    "    U_df_0 = U_df_0.loc[U_df_0[valid_flag] == True]\n",
    "    U_df_1 = U_df_1.loc[U_df_1[valid_flag] == True]\n",
    "\n",
    "    del U_df_0['diff']\n",
    "    del U_df_1['diff']\n",
    "    del U_df_0[valid_flag]\n",
    "    del U_df_1[valid_flag]\n",
    "    count = int (min ( min(len(U_df_0),len(U_df_1)), max_count/2))\n",
    "    res_df = U_df_1.head(count)\n",
    "    res_df = res_df.append(U_df_0.head(count),ignore_index=True)\n",
    "\n",
    "    return res_df\n",
    "\n",
    "\n",
    "\n",
    "def convert_to_serial_IDs(\n",
    "        df,\n",
    "        keep_entity_ids=False\n",
    "):\n",
    "    global feature_col_list\n",
    "    global serial_mapping_df\n",
    "\n",
    "    # Inplace conversion\n",
    "    def aux_conv_toSerialID(_row):\n",
    "        row = _row.copy()\n",
    "        for fc in feature_col_list:\n",
    "            col_name = fc\n",
    "            if keep_entity_ids:\n",
    "                col_name = '_' + fc\n",
    "            row[col_name] = list(serial_mapping_df.loc[\n",
    "                                     (serial_mapping_df['Domain'] == fc) &\n",
    "                                     (serial_mapping_df['Entity_ID'] == row[fc])\n",
    "                                     ])[0]\n",
    "        return row\n",
    "\n",
    "    df = df.parallel_apply(aux_conv_toSerialID, axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------- #\n",
    "# Custom regularization_loss\n",
    "def regularization_loss( g_ij, fi_yj):\n",
    "    val1 = (fi_yj[0] - fi_yj[1])**2\n",
    "    val2 = val1 * g_ij\n",
    "    val3 = torch.nn.mean(val2,-1,False)\n",
    "    return val3\n",
    "\n",
    "\n",
    "# -------------------------------------------------- #\n",
    "\n",
    "'''\n",
    "1. Train g\n",
    "2. Train f\n",
    "3. Add in the most confident labels\n",
    "'''\n",
    "\n",
    "class net(nn.Module):\n",
    "    def __init(self):\n",
    "        super(net, self).__init__()\n",
    "        # valid values for train_mode are 'f', 'g', False\n",
    "        self.train_mode = False\n",
    "        self.test_mode = False\n",
    "\n",
    "    def setup_Net(\n",
    "            self,\n",
    "            node_emb_dimension,\n",
    "            num_domains,\n",
    "            gnet_output_dimensions,\n",
    "            matrix_pretrained_node_embeddings,\n",
    "            gam_record_input_dimension,\n",
    "            gam_encoder_dimensions,\n",
    "            clf_inp_emb_dimension,\n",
    "            clf_layer_dimensions\n",
    "    ):\n",
    "        self.graph_net = graph_net(\n",
    "            node_emb_dimension,\n",
    "            num_domains,\n",
    "            gnet_output_dimensions,\n",
    "            matrix_pretrained_node_embeddings\n",
    "        )\n",
    "        self.gam_net = gam_net(\n",
    "            gam_record_input_dimension,\n",
    "            gam_encoder_dimensions,\n",
    "        )\n",
    "        self.clf_net = clf_net(\n",
    "            clf_inp_emb_dimension,\n",
    "            clf_layer_dimensions\n",
    "        )\n",
    "\n",
    "    # ---------------------------\n",
    "    # Input should be [ Batch, record( list of entities ) ]\n",
    "    # record( list of entities ) should have serialized entity id\n",
    "    # ---------------------------\n",
    "    # input_xy is an list\n",
    "    def forward(\n",
    "            self, input_x\n",
    "    ):\n",
    "\n",
    "        # ----------------\n",
    "        # Train the agreement module\n",
    "        # ----------------\n",
    "        if self.train_mode == 'g':\n",
    "            x1 = input_x[0]\n",
    "            x1 = self.graph_net(x1)\n",
    "            x2 = input_x[1]\n",
    "            x2 = self.graph_net(x2)\n",
    "\n",
    "            y_pred = self.gam_net(\n",
    "                x1,\n",
    "                x2\n",
    "            )\n",
    "            return y_pred\n",
    "        elif self.train_mode == 'f':\n",
    "            x1 = input_x\n",
    "            x1 = self.graph_net(x1)\n",
    "            y_pred = self.clf_net(x1)\n",
    "            return y_pred\n",
    "        else:\n",
    "            x1 = input_x\n",
    "            x1 = self.graph_net(x1)\n",
    "            y_pred = self.clf_net(x1)\n",
    "            return y_pred\n",
    "\n",
    "class dataGeneratorWrapper():\n",
    "    def __init__(self, obj_dataloader):\n",
    "        self.obj_dataloader = obj_dataloader.copy()\n",
    "\n",
    "    def generator(self):\n",
    "        for _, batch_data in enumerate(self.obj_dataloader):\n",
    "            yield batch_data\n",
    "\n",
    "\n",
    "matrix_node_emb = read_matrix_node_emb()\n",
    "NN = net()\n",
    "NN.setup_Net(\n",
    "    node_emb_dimension=128,\n",
    "    num_domains=8,\n",
    "    gnet_output_dimensions=128,\n",
    "    matrix_pretrained_node_embeddings = FT(matrix_node_emb),\n",
    "    gam_record_input_dimension = 128*8,\n",
    "    gam_encoder_dimensions = [512,512,256],\n",
    "    clf_inp_emb_dimension = 128*8,\n",
    "    clf_layer_dimensions = [ 96,64,48 ]\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ===========================================\n",
    "# Iterative training\n",
    "# ===========================================\n",
    "\n",
    "\n",
    "def train_model(df , NN):\n",
    "\n",
    "    global serialized_feature_col_list\n",
    "    global feature_col_list\n",
    "    batch_size = 256\n",
    "    num_epochs_g = 10\n",
    "    num_epochs_f = 10\n",
    "    log_interval = 500\n",
    "    num_proc = multiprocessing.cpu_count()\n",
    "    lambda_LL = 0.1\n",
    "    lambda_UL = 0.1\n",
    "    lambda_UU = 0.05\n",
    "    current_iter_count = 0\n",
    "    max_iter_count = 10\n",
    "    continue_training = True\n",
    "\n",
    "    df_L = extract_labelled_df(df)\n",
    "    df_U = extract_unlabelled_df(df)\n",
    "\n",
    "    while continue_training:\n",
    "        # GAM gets inputs as embeddings, which are obtained through the graph embeddings\n",
    "        # that requires serialized feature ids\n",
    "        g_feature_cols = serialized_feature_col_list\n",
    "\n",
    "        NN.train_mode = 'g'\n",
    "        data_source_L1 = type1_Dataset(\n",
    "            df_L,\n",
    "            x_cols=g_feature_cols,\n",
    "            y_col=label_col\n",
    "        )\n",
    "\n",
    "        dataLoader_obj_L1a = DataLoader(\n",
    "            data_source_L1,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=num_proc,\n",
    "            sampler=RandomSampler(data_source_L1)\n",
    "        )\n",
    "        dataLoader_obj_L1b = DataLoader(\n",
    "            data_source_L1,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=num_proc,\n",
    "            sampler=RandomSampler(data_source_L1)\n",
    "        )\n",
    "\n",
    "        optimizer_g = torch.optim.Adam(\n",
    "            [NN.graph_net.parameters(), NN.gam_net.parameters()],\n",
    "            lr=0.005\n",
    "        )\n",
    "        optimizer_f = torch.optim.Adam(\n",
    "            [NN.graph_net.parameters(), NN.clf_net.parameters()],\n",
    "            lr=0.005\n",
    "        )\n",
    "\n",
    "        # ----\n",
    "        # input_x1,y2 : from Dataloader ( L )\n",
    "        # input x2,y2 : from Dataloader ( L )\n",
    "        # For every pair, so nest them\n",
    "        # -----\n",
    "\n",
    "        optimizer_g.zero_grad()\n",
    "        for epoch in range(num_epochs_g):\n",
    "            record_loss = []\n",
    "            for i, data_i in enumerate(dataLoader_obj_L1a):\n",
    "                x1 = data_i[0]\n",
    "                y1 = data_i[1]\n",
    "                for j, data_j in enumerate(dataLoader_obj_L1b):\n",
    "                    x2 = data_j[0]\n",
    "                    y2 = data_j[1]\n",
    "                    input_x = [x1, x2]\n",
    "                    true_agreement = np.array((y1 == y2)).astype(float)\n",
    "                    true_agreement = FT(true_agreement)\n",
    "                    pred_agreement = NN(input_x)\n",
    "                    loss = gam_loss(pred_agreement, true_agreement)\n",
    "                    loss.backward()\n",
    "                    optimizer_g.step()\n",
    "                    record_loss.append(float(loss))\n",
    "\n",
    "        # -----------------------\n",
    "        # Train the classifier\n",
    "        # Use only labelled data\n",
    "        # ----------------------\n",
    "        # To do separate out f and g features\n",
    "        net.train_mode = 'f'\n",
    "\n",
    "        data_source_L2 = type1_Dataset(\n",
    "            df_L,\n",
    "            x_cols=g_feature_cols,\n",
    "            y_col=label_col\n",
    "        )\n",
    "\n",
    "        dataLoader_obj_L2 = DataLoader(\n",
    "            data_source_L2,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=num_proc,\n",
    "            sampler=RandomSampler(data_source_L2)\n",
    "        )\n",
    "        data_source_LL =  pair_Dataset(\n",
    "                df_L,\n",
    "                df_L,\n",
    "                x_cols=g_feature_cols,\n",
    "                y_col=label_col\n",
    "            )\n",
    "\n",
    "        dataLoader_obj_L3  = DataLoader(\n",
    "            data_source_LL,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=num_proc,\n",
    "            sampler=RandomSampler(data_source_LL)\n",
    "        )\n",
    "        data_source_UL = pair_Dataset(\n",
    "            df_U,\n",
    "            df_L,\n",
    "            x_cols=g_feature_cols,\n",
    "            y_col=label_col\n",
    "        )\n",
    "\n",
    "        dataLoader_obj_L4 = DataLoader(\n",
    "            data_source_UL,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=num_proc,\n",
    "            sampler=RandomSampler(data_source_LL)\n",
    "        )\n",
    "\n",
    "    data_source_UU = pair_Dataset(\n",
    "        df_U,\n",
    "        df_U,\n",
    "        x_cols=g_feature_cols,\n",
    "        y_col=label_col\n",
    "    )\n",
    "\n",
    "    dataLoader_obj_L5 = DataLoader(\n",
    "        data_source_UU,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_proc,\n",
    "        sampler=RandomSampler(data_source_UU)\n",
    "    )\n",
    "\n",
    "    optimizer_f.zero_grad()\n",
    "    for epoch in range(num_epochs_f):\n",
    "        data_L_generator = dataGeneratorWrapper(dataLoader_obj_L2).generator()\n",
    "        data_LL_generator = dataGeneratorWrapper(dataLoader_obj_L3).generator()\n",
    "        data_UL_generator = dataGeneratorWrapper(dataLoader_obj_L4).generator()\n",
    "        data_UU_generator = dataGeneratorWrapper(dataLoader_obj_L5).generator()\n",
    "\n",
    "        batch_idx_f = 0\n",
    "        data_L = next(data_L_generator)\n",
    "        while data_L is not None:\n",
    "            # Supervised Loss\n",
    "            x1 = data_L[0]\n",
    "            y_true = data_L[1]\n",
    "            pred_label = NN(x1)\n",
    "            loss_s = clf_loss(pred_label, y_true)\n",
    "\n",
    "            # LL :: lambda_LL * g(x_i,x_j) * d (f(x_i),y_j)\n",
    "            data_LL_x, data_LL_y = next(data_LL_generator)\n",
    "            x1 = data_LL_x[0]\n",
    "            x2 = data_LL_x[1]\n",
    "            y2 = data_LL_y[1]\n",
    "            pred_y1 = torch.argmax(net(x1),dim=1)\n",
    "            pred_agreement = NN.gam_net(x1,x2)\n",
    "            loss_LL = regularization_loss (pred_agreement, [pred_y1, y2])\n",
    "\n",
    "            # UL\n",
    "            data_UL_x, data_UL_y = next(data_LL_generator)\n",
    "            x1 = data_UL_x[0]\n",
    "            x2 = data_UL_x[1]\n",
    "            y2 = data_UL_y[1]\n",
    "            pred_y1 = torch.argmax(net(x1),dim=1)\n",
    "            pred_agreement = NN.gam_net(x1, x2)\n",
    "            loss_UL = regularization_loss(pred_agreement, [pred_y1, y2])\n",
    "\n",
    "            # UU\n",
    "            data_UU = next(data_UU_generator)\n",
    "            x1 = data_UU[0]\n",
    "            x2 = data_UU[1]\n",
    "            pred_y1 =  torch.argmax(net(x1),dim=1)\n",
    "            pred_y2 = torch.argmax(net(x2),dim=1)\n",
    "            pred_agreement = NN.gam_net(x1, x2)\n",
    "            loss_UU = regularization_loss(pred_agreement, [pred_y1, pred_y2])\n",
    "\n",
    "            loss_total = loss_s + lambda_LL * loss_LL + lambda_UL * loss_UL + lambda_UU * loss_UU\n",
    "            loss_total.backward()\n",
    "            optimizer_f.step()\n",
    "            data_L = next(data_L_generator)\n",
    "            batch_idx_f +=1\n",
    "            if batch_idx_f%log_interval == 0 :\n",
    "                print('Batch[f] {} :: Loss {}'.format(batch_idx_f, loss_total))\n",
    "\n",
    "    # -------------------------\n",
    "    # Predict and  Evaluate\n",
    "    # ---------------------------\n",
    "    data_source_EU = type1_Dataset(\n",
    "        df_U,\n",
    "        x_cols=g_feature_cols,\n",
    "        y_col=None\n",
    "    )\n",
    "    dataLoader_obj_EU = DataLoader(\n",
    "        data_source_EU,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_proc,\n",
    "        sampler= SequentialSampler(data_source_EU)\n",
    "    )\n",
    "    # data_EU_generator = dataGeneratorWrapper(dataLoader_obj_EU).generator()\n",
    "\n",
    "    pred_y_label = []\n",
    "    pred_y_probs = []\n",
    "    for batch_idx, data_x in enumerate(dataLoader_obj_EU):\n",
    "        _pred_y_probs = net(data_x)\n",
    "        _pred_y_label =  torch.argmax(_pred_y_probs,dim=1)\n",
    "        pred_y_label.extend(np.array(_pred_y_label))\n",
    "        pred_y_probs.extend(np.array(_pred_y_probs))\n",
    "\n",
    "    # ----------------\n",
    "    # Find the top-k most confident label\n",
    "    # Update the set of labelled and unlabelled samples\n",
    "    # ----------------\n",
    "\n",
    "    k = int(len(df_U) * 0.05)\n",
    "    self_labelled_samples = find_most_confident_samples (\n",
    "            U_df = df_U.copy(),\n",
    "            y_probs = pred_y_probs,\n",
    "            y_pred_label = pred_y_label,\n",
    "            confidence_lb = 0.25,\n",
    "            max_count = k\n",
    "    )\n",
    "\n",
    "    # remove those ids from df_U\n",
    "    rmv_id_list = list(self_labelled_samples[id_col])\n",
    "    df_L = df_L.append(self_labelled_samples,ignore_index=True)\n",
    "    df_U = df_U.loc[~(df_U[id_col].isin(rmv_id_list))]\n",
    "\n",
    "    # Also check for convergence\n",
    "    current_iter_count += 1\n",
    "    if current_iter_count > max_iter_count:\n",
    "        continue_training = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
