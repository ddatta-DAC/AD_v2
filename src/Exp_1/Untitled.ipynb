{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import glob \n",
    "import pickle\n",
    "sys.path.append('./../../')\n",
    "sys.path.append('./..')\n",
    "import inspect\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import time\n",
    "import yaml\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import auc\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ape import tf_model_ape_1\n",
    "from src.data_fetcher import data_fetcher\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../generated_data_v1\n",
      "output/us_import1\n",
      "save_dir/us_import1\n"
     ]
    }
   ],
   "source": [
    "CONFIG = None\n",
    "OP_DIR = None\n",
    "DATA_DIR = None\n",
    "domain_dims = None\n",
    "SAVE_DIR = None\n",
    "CONFIG_FILE = 'config_1.yaml'\n",
    "domain_dims = None\n",
    "\n",
    "# ------------------------------------- #\n",
    "def get_domain_dims(dd_file_path):\n",
    "    with open(dd_file_path, 'rb') as fh:\n",
    "        domain_dims = pickle.load(fh)\n",
    "    _tmpDF = pd.DataFrame.from_dict(domain_dims, orient='index')\n",
    "    _tmpDF = _tmpDF.reset_index()\n",
    "    _tmpDF = _tmpDF.rename(columns={'index': 'domain'})\n",
    "    _tmpDF = _tmpDF.sort_values(by=['domain'])\n",
    "    res = {k: v for k, v in zip(_tmpDF['domain'], _tmpDF[0])}\n",
    "    return res\n",
    "# ------------------------------------- #\n",
    "# Set up config & global vars\n",
    "# ------------------------------------- #\n",
    "def setup():\n",
    "    global SAVE_DIR\n",
    "    global DIR\n",
    "    global CONFIG\n",
    "    global OP_DIR\n",
    "    global DATA_DIR\n",
    "    global domain_dims\n",
    "   \n",
    "    with open(CONFIG_FILE) as f:\n",
    "        CONFIG = yaml.safe_load(f)\n",
    "  \n",
    "    SAVE_DIR = CONFIG['SAVE_DIR']\n",
    "    DIR = CONFIG['DIR']\n",
    "        \n",
    "    OP_DIR = CONFIG['OP_DIR']\n",
    "    DATA_DIR = CONFIG['DATA_DIR']\n",
    "    \n",
    "    if not os.path.exists(SAVE_DIR):\n",
    "        os.mkdir(SAVE_DIR)\n",
    "    SAVE_DIR = os.path.join(SAVE_DIR, DIR)\n",
    " \n",
    "    if not os.path.exists(SAVE_DIR):\n",
    "        os.mkdir(SAVE_DIR)\n",
    "\n",
    "    if not os.path.exists(OP_DIR):\n",
    "        os.mkdir(OP_DIR)\n",
    "\n",
    "    OP_DIR = os.path.join(OP_DIR, DIR)\n",
    "    if not os.path.exists(OP_DIR):\n",
    "        os.mkdir(OP_DIR)\n",
    "        \n",
    "    tf_model_ape_1._DIR = DIR\n",
    "    tf_model_ape_1.OP_DIR = OP_DIR\n",
    "    if not os.path.exists(OP_DIR):\n",
    "        os.mkdir(OP_DIR)\n",
    "        \n",
    "    \n",
    "    dd_file_path = os.path.join(DATA_DIR, DIR, 'domain_dims.pkl')\n",
    "    domain_dims = get_domain_dims(dd_file_path)\n",
    " \n",
    "    print(DATA_DIR)\n",
    "    print(OP_DIR)\n",
    "    print(SAVE_DIR)\n",
    "    \n",
    "setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_APE():\n",
    "    global DIR\n",
    "    global OP_DIR\n",
    "    global SAVE_DIR\n",
    "    global DATA_DIR\n",
    "    global CONFIG\n",
    "    global domain_dims\n",
    "    \n",
    "    MODEL_NAME = 'model_APE'\n",
    "    \n",
    "    # ------------ #\n",
    "    \n",
    "    checkpoint_dir = os.path.join(SAVE_DIR,'checkpoints')\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.mkdir(checkpoint_dir)\n",
    "\n",
    "    _DATA_DIR = CONFIG['DATA_DIR']\n",
    "    train_x_pos, train_x_neg, APE_term_2, APE_term_4, _, _,  _,_ = data_fetcher.get_data_APE(\n",
    "        _DATA_DIR,\n",
    "        DIR\n",
    "    )\n",
    "    \n",
    "    print(train_x_pos.shape, train_x_neg.shape, APE_term_2.shape, APE_term_4.shape)\n",
    "    \n",
    "    neg_samples = train_x_neg.shape[1]\n",
    "    start_time = time.time()\n",
    "    num_domains = len(domain_dims)\n",
    "    inp_dims = list(domain_dims.values())\n",
    "\n",
    "    print('Number of domains ', num_domains )\n",
    "    print(' domains ', inp_dims)\n",
    "\n",
    "    ape_model_obj = tf_model_ape_1.model_ape_1(MODEL_NAME)\n",
    "\n",
    "    ape_model_obj.set_model_params(\n",
    "        num_entities = num_domains,\n",
    "        inp_dims = inp_dims,\n",
    "        neg_samples = neg_samples,\n",
    "        batch_size = CONFIG[DIR]['batchsize'],\n",
    "        num_epochs = CONFIG[DIR]['num_epochs'],\n",
    "        lr = CONFIG[DIR]['ape_learning_rate'],\n",
    "        chkpt_dir = checkpoint_dir\n",
    "    )\n",
    "\n",
    "    _emb_size = int(CONFIG[DIR]['ape_embed_size'])\n",
    "    ape_model_obj.set_hyper_parameters(\n",
    "        emb_dims=[_emb_size],\n",
    "        use_bias=[True, False]\n",
    "    )\n",
    "\n",
    "    _use_pretrained = CONFIG[DIR]['ape_use_pretrained']\n",
    "\n",
    "    if _use_pretrained is False:\n",
    "        print(' Building and training model ')\n",
    "        ape_model_obj.build_model()\n",
    "        ape_model_obj.train_model(\n",
    "            train_x_pos,\n",
    "            train_x_neg,\n",
    "            APE_term_2,\n",
    "            APE_term_4\n",
    "        )\n",
    "    \n",
    "    return  \n",
    "\n",
    "    bounds = []\n",
    "    training_pos_scores = model_obj.inference(\n",
    "        train_x_pos\n",
    "    )\n",
    "    training_pos_scores = [_[0] for _ in training_pos_scores]\n",
    "\n",
    "    train_noise = np.reshape(train_x_neg, [-1, train_x_pos.shape[-1]])\n",
    "    training_noise_scores = model_obj.inference(\n",
    "        train_noise\n",
    "    )\n",
    "    training_noise_scores = [_[0] for _ in training_noise_scores]\n",
    "\n",
    "    bounds.append(min(training_noise_scores))\n",
    "    bounds.append(max(training_pos_scores))\n",
    "\n",
    "    for anomaly_type in range(1,3+1):\n",
    "        _, _, _, _, test_x, test_idList, anomaly_x, anomaly_idList = data_fetcher.get_data_APE(\n",
    "            DATA_DIR,\n",
    "            DIR,\n",
    "            anomaly_type\n",
    "        )\n",
    "\n",
    "        '''\n",
    "        join the normal data + anomaly data\n",
    "        join the normal data id +  anomaly data id \n",
    "        Maintain order\n",
    "        '''\n",
    "\n",
    "        test_normal_ids = test_idList\n",
    "        test_anomaly_ids = anomaly_idList\n",
    "        test_ids = list(np.hstack(\n",
    "            [test_normal_ids,\n",
    "             test_anomaly_ids]\n",
    "        ))\n",
    "        print (' Len of test_ids ', len(test_ids))\n",
    "        test_normal_data = test_x\n",
    "        test_anomaly_data = anomaly_x\n",
    "        test_data_x = np.vstack([\n",
    "            test_normal_data,\n",
    "            test_anomaly_data\n",
    "        ])\n",
    "\n",
    "        # ---------- #\n",
    "\n",
    "\n",
    "\n",
    "        # ---------- #\n",
    "\n",
    "        print('Length of test data',test_data_x.shape)\n",
    "        res = model_obj.inference(\n",
    "            test_data_x\n",
    "        )\n",
    "\n",
    "        test_ids = list(test_ids)\n",
    "        print('Length of results ', len(res))\n",
    "\n",
    "\n",
    "        res = list(res)\n",
    "        _id_score_dict = {\n",
    "            id: _res for id, _res in zip(test_ids, res)\n",
    "        }\n",
    "\n",
    "        '''\n",
    "        sort by ascending \n",
    "        since lower likelihood means anomalous\n",
    "        '''\n",
    "        tmp = sorted(\n",
    "            _id_score_dict.items(),\n",
    "            key=operator.itemgetter(1)\n",
    "        )\n",
    "        sorted_id_score_dict = OrderedDict()\n",
    "\n",
    "        for e in tmp:\n",
    "            sorted_id_score_dict[e[0]] = e[1][0]\n",
    "\n",
    "\n",
    "\n",
    "        recall, precison = eval.precision_recall_curve(\n",
    "            sorted_id_score_dict,\n",
    "            anomaly_id_list=test_anomaly_ids\n",
    "        )\n",
    "\n",
    "        recall_str = ','.join([str(_) for _ in recall])\n",
    "        precision_str = ','.join([str(_) for _ in precison])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(140382, 8) (140382, 24, 8) (140382, 1) (140382, 24, 1)\n",
      "Number of domains  8\n",
      " domains  [548, 5113, 95, 238, 64, 113, 116, 6193]\n",
      "save_dir/us_import1/checkpoints/model_APE_10_k_24_frozen.pb\n",
      " Building and training model \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid reduction dimension -1 for input with 0 dimensions. for 'model_3/Sum_28' (op: 'Sum') with input shapes: [], [] and with computed input tensors: input[1] = <-1>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/AD_v2/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[1;32m    685\u001b[0m           \u001b[0mgraph_def_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_def_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m           input_tensors_as_shapes, status)\n\u001b[0m\u001b[1;32m    687\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/AD_v2/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Invalid reduction dimension -1 for input with 0 dimensions. for 'model_3/Sum_28' (op: 'Sum') with input shapes: [], [] and with computed input tensors: input[1] = <-1>.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-9fe5fffd92d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_APE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-45-7d5460872a86>\u001b[0m in \u001b[0;36mrun_APE\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_pretrained\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' Building and training model '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mape_model_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         ape_model_obj.train_model(\n\u001b[1;32m     57\u001b[0m             \u001b[0mtrain_x_pos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Code/AD_v2/AD_v2/src/ape/tf_model_ape_1.py\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(self, inference)\u001b[0m\n\u001b[1;32m    381\u001b[0m                         \u001b[0m_z1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m                         \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m                         keepdims=True)\n\u001b[0m\u001b[1;32m    384\u001b[0m                     \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_ij\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m                     \u001b[0mneg_pair_dp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/AD_v2/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m                 instructions)\n\u001b[0;32m--> 316\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m     return tf_decorator.make_decorator(func, new_func, 'deprecated',\n\u001b[1;32m    318\u001b[0m                                        _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[0;32m~/anaconda3/envs/AD_v2/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mreduce_sum\u001b[0;34m(input_tensor, axis, keepdims, name, reduction_indices, keep_dims)\u001b[0m\n\u001b[1;32m   1338\u001b[0m                                                   reduction_indices),\n\u001b[1;32m   1339\u001b[0m                                    \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1340\u001b[0;31m                                    name=name))\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/AD_v2/lib/python3.6/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36m_sum\u001b[0;34m(input, axis, keep_dims, name)\u001b[0m\n\u001b[1;32m   4899\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m   4900\u001b[0m         \u001b[0;34m\"Sum\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep_dims\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4901\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m   4902\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4903\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/AD_v2/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    788\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/AD_v2/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   3160\u001b[0m         op_def=op_def)\n\u001b[1;32m   3161\u001b[0m     self._create_op_helper(ret, compute_shapes=compute_shapes,\n\u001b[0;32m-> 3162\u001b[0;31m                            compute_device=compute_device)\n\u001b[0m\u001b[1;32m   3163\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/AD_v2/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_op_helper\u001b[0;34m(self, op, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   3206\u001b[0m     \u001b[0;31m# compute_shapes argument.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3207\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3208\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3209\u001b[0m     \u001b[0;31m# TODO(b/XXXX): move to Operation.__init__ once _USE_C_API flag is removed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3210\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/AD_v2/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   2425\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_set_shapes_for_outputs_c_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2426\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2427\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_set_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/AD_v2/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_set_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   2398\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2400\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2401\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2402\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m~/anaconda3/envs/AD_v2/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   2328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2329\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2330\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2332\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/AD_v2/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, require_shape_fn)\u001b[0m\n\u001b[1;32m    625\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    626\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m                                   require_shape_fn)\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/AD_v2/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[1;32m    689\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid reduction dimension -1 for input with 0 dimensions. for 'model_3/Sum_28' (op: 'Sum') with input shapes: [], [] and with computed input tensors: input[1] = <-1>."
     ]
    }
   ],
   "source": [
    "run_APE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
