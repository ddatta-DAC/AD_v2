{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yaml\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import glob \n",
    "import pickle\n",
    "sys.path.append('./../../')\n",
    "sys.path.append('./..')\n",
    "import inspect\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import time\n",
    "import yaml\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import auc\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ddatta/anaconda3/envs/AD_v2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ddatta/anaconda3/envs/AD_v2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ddatta/anaconda3/envs/AD_v2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ddatta/anaconda3/envs/AD_v2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ddatta/anaconda3/envs/AD_v2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ddatta/anaconda3/envs/AD_v2/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/ddatta/anaconda3/envs/AD_v2/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/ddatta/anaconda3/envs/AD_v2/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/ddatta/anaconda3/envs/AD_v2/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/ddatta/anaconda3/envs/AD_v2/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/ddatta/anaconda3/envs/AD_v2/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/ddatta/anaconda3/envs/AD_v2/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From ./../../src/ape/tf_model_ape_1.py:18: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.ape import tf_model_ape_1\n",
    "from src.data_fetcher import data_fetcher\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_DIR :  ../../generated_data_v1\n",
      "OP_DIR : output/us_import1\n",
      "SAVE_DIR : save_dir/us_import1\n"
     ]
    }
   ],
   "source": [
    "CONFIG = None\n",
    "OP_DIR = None\n",
    "DATA_DIR = None\n",
    "domain_dims = None\n",
    "SAVE_DIR = None\n",
    "CONFIG_FILE = 'config_1.yaml'\n",
    "domain_dims = None\n",
    "\n",
    "# ------------------------------------- #\n",
    "def get_domain_dims(dd_file_path):\n",
    "    with open(dd_file_path, 'rb') as fh:\n",
    "        domain_dims = pickle.load(fh)\n",
    "    _tmpDF = pd.DataFrame.from_dict(domain_dims, orient='index')\n",
    "    _tmpDF = _tmpDF.reset_index()\n",
    "    _tmpDF = _tmpDF.rename(columns={'index': 'domain'})\n",
    "    _tmpDF = _tmpDF.sort_values(by=['domain'])\n",
    "    res = {k: v for k, v in zip(_tmpDF['domain'], _tmpDF[0])}\n",
    "    return res\n",
    "# ------------------------------------- #\n",
    "# Set up config & global vars\n",
    "# ------------------------------------- #\n",
    "def setup():\n",
    "    global SAVE_DIR\n",
    "    global DIR\n",
    "    global CONFIG\n",
    "    global OP_DIR\n",
    "    global DATA_DIR\n",
    "    global domain_dims\n",
    "   \n",
    "    with open(CONFIG_FILE) as f:\n",
    "        CONFIG = yaml.safe_load(f)\n",
    "  \n",
    "    SAVE_DIR = CONFIG['SAVE_DIR']\n",
    "    DIR = CONFIG['DIR']\n",
    "        \n",
    "    OP_DIR = CONFIG['OP_DIR']\n",
    "    DATA_DIR = CONFIG['DATA_DIR']\n",
    "    \n",
    "    if not os.path.exists(SAVE_DIR):\n",
    "        os.mkdir(SAVE_DIR)\n",
    "    SAVE_DIR = os.path.join(SAVE_DIR, DIR)\n",
    " \n",
    "    if not os.path.exists(SAVE_DIR):\n",
    "        os.mkdir(SAVE_DIR)\n",
    "\n",
    "    if not os.path.exists(OP_DIR):\n",
    "        os.mkdir(OP_DIR)\n",
    "\n",
    "    OP_DIR = os.path.join(OP_DIR, DIR)\n",
    "    if not os.path.exists(OP_DIR):\n",
    "        os.mkdir(OP_DIR)\n",
    "        \n",
    "    tf_model_ape_1._DIR = DIR\n",
    "    tf_model_ape_1.OP_DIR = OP_DIR\n",
    "    if not os.path.exists(OP_DIR):\n",
    "        os.mkdir(OP_DIR)\n",
    "        \n",
    "    \n",
    "    dd_file_path = os.path.join(DATA_DIR, DIR, 'domain_dims.pkl')\n",
    "    domain_dims = get_domain_dims(dd_file_path)\n",
    " \n",
    "    print('DATA_DIR : ' ,DATA_DIR)\n",
    "    print('OP_DIR :', OP_DIR)\n",
    "    print('SAVE_DIR :', SAVE_DIR)\n",
    "    \n",
    "setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------------------------------- \n",
    "\n",
    "This function gets the testing data\n",
    "Set :\n",
    "1. Anomaly percentage to be 20 % of data: So 1:4 ratio\n",
    "2. Why type 2 ? : Clusters\n",
    "3. what are the configs ? 'test_anomaly_perc'\n",
    "\n",
    "## ----------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_testing_data(anomaly_type = 2):\n",
    "    global DIR\n",
    "    global DATA_DIR\n",
    "    global CONFIG\n",
    "    \n",
    "    p =  int(CONFIG['test_anomaly_perc'])\n",
    "    _, _, _, _, test_x ,test_idList, anomaly_x, anomaly_idList = data_fetcher.get_data_APE(\n",
    "        DATA_DIR,\n",
    "        DIR,\n",
    "        anomaly_type=2\n",
    "    )\n",
    "    print(len(anomaly_idList))\n",
    "    print(len(test_idList))\n",
    "    reqd_anom_count = int(p/(100-p) * len(test_idList))\n",
    "    \n",
    "    anomaly_idList = anomaly_idList[:reqd_anom_count]\n",
    "    anomaly_x = anomaly_x[:reqd_anom_count]\n",
    "    \n",
    "    return  test_x ,test_idList, anomaly_idList, anomaly_x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_APE():\n",
    "    global DIR\n",
    "    global OP_DIR\n",
    "    global SAVE_DIR\n",
    "    global DATA_DIR\n",
    "    global CONFIG\n",
    "    global domain_dims\n",
    "    \n",
    "    MODEL_NAME = 'model_APE'\n",
    "    \n",
    "    # ------------ #\n",
    "    \n",
    "    checkpoint_dir = os.path.join(SAVE_DIR,'checkpoints')\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.mkdir(checkpoint_dir)\n",
    "\n",
    "    _DATA_DIR = CONFIG['DATA_DIR']\n",
    "    train_x_pos, train_x_neg, APE_term_2, APE_term_4, _, _,  _,_ = data_fetcher.get_data_APE(\n",
    "        _DATA_DIR,\n",
    "        DIR\n",
    "    )\n",
    "    \n",
    "    print(train_x_pos.shape, train_x_neg.shape, APE_term_2.shape, APE_term_4.shape)\n",
    "    \n",
    "    neg_samples = train_x_neg.shape[1]\n",
    "    start_time = time.time()\n",
    "    num_domains = len(domain_dims)\n",
    "    inp_dims = list(domain_dims.values())\n",
    "\n",
    "    print('Number of domains ', num_domains )\n",
    "    print(' domains ', inp_dims)\n",
    "    ape_model_obj = tf_model_ape_1.model_ape_1(MODEL_NAME)\n",
    "    \n",
    "    ape_model_obj.set_model_params(\n",
    "        num_entities = num_domains,\n",
    "        inp_dims = inp_dims,\n",
    "        neg_samples = neg_samples,\n",
    "        batch_size = CONFIG[DIR]['ape_batchsize'],\n",
    "        num_epochs = CONFIG[DIR]['num_epochs'],\n",
    "        lr = CONFIG[DIR]['ape_learning_rate'],\n",
    "        chkpt_dir = checkpoint_dir\n",
    "    )\n",
    "\n",
    "    _emb_size = int(CONFIG[DIR]['ape_embed_size'])\n",
    "    ape_model_obj.set_hyper_parameters(\n",
    "        emb_dims=[_emb_size],\n",
    "        use_bias=[True, False]\n",
    "    )\n",
    "\n",
    "    _use_pretrained = CONFIG[DIR]['ape_use_pretrained']\n",
    "\n",
    "    if _use_pretrained is False:\n",
    "        print(' Building and training model ')\n",
    "        ape_model_obj.build_model()\n",
    "        ape_model_obj.train_model(\n",
    "            train_x_pos,\n",
    "            train_x_neg,\n",
    "            APE_term_2,\n",
    "            APE_term_4\n",
    "        )\n",
    "    \n",
    "    \n",
    "    bounds = []\n",
    "    training_pos_scores = model_obj.inference(\n",
    "        train_x_pos\n",
    "    )\n",
    "    \n",
    "    training_pos_scores = [_[0] for _ in training_pos_scores]\n",
    "\n",
    "    train_noise = np.reshape(train_x_neg, [-1, train_x_pos.shape[-1]])\n",
    "    training_noise_scores = model_obj.inference(\n",
    "        train_noise\n",
    "    )\n",
    "    training_noise_scores = [_[0] for _ in training_noise_scores]\n",
    "\n",
    "    bounds.append(min(training_noise_scores))\n",
    "    bounds.append(max(training_pos_scores))\n",
    "\n",
    "    test_x ,test_idList, anomaly_idList, anomaly_x = get_testing_data(anomaly_type = 2)\n",
    "    \n",
    "\n",
    "    '''\n",
    "    join the normal data + anomaly data\n",
    "    join the normal data id +  anomaly data id \n",
    "    Maintain order\n",
    "    '''\n",
    "    test_normal_ids = test_idList\n",
    "    test_anomaly_ids = anomaly_idList\n",
    "    test_ids = list(np.hstack(\n",
    "        [test_normal_ids,\n",
    "         test_anomaly_ids]\n",
    "    ))\n",
    "    print (' Len of test_ids ', len(test_ids))\n",
    "    test_normal_data = test_x\n",
    "    test_anomaly_data = anomaly_x\n",
    "    test_data_x = np.vstack([\n",
    "        test_normal_data,\n",
    "        test_anomaly_data\n",
    "    ])\n",
    "\n",
    "    # ---------- #\n",
    "\n",
    "    print('Length of test data',test_data_x.shape)\n",
    "    res = model_obj.inference(\n",
    "        test_data_x\n",
    "    )\n",
    "\n",
    "    test_ids = list(test_ids)\n",
    "    print('Length of results ', len(res))\n",
    "\n",
    "\n",
    "    res = list(res)\n",
    "    _id_score_dict = {\n",
    "        id: _res for id, _res in zip(test_ids, res)\n",
    "    }\n",
    "\n",
    "    '''\n",
    "    sort by ascending \n",
    "    since lower likelihood means anomalous\n",
    "    '''\n",
    "    tmp = sorted(\n",
    "        _id_score_dict.items(),\n",
    "        key=operator.itemgetter(1)\n",
    "    )\n",
    "    sorted_id_score_dict = OrderedDict()\n",
    "\n",
    "    for e in tmp:\n",
    "        sorted_id_score_dict[e[0]] = e[1][0]\n",
    "        recall, precison = eval.precision_recall_curve(\n",
    "            sorted_id_score_dict,\n",
    "            anomaly_id_list=test_anomaly_ids\n",
    "        )\n",
    "\n",
    "        recall_str = ','.join([str(_) for _ in recall])\n",
    "        precision_str = ','.join([str(_) for _ in precison])\n",
    "    print( 'AUC ::' , auc(precison,recall))\n",
    "    return \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(140382, 8) (140382, 24, 8) (140382, 1) (140382, 24, 1)\n",
      "Number of domains  8\n",
      " domains  [548, 5113, 95, 238, 64, 113, 116, 6193]\n",
      "save_dir/us_import1/checkpoints/model_APE_12_k_24_frozen.pb\n",
      " Building and training model \n",
      "WARNING:tensorflow:From ./../../src/ape/tf_model_ape_1.py:222: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From ./../../src/ape/tf_model_ape_1.py:223: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From ./../../src/ape/tf_model_ape_1.py:90: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From ./../../src/ape/tf_model_ape_1.py:394: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "Epoch ::  1\n",
      "Epoch ::  1 Batch : 0  Loss : -251.0401\n",
      "Epoch ::  1 Batch : 200  Loss : -197.1187\n",
      "Epoch ::  1 Batch : 400  Loss : -196.28271\n",
      "WARNING:tensorflow:From ./../../src/ape/tf_model_ape_1.py:475: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
      "WARNING:tensorflow:From /home/ddatta/anaconda3/envs/AD_v2/lib/python3.7/site-packages/tensorflow/python/framework/graph_util_impl.py:270: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "INFO:tensorflow:Froze 18 variables.\n",
      "INFO:tensorflow:Converted 18 variables to const ops.\n",
      "Saving file.  save_dir/us_import1/checkpoints/model_APE_12_k_24_frozen.pb\n"
     ]
    }
   ],
   "source": [
    "run_APE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
