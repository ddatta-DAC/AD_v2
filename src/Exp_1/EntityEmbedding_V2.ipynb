{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True 0 Tesla P100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from random import shuffle\n",
    "from torch.nn.parameter import Parameter\n",
    "print( torch.cuda.is_available(), torch.cuda.current_device(),torch.cuda.get_device_name(0))\n",
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda:0\"\n",
    "else:\n",
    "    dev = \"cpu\"\n",
    "\n",
    "device = torch.device(dev)\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import os\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from collections import OrderedDict\n",
    "import pickle\n",
    "\n",
    "import multiprocessing as mp\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "try:\n",
    "    from . import utils_1\n",
    "except:\n",
    "    import utils_1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Set up config\n"
     ]
    }
   ],
   "source": [
    "CONFIG_FILE = 'config_1.yaml'\n",
    "DIR = None\n",
    "OP_DIR = None\n",
    "modelData_SaveDir = None\n",
    "DATA_DIR = None\n",
    "num_jobs = None\n",
    "CONFIG = None\n",
    "Refresh_Embeddings = None\n",
    "logger = None\n",
    "domain_dims = None\n",
    "train_data_file = None\n",
    "id_col = 'PanjivaRecordID'\n",
    "# ------ #\n",
    "\n",
    "def get_domain_dims(dd_file_path):\n",
    "    with open(dd_file_path, 'rb') as fh:\n",
    "        domain_dims = pickle.load(fh)\n",
    "    _tmpDF = pd.DataFrame.from_dict(domain_dims, orient='index')\n",
    "    _tmpDF = _tmpDF.reset_index()\n",
    "    _tmpDF = _tmpDF.rename(columns={'index': 'domain'})\n",
    "    _tmpDF = _tmpDF.sort_values(by=['domain'])\n",
    "    res = {k: v for k, v in zip(_tmpDF['domain'], _tmpDF[0])}\n",
    "    return res\n",
    "\n",
    "\n",
    "def setup_config(_DIR=None):\n",
    "    global CONFIG_FILE\n",
    "    global DATA_DIR\n",
    "    global modelData_SaveDir\n",
    "    global OP_DIR\n",
    "    global DIR\n",
    "    global num_jobs\n",
    "    global Refresh_Embeddings\n",
    "    global logger\n",
    "    global CONFIG\n",
    "    global domain_dims\n",
    "    global train_data_file\n",
    "    \n",
    "    with open(CONFIG_FILE) as f:\n",
    "        CONFIG = yaml.safe_load(f)\n",
    "    if _DIR is None:\n",
    "        DIR = CONFIG['DIR']\n",
    "    else:\n",
    "        DIR = _DIR\n",
    "\n",
    "    DATA_DIR = os.path.join(CONFIG['DATA_DIR'])\n",
    "    modelData_SaveDir = os.path.join(\n",
    "        CONFIG['model_data_save_dir'],\n",
    "        DIR\n",
    "    )\n",
    "    train_data_file = CONFIG['train_data_file']\n",
    "    \n",
    "    if not os.path.exists(CONFIG['OP_DIR']):\n",
    "        os.mkdir(CONFIG['OP_DIR'])\n",
    "    OP_DIR = os.path.join(CONFIG['OP_DIR'], DIR)\n",
    "    if not os.path.exists(OP_DIR):\n",
    "        os.mkdir(OP_DIR)\n",
    "\n",
    "    Refresh_Embeddings = CONFIG[DIR]['Refresh_Embeddings']\n",
    "    cpu_count = mp.cpu_count()\n",
    "    num_jobs = min(cpu_count, CONFIG['num_jobs'])\n",
    "\n",
    "    if not os.path.exists(CONFIG['model_data_save_dir']):\n",
    "        os.mkdir(CONFIG['model_data_save_dir'])\n",
    "\n",
    "    if not os.path.exists(modelData_SaveDir):\n",
    "        os.mkdir(modelData_SaveDir)\n",
    "    \n",
    "    domain_dims_file = os.path.join(DATA_DIR, DIR, \"domain_dims.pkl\")\n",
    "    domain_dims = get_domain_dims(domain_dims_file)\n",
    "    print(' Set up config')\n",
    "    return\n",
    "\n",
    "setup_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_coocc_matrix(df, col_1, col_2):\n",
    "    set_elements_1 = set(list(df[col_1]))\n",
    "    set_elements_2 = set(list(df[col_2]))\n",
    "    count_1 = len(set_elements_1)\n",
    "    count_2 = len(set_elements_2)\n",
    "    coocc = np.zeros([count_1, count_2])\n",
    "    df = df[[col_1, col_2]]\n",
    "    new_df = df.groupby([col_1, col_2]).size().reset_index(name='count')\n",
    "\n",
    "    for _, row in new_df.iterrows():\n",
    "        i = row[col_1]\n",
    "        j = row[col_2]\n",
    "        coocc[i][j] = row['count']\n",
    "\n",
    "    print('Col 1 & 2', col_1, col_2, coocc.shape, '>>', (count_1, count_2))\n",
    "    return coocc\n",
    "\n",
    "\n",
    "\n",
    "def get_coOccMatrix_dict(df, id_col):\n",
    "    columns = list(df.columns)\n",
    "    columns.remove(id_col)\n",
    "    columns = list(sorted(columns))\n",
    "    columnWise_coOccMatrix_dict = {}\n",
    "\n",
    "    for i in range(len(columns)):\n",
    "        for j in range(i + 1, len(columns)):\n",
    "            col_1 = columns[i]\n",
    "            col_2 = columns[j]\n",
    "            key = col_1 + '_+_' + col_2\n",
    "            res = create_coocc_matrix(df, col_1, col_2)\n",
    "            columnWise_coOccMatrix_dict[key] = res\n",
    "    columnWise_coOccMatrix_dict = OrderedDict(columnWise_coOccMatrix_dict)\n",
    "    return columnWise_coOccMatrix_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Carrier', 'ConsigneePanjivaID', 'HSCode', 'PortOfLading', 'PortOfUnlading', 'ShipmentDestination', 'ShipmentOrigin', 'ShipperPanjivaID']\n"
     ]
    }
   ],
   "source": [
    "src_DIR = os.path.join(DATA_DIR, DIR)\n",
    "training_data_file = CONFIG['train_data_file']\n",
    "train_df = pd.read_csv(os.path.join(src_DIR, train_data_file))\n",
    "feature_cols = list(train_df.columns)\n",
    "feature_cols = list(feature_cols)\n",
    "feature_cols.remove(id_col)\n",
    "domains = sorted(feature_cols)\n",
    "print(feature_cols)\n",
    "\n",
    "model_data_save_dir = modelData_SaveDir\n",
    "\n",
    "data = train_df[feature_cols].values\n",
    "# ------------------------------- #\n",
    "coOcc_dict_file = os.path.join(model_data_save_dir, \"coOccMatrix_dict.pkl\")\n",
    "X_ij_file = os.path.join(model_data_save_dir, \"X_ij.npy\")\n",
    "domain_dims_file = os.path.join(src_DIR, \"domain_dims.pkl\")\n",
    "domain_dims = get_domain_dims(domain_dims_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Carrier': 548,\n",
       " 'ConsigneePanjivaID': 5113,\n",
       " 'HSCode': 95,\n",
       " 'PortOfLading': 238,\n",
       " 'PortOfUnlading': 64,\n",
       " 'ShipmentDestination': 113,\n",
       " 'ShipmentOrigin': 116,\n",
       " 'ShipperPanjivaID': 6193}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domain_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----\n",
    "# Check if pairwise co-occurrence dictionary exists\n",
    "# -----\n",
    "if os.path.exists(coOcc_dict_file):\n",
    "    with open(coOcc_dict_file, 'rb') as fh:\n",
    "        coOccMatrix_dict = pickle.load(fh)\n",
    "else:\n",
    "    coOccMatrix_dict = get_coOccMatrix_dict(train_df, id_col='PanjivaRecordID')\n",
    "    with open(coOcc_dict_file, \"wb\") as fh:\n",
    "        pickle.dump(coOccMatrix_dict, fh, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "# ----------------\n",
    "# Ensure X_ij \n",
    "# ----------------\n",
    "if os.path.exists(X_ij_file):\n",
    "    with open(X_ij_file, 'rb') as fh:\n",
    "        X_ij = np.load(fh)\n",
    "\n",
    "else:\n",
    "   \n",
    "    nd = len(feature_cols)\n",
    "    X_ij = np.zeros([data.shape[0], nd, nd])\n",
    "    print( X_ij.shape )\n",
    "\n",
    "    for i in range(nd):\n",
    "        for j in range(nd):\n",
    "            if i == j :\n",
    "                for d in range(data.shape[0]):\n",
    "                    X_ij[d][i][j] = 0\n",
    "            else:\n",
    "                if i < j: \n",
    "                    _i =i\n",
    "                    _j =j\n",
    "                else : \n",
    "                    _i =j\n",
    "                    _j =i\n",
    "                key = feature_cols[_i] + '_+_' + feature_cols[_j]\n",
    "                \n",
    "                for d in range(data.shape[0]):\n",
    "                    e1 = data[d][_i]\n",
    "                    e2 = data[d][_j]\n",
    "                    X_ij[d][i][j] = coOccMatrix_dict[key][e1][e2]\n",
    "                    \n",
    "    X_ij = np.asarray(X_ij,np.int32)\n",
    "    with open(X_ij_file, \"wb\") as fh:\n",
    "        np.save(fh, X_ij)\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nd = len(feature_cols)\n",
    "X_ij_max = np.zeros([nd,nd])\n",
    "for i in range(nd):\n",
    "    for j in range(nd):\n",
    "        if i==j : continue\n",
    "        if i < j: \n",
    "            _i =i\n",
    "            _j =j\n",
    "        else : \n",
    "            _i =j\n",
    "            _j =i\n",
    "        key = feature_cols[_i] + '_+_' + feature_cols[_j]\n",
    "        X_ij_max[i][j] = np.max(coOccMatrix_dict[key])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ij_max = X_ij_max+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =================================\n",
    "# Co-occurrence based embedding model\n",
    "# Projecting GloVe to multivariate categorical \n",
    "# =================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate doamin wise MI\n",
    "import math \n",
    "\n",
    "def MI(dff, x, y):\n",
    "    \n",
    "    df = dff.copy()\n",
    "    f_x = 'f_x'\n",
    "    f_y = 'f_y'\n",
    "    \n",
    "    df[f_x] = df.groupby(x)[x].transform('count')/len(df)\n",
    "    df[f_y] = df.groupby(y)[y].transform('count')/len(df)\n",
    "    mi = 0\n",
    "    # run loop\n",
    "    for _x in set(df[x]):\n",
    "        a = list(df.loc[df[x]==_x][f_x])[0] \n",
    "        for _y in set(df[y]):\n",
    "            b = list(df.loc[df[y]==_y][f_y])[0] \n",
    "            c = len(df.loc[(df[x]==_x) & (df[y]==_y)])/len(df)\n",
    "            if c > 0:\n",
    "                mi += c * math.log( c / (a*b))\n",
    "    return -mi\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MI(train_df, 'ConsigneePanjivaID', 'ShipperPanjivaID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y : shape [ ?, d, d]\n",
    "def custom_loss(y_pred, y_true):\n",
    "    # X_ij shape should be [ d,d ]\n",
    "    global X_ij_max\n",
    "    _X_ij_max = torch.FloatTensor(X_ij_max)\n",
    "    a = 0.9\n",
    "    epsilon = 0.000001\n",
    "\n",
    "    e1 = torch.pow(y_pred - torch.log(y_true + epsilon) , 2)\n",
    "    _xij_m = _X_ij_max.repeat(y_pred.size()[0], 1,1)\n",
    "    z = y_true / _xij_m \n",
    "    s1 = torch.pow( torch.clamp(z, 0.0, 1.0),a)\n",
    "    loss = s1 * e1\n",
    "    sample_loss = torch.sum(loss,keepdim = False, dim=-1)\n",
    "    sample_loss = torch.sum(sample_loss,keepdim = False, dim=-1)\n",
    "    return torch.mean(\n",
    "        sample_loss\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        emb_dim,\n",
    "        domain_dims\n",
    "    ):\n",
    "        super(Net, self).__init__()\n",
    "        self.num_domains = len(domain_dims)\n",
    "        self.domain_dims = domain_dims\n",
    "        self.emb_dim = emb_dim\n",
    "        \n",
    "        self.list_W_m = []\n",
    "        self.list_W_c = []\n",
    "        self.list_B_m = []\n",
    "        self.list_B_c = []\n",
    "        \n",
    "        \n",
    "        for d_idx in range(self.num_domains):\n",
    "            e = nn.Embedding(num_embeddings= domain_dims[d_idx], embedding_dim=emb_dim)\n",
    "            e.weight = Parameter(torch.Tensor(torch.empty(self.domain_dims[d_idx], emb_dim).uniform_(-1, 1)))\n",
    "            self.register_parameter('e_'+str(d_idx), e.weight)\n",
    "            self.list_W_m.append(e)\n",
    "            \n",
    "            e1 = nn.Embedding(num_embeddings= domain_dims[d_idx], embedding_dim=emb_dim)\n",
    "            e1.weight = Parameter(torch.Tensor(torch.empty(self.domain_dims[d_idx], emb_dim).uniform_(-1, 1)))\n",
    "            self.register_parameter('e1_'+str(d_idx), e1.weight)\n",
    "            self.list_W_c.append(e1)\n",
    "            \n",
    "            b = nn.Embedding(num_embeddings= domain_dims[d_idx], embedding_dim=1)\n",
    "            b.weight = Parameter(torch.Tensor(torch.empty(domain_dims[d_idx], 1).uniform_(-1, 1)))\n",
    "            self.register_parameter('b_'+str(d_idx), b.weight)\n",
    "            self.list_B_m.append(b)\n",
    "            \n",
    "            b1 = nn.Embedding(num_embeddings=domain_dims[d_idx], embedding_dim=1)\n",
    "            b1._weight = Parameter(torch.Tensor(torch.empty(domain_dims[d_idx], 1).uniform_(-1, 1)))\n",
    "            self.register_parameter('b1_'+str(d_idx), b1.weight)\n",
    "            self.list_B_c.append(b1) \n",
    "            \n",
    "    \n",
    "    # --------------------------------------\n",
    "    # Define network structure\n",
    "    # x : [? , dims]\n",
    "    # --------------------------------------\n",
    "    def forward(self, x):\n",
    "        split_x = torch.chunk(\n",
    "            x, \n",
    "            chunks = self.num_domains, \n",
    "            dim = 1\n",
    "        )\n",
    "        \n",
    "        nd = self.num_domains\n",
    "        res = []\n",
    "        for m_idx in range(nd):\n",
    "            _zero = split_x[m_idx]*0\n",
    "            _zero = _zero.type(torch.FloatTensor).view([-1,1,1])\n",
    "            \n",
    "            \n",
    "            w_i = self.list_W_m[m_idx](split_x[m_idx])\n",
    "            b_i = self.list_B_m[m_idx](split_x[m_idx])\n",
    "            \n",
    "            for c_idx in range(nd):\n",
    "                if m_idx == c_idx : \n",
    "                    res.append(_zero)\n",
    "                else:\n",
    "                    w_j = self.list_W_c[c_idx](split_x[c_idx])\n",
    "                    b_j = self.list_B_c[c_idx](split_x[c_idx])\n",
    "\n",
    "                    s = torch.bmm(\n",
    "                        w_i.view(-1, 1, self.emb_dim), \n",
    "                        w_j.view(-1, self.emb_dim, 1)\n",
    "                    ) \n",
    "                    s = s + b_i + b_j\n",
    "                    \n",
    "                    res.append(s)\n",
    "\n",
    "        # Reshape from  list [ [?,1] ...[?,d*d] ] to  [?, d ,d]\n",
    "      \n",
    "        res = torch.stack(\n",
    "            res,\n",
    "            dim=1\n",
    "        )\n",
    "        res = torch.squeeze(res,dim=-1)\n",
    "        res = torch.squeeze(res,dim=-1)\n",
    "        \n",
    "        res = res.view([-1, nd, nd])\n",
    "        return res\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 256\n",
    "domain_dims_vals = list( domain_dims.values() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(emb_dim,domain_dims_vals)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.005)\n",
    "criterion = custom_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(net.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_df[feature_cols].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140382, 8, 8)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ij.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 25\n",
    "bs = 256\n",
    "log_interval = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = train_x.shape[0]//bs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ::  Epoch: 0, Batch 0, Loss 0.022809\n",
      "Train ::  Epoch: 0, Batch 50, Loss 0.013123\n",
      "Train ::  Epoch: 0, Batch 100, Loss 0.008452\n",
      "Train ::  Epoch: 0, Batch 150, Loss 0.006233\n",
      "Train ::  Epoch: 0, Batch 200, Loss 0.004292\n",
      "Train ::  Epoch: 0, Batch 250, Loss 0.004539\n",
      "Train ::  Epoch: 0, Batch 300, Loss 0.002880\n",
      "Train ::  Epoch: 0, Batch 350, Loss 0.002512\n",
      "Train ::  Epoch: 0, Batch 400, Loss 0.002218\n",
      "Train ::  Epoch: 0, Batch 450, Loss 0.001784\n",
      "Train ::  Epoch: 0, Batch 500, Loss 0.001422\n",
      "Train ::  Epoch: 1, Batch 0, Loss 0.001162\n",
      "Train ::  Epoch: 1, Batch 50, Loss 0.000969\n",
      "Train ::  Epoch: 1, Batch 100, Loss 0.001031\n",
      "Train ::  Epoch: 1, Batch 150, Loss 0.001191\n",
      "Train ::  Epoch: 1, Batch 200, Loss 0.000838\n",
      "Train ::  Epoch: 1, Batch 250, Loss 0.000897\n",
      "Train ::  Epoch: 1, Batch 300, Loss 0.000832\n",
      "Train ::  Epoch: 1, Batch 350, Loss 0.001178\n",
      "Train ::  Epoch: 1, Batch 400, Loss 0.000959\n",
      "Train ::  Epoch: 1, Batch 450, Loss 0.000952\n",
      "Train ::  Epoch: 1, Batch 500, Loss 0.000690\n",
      "Train ::  Epoch: 2, Batch 0, Loss 0.000760\n",
      "Train ::  Epoch: 2, Batch 50, Loss 0.000825\n",
      "Train ::  Epoch: 2, Batch 100, Loss 0.000749\n",
      "Train ::  Epoch: 2, Batch 150, Loss 0.000452\n",
      "Train ::  Epoch: 2, Batch 200, Loss 0.000487\n",
      "Train ::  Epoch: 2, Batch 250, Loss 0.000825\n",
      "Train ::  Epoch: 2, Batch 300, Loss 0.000784\n",
      "Train ::  Epoch: 2, Batch 350, Loss 0.000721\n",
      "Train ::  Epoch: 2, Batch 400, Loss 0.000706\n",
      "Train ::  Epoch: 2, Batch 450, Loss 0.000506\n",
      "Train ::  Epoch: 2, Batch 500, Loss 0.000457\n",
      "Train ::  Epoch: 3, Batch 0, Loss 0.000852\n",
      "Train ::  Epoch: 3, Batch 50, Loss 0.000908\n",
      "Train ::  Epoch: 3, Batch 100, Loss 0.001377\n",
      "Train ::  Epoch: 3, Batch 150, Loss 0.000691\n",
      "Train ::  Epoch: 3, Batch 200, Loss 0.001727\n",
      "Train ::  Epoch: 3, Batch 250, Loss 0.001875\n",
      "Train ::  Epoch: 3, Batch 300, Loss 0.002411\n",
      "Train ::  Epoch: 3, Batch 350, Loss 0.001429\n",
      "Train ::  Epoch: 3, Batch 400, Loss 0.002184\n",
      "Train ::  Epoch: 3, Batch 450, Loss 0.002440\n",
      "Train ::  Epoch: 3, Batch 500, Loss 0.001604\n",
      "Train ::  Epoch: 4, Batch 0, Loss 0.002123\n",
      "Train ::  Epoch: 4, Batch 50, Loss 0.002074\n",
      "Train ::  Epoch: 4, Batch 100, Loss 0.004135\n",
      "Train ::  Epoch: 4, Batch 150, Loss 0.003072\n",
      "Train ::  Epoch: 4, Batch 200, Loss 0.003180\n",
      "Train ::  Epoch: 4, Batch 250, Loss 0.004095\n",
      "Train ::  Epoch: 4, Batch 300, Loss 0.004910\n",
      "Train ::  Epoch: 4, Batch 350, Loss 0.004362\n",
      "Train ::  Epoch: 4, Batch 400, Loss 0.004174\n",
      "Train ::  Epoch: 4, Batch 450, Loss 0.006644\n",
      "Train ::  Epoch: 4, Batch 500, Loss 0.010241\n",
      "Train ::  Epoch: 5, Batch 0, Loss 0.012349\n",
      "Train ::  Epoch: 5, Batch 50, Loss 0.011457\n",
      "Train ::  Epoch: 5, Batch 100, Loss 0.011833\n",
      "Train ::  Epoch: 5, Batch 150, Loss 0.008669\n",
      "Train ::  Epoch: 5, Batch 200, Loss 0.014546\n",
      "Train ::  Epoch: 5, Batch 250, Loss 0.013399\n",
      "Train ::  Epoch: 5, Batch 300, Loss 0.012852\n",
      "Train ::  Epoch: 5, Batch 350, Loss 0.014450\n",
      "Train ::  Epoch: 5, Batch 400, Loss 0.016512\n",
      "Train ::  Epoch: 5, Batch 450, Loss 0.013218\n",
      "Train ::  Epoch: 5, Batch 500, Loss 0.016684\n",
      "Train ::  Epoch: 6, Batch 0, Loss 0.015842\n",
      "Train ::  Epoch: 6, Batch 50, Loss 0.017469\n",
      "Train ::  Epoch: 6, Batch 100, Loss 0.017247\n",
      "Train ::  Epoch: 6, Batch 150, Loss 0.015385\n",
      "Train ::  Epoch: 6, Batch 200, Loss 0.014883\n",
      "Train ::  Epoch: 6, Batch 250, Loss 0.013247\n",
      "Train ::  Epoch: 6, Batch 300, Loss 0.012361\n",
      "Train ::  Epoch: 6, Batch 350, Loss 0.008599\n",
      "Train ::  Epoch: 6, Batch 400, Loss 0.016163\n",
      "Train ::  Epoch: 6, Batch 450, Loss 0.018445\n",
      "Train ::  Epoch: 6, Batch 500, Loss 0.014320\n",
      "Train ::  Epoch: 7, Batch 0, Loss 0.007973\n",
      "Train ::  Epoch: 7, Batch 50, Loss 0.010250\n",
      "Train ::  Epoch: 7, Batch 100, Loss 0.009382\n",
      "Train ::  Epoch: 7, Batch 150, Loss 0.010689\n",
      "Train ::  Epoch: 7, Batch 200, Loss 0.009438\n",
      "Train ::  Epoch: 7, Batch 250, Loss 0.011389\n",
      "Train ::  Epoch: 7, Batch 300, Loss 0.007641\n",
      "Train ::  Epoch: 7, Batch 350, Loss 0.009253\n",
      "Train ::  Epoch: 7, Batch 400, Loss 0.012045\n",
      "Train ::  Epoch: 7, Batch 450, Loss 0.010488\n",
      "Train ::  Epoch: 7, Batch 500, Loss 0.010726\n",
      "Train ::  Epoch: 8, Batch 0, Loss 0.011894\n",
      "Train ::  Epoch: 8, Batch 50, Loss 0.006662\n",
      "Train ::  Epoch: 8, Batch 100, Loss 0.007050\n",
      "Train ::  Epoch: 8, Batch 150, Loss 0.008632\n",
      "Train ::  Epoch: 8, Batch 200, Loss 0.013791\n",
      "Train ::  Epoch: 8, Batch 250, Loss 0.010896\n",
      "Train ::  Epoch: 8, Batch 300, Loss 0.007465\n",
      "Train ::  Epoch: 8, Batch 350, Loss 0.011683\n",
      "Train ::  Epoch: 8, Batch 400, Loss 0.009799\n",
      "Train ::  Epoch: 8, Batch 450, Loss 0.014316\n",
      "Train ::  Epoch: 8, Batch 500, Loss 0.008858\n",
      "Train ::  Epoch: 9, Batch 0, Loss 0.006948\n",
      "Train ::  Epoch: 9, Batch 50, Loss 0.009892\n",
      "Train ::  Epoch: 9, Batch 100, Loss 0.008918\n",
      "Train ::  Epoch: 9, Batch 150, Loss 0.009170\n",
      "Train ::  Epoch: 9, Batch 200, Loss 0.011042\n",
      "Train ::  Epoch: 9, Batch 250, Loss 0.007198\n",
      "Train ::  Epoch: 9, Batch 300, Loss 0.011770\n",
      "Train ::  Epoch: 9, Batch 350, Loss 0.012251\n",
      "Train ::  Epoch: 9, Batch 400, Loss 0.009585\n",
      "Train ::  Epoch: 9, Batch 450, Loss 0.014807\n",
      "Train ::  Epoch: 9, Batch 500, Loss 0.014050\n",
      "Train ::  Epoch: 10, Batch 0, Loss 0.010126\n",
      "Train ::  Epoch: 10, Batch 50, Loss 0.009968\n",
      "Train ::  Epoch: 10, Batch 100, Loss 0.014005\n",
      "Train ::  Epoch: 10, Batch 150, Loss 0.012682\n",
      "Train ::  Epoch: 10, Batch 200, Loss 0.013312\n",
      "Train ::  Epoch: 10, Batch 250, Loss 0.011118\n",
      "Train ::  Epoch: 10, Batch 300, Loss 0.011539\n",
      "Train ::  Epoch: 10, Batch 350, Loss 0.011379\n",
      "Train ::  Epoch: 10, Batch 400, Loss 0.015010\n",
      "Train ::  Epoch: 10, Batch 450, Loss 0.013467\n",
      "Train ::  Epoch: 10, Batch 500, Loss 0.009385\n",
      "Train ::  Epoch: 11, Batch 0, Loss 0.007729\n",
      "Train ::  Epoch: 11, Batch 50, Loss 0.008220\n",
      "Train ::  Epoch: 11, Batch 100, Loss 0.008534\n",
      "Train ::  Epoch: 11, Batch 150, Loss 0.010521\n",
      "Train ::  Epoch: 11, Batch 200, Loss 0.013152\n",
      "Train ::  Epoch: 11, Batch 250, Loss 0.009823\n",
      "Train ::  Epoch: 11, Batch 300, Loss 0.011334\n",
      "Train ::  Epoch: 11, Batch 350, Loss 0.009260\n",
      "Train ::  Epoch: 11, Batch 400, Loss 0.013440\n",
      "Train ::  Epoch: 11, Batch 450, Loss 0.008395\n",
      "Train ::  Epoch: 11, Batch 500, Loss 0.010101\n",
      "Train ::  Epoch: 12, Batch 0, Loss 0.007737\n",
      "Train ::  Epoch: 12, Batch 50, Loss 0.007159\n",
      "Train ::  Epoch: 12, Batch 100, Loss 0.009556\n",
      "Train ::  Epoch: 12, Batch 150, Loss 0.007332\n",
      "Train ::  Epoch: 12, Batch 200, Loss 0.010111\n",
      "Train ::  Epoch: 12, Batch 250, Loss 0.007256\n",
      "Train ::  Epoch: 12, Batch 300, Loss 0.007539\n",
      "Train ::  Epoch: 12, Batch 350, Loss 0.009310\n",
      "Train ::  Epoch: 12, Batch 400, Loss 0.010966\n",
      "Train ::  Epoch: 12, Batch 450, Loss 0.009732\n",
      "Train ::  Epoch: 12, Batch 500, Loss 0.008341\n",
      "Train ::  Epoch: 13, Batch 0, Loss 0.005981\n",
      "Train ::  Epoch: 13, Batch 50, Loss 0.006490\n",
      "Train ::  Epoch: 13, Batch 100, Loss 0.009640\n",
      "Train ::  Epoch: 13, Batch 150, Loss 0.015175\n",
      "Train ::  Epoch: 13, Batch 200, Loss 0.007565\n",
      "Train ::  Epoch: 13, Batch 250, Loss 0.008728\n",
      "Train ::  Epoch: 13, Batch 300, Loss 0.006948\n",
      "Train ::  Epoch: 13, Batch 350, Loss 0.009424\n",
      "Train ::  Epoch: 13, Batch 400, Loss 0.007190\n",
      "Train ::  Epoch: 13, Batch 450, Loss 0.009555\n",
      "Train ::  Epoch: 13, Batch 500, Loss 0.008020\n",
      "Train ::  Epoch: 14, Batch 0, Loss 0.008238\n",
      "Train ::  Epoch: 14, Batch 50, Loss 0.006736\n",
      "Train ::  Epoch: 14, Batch 100, Loss 0.007644\n",
      "Train ::  Epoch: 14, Batch 150, Loss 0.009796\n",
      "Train ::  Epoch: 14, Batch 200, Loss 0.008717\n",
      "Train ::  Epoch: 14, Batch 250, Loss 0.006501\n",
      "Train ::  Epoch: 14, Batch 300, Loss 0.007778\n",
      "Train ::  Epoch: 14, Batch 350, Loss 0.010157\n",
      "Train ::  Epoch: 14, Batch 400, Loss 0.008452\n",
      "Train ::  Epoch: 14, Batch 450, Loss 0.007194\n",
      "Train ::  Epoch: 14, Batch 500, Loss 0.009738\n",
      "Train ::  Epoch: 15, Batch 0, Loss 0.008015\n",
      "Train ::  Epoch: 15, Batch 50, Loss 0.007821\n",
      "Train ::  Epoch: 15, Batch 100, Loss 0.006814\n",
      "Train ::  Epoch: 15, Batch 150, Loss 0.007365\n",
      "Train ::  Epoch: 15, Batch 200, Loss 0.008784\n",
      "Train ::  Epoch: 15, Batch 250, Loss 0.008495\n",
      "Train ::  Epoch: 15, Batch 300, Loss 0.007422\n",
      "Train ::  Epoch: 15, Batch 350, Loss 0.007510\n",
      "Train ::  Epoch: 15, Batch 400, Loss 0.006848\n",
      "Train ::  Epoch: 15, Batch 450, Loss 0.007948\n",
      "Train ::  Epoch: 15, Batch 500, Loss 0.008542\n",
      "Train ::  Epoch: 16, Batch 0, Loss 0.007263\n",
      "Train ::  Epoch: 16, Batch 50, Loss 0.008828\n",
      "Train ::  Epoch: 16, Batch 100, Loss 0.008145\n",
      "Train ::  Epoch: 16, Batch 150, Loss 0.005354\n",
      "Train ::  Epoch: 16, Batch 200, Loss 0.008000\n",
      "Train ::  Epoch: 16, Batch 250, Loss 0.006102\n",
      "Train ::  Epoch: 16, Batch 300, Loss 0.006728\n",
      "Train ::  Epoch: 16, Batch 350, Loss 0.006828\n",
      "Train ::  Epoch: 16, Batch 400, Loss 0.006690\n",
      "Train ::  Epoch: 16, Batch 450, Loss 0.008148\n",
      "Train ::  Epoch: 16, Batch 500, Loss 0.007241\n",
      "Train ::  Epoch: 17, Batch 0, Loss 0.005794\n",
      "Train ::  Epoch: 17, Batch 50, Loss 0.006150\n",
      "Train ::  Epoch: 17, Batch 100, Loss 0.007432\n",
      "Train ::  Epoch: 17, Batch 150, Loss 0.007042\n",
      "Train ::  Epoch: 17, Batch 200, Loss 0.006512\n",
      "Train ::  Epoch: 17, Batch 250, Loss 0.005556\n",
      "Train ::  Epoch: 17, Batch 300, Loss 0.007071\n",
      "Train ::  Epoch: 17, Batch 350, Loss 0.006681\n",
      "Train ::  Epoch: 17, Batch 400, Loss 0.007038\n",
      "Train ::  Epoch: 17, Batch 450, Loss 0.006980\n",
      "Train ::  Epoch: 17, Batch 500, Loss 0.006298\n",
      "Train ::  Epoch: 18, Batch 0, Loss 0.006136\n",
      "Train ::  Epoch: 18, Batch 50, Loss 0.006421\n",
      "Train ::  Epoch: 18, Batch 100, Loss 0.005313\n",
      "Train ::  Epoch: 18, Batch 150, Loss 0.006508\n",
      "Train ::  Epoch: 18, Batch 200, Loss 0.005532\n",
      "Train ::  Epoch: 18, Batch 250, Loss 0.004959\n",
      "Train ::  Epoch: 18, Batch 300, Loss 0.004609\n",
      "Train ::  Epoch: 18, Batch 350, Loss 0.007002\n",
      "Train ::  Epoch: 18, Batch 400, Loss 0.008035\n",
      "Train ::  Epoch: 18, Batch 450, Loss 0.006173\n",
      "Train ::  Epoch: 18, Batch 500, Loss 0.005398\n",
      "Train ::  Epoch: 19, Batch 0, Loss 0.005126\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Shuffle\n",
    "    ind_list = list(range(train_x.shape[0]))\n",
    "    shuffle(ind_list)\n",
    "    _train_x = train_x[ind_list,:]\n",
    "    _y = X_ij[ind_list,:,:]\n",
    "    _y = np.log(_y+1)\n",
    "    \n",
    "    for batch_idx in range(num_batches+1):\n",
    "        _x_pos = _train_x[batch_idx*bs:(batch_idx+1)*bs]\n",
    "        _y_true = _y[batch_idx*bs:(batch_idx+1)*bs]\n",
    "        # feed tensor\n",
    "        _x_pos = torch.LongTensor(_x_pos)\n",
    "        _y_true = torch.FloatTensor(_y_true)\n",
    "        # ----- #\n",
    "        optimizer.zero_grad()\n",
    "        output = net(_x_pos)\n",
    "       \n",
    "        loss = criterion(output, _y_true)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # ----- #\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train ::  Epoch: {}, Batch {}, Loss {:4f}'.format(epoch, batch_idx,loss))\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_dict = OrderedDict({})\n",
    "\n",
    "for i in range(len(domain_dims)):\n",
    "    arr = (net.list_W_m[i].weight.detach().numpy() + net.list_W_c[i].weight.detach().numpy())/2\n",
    "    print(arr.shape)\n",
    "    _D = list(domain_dims.items())[i][0]\n",
    "    weight_dict[_D] = arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    hscodeList = [10,25,35,40,50,55,75,90]\n",
    "\n",
    "    for hscode in hscodeList:\n",
    "        print('-----> ::: ',hscode)\n",
    "        # find the 10 closest  to ShipmentDestination to HSCode in data\n",
    "        df = train_df.loc[train_df['HSCode'] == hscode]\n",
    "        df = df.groupby(['HSCode', 'PortOfLading']).size().reset_index(name='counts')\n",
    "        df = df.sort_values(by=['counts'])\n",
    "        print(df)\n",
    "        k_closest = df.tail(10)['PortOfLading'].values\n",
    "        print(k_closest)\n",
    "\n",
    "        # hs_code_vec = wt[0][hscode] + bias[0][hscode]\n",
    "        hs_code_vec = weight_dict['HSCode'][hscode]\n",
    "\n",
    "        shp_dest_vec = []\n",
    "        wt = weight_dict['PortOfLading']\n",
    "        for i in range(wt.shape[0]):\n",
    "            r = wt[i]\n",
    "            shp_dest_vec.append(r)\n",
    "\n",
    "        res = {}\n",
    "        for i in range(wt.shape[0]):\n",
    "            a = np.reshape(shp_dest_vec[i], [1, -1])\n",
    "            b = np.reshape(hs_code_vec, [1, -1])\n",
    "            res[i] = cosine_similarity(a, b)[0][0]\n",
    "\n",
    "        new_df = pd.DataFrame(list(res.items()))\n",
    "        new_df = new_df.sort_values(by=[1])\n",
    "        print(new_df)\n",
    "        new_df = new_df.tail(10)\n",
    "        print(list(new_df[0]))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----> :::  10\n",
      "   HSCode  PortOfLading  counts\n",
      "0      10            63       1\n",
      "1      10            79       1\n",
      "2      10           107       1\n",
      "3      10           234       4\n",
      "[ 63  79 107 234]\n",
      "       0         1\n",
      "196  196 -0.136272\n",
      "228  228 -0.130471\n",
      "144  144 -0.117312\n",
      "131  131 -0.113933\n",
      "214  214 -0.110282\n",
      "..   ...       ...\n",
      "204  204  0.111222\n",
      "47    47  0.119891\n",
      "70    70  0.123156\n",
      "60    60  0.124895\n",
      "49    49  0.146232\n",
      "\n",
      "[238 rows x 2 columns]\n",
      "[56, 93, 48, 110, 23, 204, 47, 70, 60, 49]\n",
      "-----> :::  25\n",
      "    HSCode  PortOfLading  counts\n",
      "18      25           139       1\n",
      "35      25           232       1\n",
      "31      25           202       1\n",
      "19      25           145       1\n",
      "36      25           234       1\n",
      "17      25           138       1\n",
      "14      25           119       1\n",
      "37      25           235       1\n",
      "4       25            60       1\n",
      "6       25            70       1\n",
      "5       25            66       2\n",
      "23      25           175       2\n",
      "26      25           183       2\n",
      "2       25            54       2\n",
      "32      25           208       2\n",
      "28      25           185       2\n",
      "1       25            53       2\n",
      "34      25           223       2\n",
      "12      25           103       3\n",
      "30      25           196       3\n",
      "29      25           190       3\n",
      "9       25            84       3\n",
      "25      25           179       3\n",
      "8       25            78       4\n",
      "21      25           155       7\n",
      "7       25            71       8\n",
      "24      25           177      11\n",
      "11      25           102      19\n",
      "0       25             4      20\n",
      "3       25            58      20\n",
      "15      25           126      24\n",
      "33      25           217      42\n",
      "27      25           184      42\n",
      "13      25           106      43\n",
      "22      25           164      46\n",
      "10      25            96      47\n",
      "16      25           130      61\n",
      "20      25           146     244\n",
      "[  4  58 126 217 184 106 164  96 130 146]\n",
      "       0         1\n",
      "124  124 -0.151250\n",
      "88    88 -0.118406\n",
      "157  157 -0.116407\n",
      "231  231 -0.114923\n",
      "45    45 -0.114453\n",
      "..   ...       ...\n",
      "100  100  0.114885\n",
      "226  226  0.117275\n",
      "127  127  0.120715\n",
      "49    49  0.141356\n",
      "168  168  0.152742\n",
      "\n",
      "[238 rows x 2 columns]\n",
      "[24, 224, 149, 198, 129, 100, 226, 127, 49, 168]\n",
      "-----> :::  35\n",
      "   HSCode  PortOfLading  counts\n",
      "0      35            27       1\n",
      "1      35            80       1\n",
      "2      35            96       1\n",
      "5      35           151       1\n",
      "6      35           164       1\n",
      "4      35           146       3\n",
      "7      35           184      11\n",
      "3      35           130      16\n",
      "[ 27  80  96 151 164 146 184 130]\n",
      "       0         1\n",
      "73    73 -0.185308\n",
      "47    47 -0.138367\n",
      "1      1 -0.134558\n",
      "94    94 -0.122299\n",
      "33    33 -0.110354\n",
      "..   ...       ...\n",
      "141  141  0.141938\n",
      "149  149  0.143365\n",
      "217  217  0.147536\n",
      "162  162  0.156378\n",
      "220  220  0.167796\n",
      "\n",
      "[238 rows x 2 columns]\n",
      "[87, 189, 84, 0, 13, 141, 149, 217, 162, 220]\n",
      "-----> :::  40\n",
      "    HSCode  PortOfLading  counts\n",
      "19      40           155       1\n",
      "2       40            17       1\n",
      "4       40            60       1\n",
      "17      40           134       1\n",
      "6       40            78       1\n",
      "7       40            79       1\n",
      "14      40           114       1\n",
      "3       40            19       2\n",
      "9       40            85       2\n",
      "11      40            96       2\n",
      "12      40           104       3\n",
      "10      40            91       3\n",
      "15      40           126       3\n",
      "16      40           130       4\n",
      "24      40           220       4\n",
      "23      40           217       4\n",
      "5       40            66       4\n",
      "1       40            10       4\n",
      "21      40           211       5\n",
      "0       40             5      10\n",
      "8       40            84      14\n",
      "13      40           111      24\n",
      "22      40           216      26\n",
      "18      40           138      29\n",
      "20      40           184      29\n",
      "[217  66  10 211   5  84 111 216 138 184]\n",
      "       0         1\n",
      "224  224 -0.176747\n",
      "46    46 -0.128527\n",
      "16    16 -0.125231\n",
      "212  212 -0.120115\n",
      "49    49 -0.115920\n",
      "..   ...       ...\n",
      "147  147  0.157613\n",
      "82    82  0.158300\n",
      "150  150  0.158338\n",
      "58    58  0.165349\n",
      "164  164  0.191470\n",
      "\n",
      "[238 rows x 2 columns]\n",
      "[127, 232, 124, 26, 35, 147, 82, 150, 58, 164]\n",
      "-----> :::  50\n",
      "     HSCode  PortOfLading  counts\n",
      "33       50            51       1\n",
      "153      50           207       1\n",
      "152      50           206       1\n",
      "150      50           203       1\n",
      "160      50           215       1\n",
      "..      ...           ...     ...\n",
      "88       50           126    2761\n",
      "173      50           232    3508\n",
      "161      50           217    3847\n",
      "69       50            96    4039\n",
      "91       50           130    8167\n",
      "\n",
      "[178 rows x 3 columns]\n",
      "[234 146 164  54 184 126 232 217  96 130]\n",
      "       0         1\n",
      "100  100 -0.102750\n",
      "68    68 -0.092102\n",
      "16    16 -0.082167\n",
      "1      1 -0.060597\n",
      "104  104 -0.057751\n",
      "..   ...       ...\n",
      "184  184  0.117731\n",
      "33    33  0.117829\n",
      "176  176  0.120491\n",
      "126  126  0.125786\n",
      "236  236  0.171909\n",
      "\n",
      "[238 rows x 2 columns]\n",
      "[217, 24, 97, 164, 130, 184, 33, 176, 126, 236]\n",
      "-----> :::  55\n",
      "    HSCode  PortOfLading  counts\n",
      "0       55            19       1\n",
      "16      55           199       1\n",
      "13      55           159       1\n",
      "11      55           126       1\n",
      "6       55            84       1\n",
      "18      55           225       1\n",
      "4       55            60       1\n",
      "3       55            57       1\n",
      "2       55            46       1\n",
      "10      55           111       3\n",
      "8       55           102       4\n",
      "14      55           163       4\n",
      "12      55           130       5\n",
      "15      55           184       6\n",
      "1       55            41       6\n",
      "5       55            71       7\n",
      "17      55           220       8\n",
      "9       55           103       8\n",
      "7       55            96      11\n",
      "[111 102 163 130 184  41  71 220 103  96]\n",
      "       0         1\n",
      "153  153 -0.163132\n",
      "87    87 -0.148037\n",
      "58    58 -0.143343\n",
      "15    15 -0.126650\n",
      "30    30 -0.116725\n",
      "..   ...       ...\n",
      "158  158  0.141357\n",
      "149  149  0.142473\n",
      "59    59  0.145950\n",
      "122  122  0.153131\n",
      "77    77  0.153237\n",
      "\n",
      "[238 rows x 2 columns]\n",
      "[234, 27, 188, 170, 189, 158, 149, 59, 122, 77]\n",
      "-----> :::  75\n",
      "    HSCode  PortOfLading  counts\n",
      "0       75            35       1\n",
      "2       75            82       1\n",
      "4       75           106       1\n",
      "5       75           126       1\n",
      "7       75           145       1\n",
      "8       75           177       1\n",
      "9       75           184       1\n",
      "10      75           234       2\n",
      "3       75            96       5\n",
      "6       75           130      10\n",
      "1       75            54      20\n",
      "[ 82 106 126 145 177 184 234  96 130  54]\n",
      "       0         1\n",
      "186  186 -0.185518\n",
      "100  100 -0.155018\n",
      "144  144 -0.145902\n",
      "216  216 -0.135451\n",
      "30    30 -0.131212\n",
      "..   ...       ...\n",
      "156  156  0.125444\n",
      "134  134  0.134885\n",
      "133  133  0.139738\n",
      "102  102  0.171284\n",
      "16    16  0.174045\n",
      "\n",
      "[238 rows x 2 columns]\n",
      "[44, 210, 143, 129, 168, 156, 134, 133, 102, 16]\n",
      "-----> :::  90\n",
      "    HSCode  PortOfLading  counts\n",
      "52      90           235       1\n",
      "27      90           121       1\n",
      "39      90           181       1\n",
      "19      90            78       1\n",
      "18      90            76       1\n",
      "17      90            72       1\n",
      "29      90           130       1\n",
      "14      90            60       1\n",
      "12      90            56       1\n",
      "43      90           199       1\n",
      "15      90            63       1\n",
      "9       90            40       1\n",
      "7       90            36       1\n",
      "30      90           132       1\n",
      "20      90            82       2\n",
      "44      90           208       2\n",
      "2       90            10       2\n",
      "35      90           147       2\n",
      "46      90           216       3\n",
      "40      90           183       3\n",
      "5       90            33       3\n",
      "4       90            23       3\n",
      "23      90            98       3\n",
      "31      90           135       4\n",
      "32      90           138       4\n",
      "8       90            37       4\n",
      "24      90           102       4\n",
      "36      90           155       4\n",
      "41      90           184       5\n",
      "49      90           223       5\n",
      "48      90           222       6\n",
      "0       90             5       6\n",
      "3       90            19       6\n",
      "42      90           190       7\n",
      "50      90           232       7\n",
      "25      90           107       7\n",
      "11      90            54       7\n",
      "28      90           126       7\n",
      "6       90            35       8\n",
      "26      90           111       8\n",
      "22      90            96       9\n",
      "21      90            84       9\n",
      "37      90           164      13\n",
      "34      90           146      13\n",
      "51      90           234      13\n",
      "45      90           211      14\n",
      "13      90            57      15\n",
      "47      90           217      17\n",
      "16      90            66      21\n",
      "38      90           177      21\n",
      "33      90           139      23\n",
      "10      90            46      70\n",
      "1       90             7     129\n",
      "[146 234 211  57 217  66 177 139  46   7]\n",
      "       0         1\n",
      "197  197 -0.118758\n",
      "92    92 -0.118335\n",
      "198  198 -0.112969\n",
      "205  205 -0.107464\n",
      "171  171 -0.102160\n",
      "..   ...       ...\n",
      "95    95  0.130902\n",
      "1      1  0.145227\n",
      "130  130  0.147907\n",
      "29    29  0.151434\n",
      "100  100  0.182994\n",
      "\n",
      "[238 rows x 2 columns]\n",
      "[53, 83, 43, 162, 80, 95, 1, 130, 29, 100]\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
