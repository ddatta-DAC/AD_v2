{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True 0 Tesla P100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from random import shuffle\n",
    "from torch.nn.parameter import Parameter\n",
    "print( torch.cuda.is_available(), torch.cuda.current_device(),torch.cuda.get_device_name(0))\n",
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda:0\"\n",
    "else:\n",
    "    dev = \"cpu\"\n",
    "\n",
    "device = torch.device(dev)\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import os\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from collections import OrderedDict\n",
    "import pickle\n",
    "\n",
    "import multiprocessing as mp\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "try:\n",
    "    from . import utils_1\n",
    "except:\n",
    "    import utils_1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Set up config\n"
     ]
    }
   ],
   "source": [
    "CONFIG_FILE = 'config_1.yaml'\n",
    "DIR = None\n",
    "OP_DIR = None\n",
    "modelData_SaveDir = None\n",
    "DATA_DIR = None\n",
    "num_jobs = None\n",
    "CONFIG = None\n",
    "Refresh_Embeddings = None\n",
    "logger = None\n",
    "domain_dims = None\n",
    "train_data_file = None\n",
    "id_col = 'PanjivaRecordID'\n",
    "# ------ #\n",
    "\n",
    "def get_domain_dims(dd_file_path):\n",
    "    with open(dd_file_path, 'rb') as fh:\n",
    "        domain_dims = pickle.load(fh)\n",
    "    _tmpDF = pd.DataFrame.from_dict(domain_dims, orient='index')\n",
    "    _tmpDF = _tmpDF.reset_index()\n",
    "    _tmpDF = _tmpDF.rename(columns={'index': 'domain'})\n",
    "    _tmpDF = _tmpDF.sort_values(by=['domain'])\n",
    "    res = {k: v for k, v in zip(_tmpDF['domain'], _tmpDF[0])}\n",
    "    return res\n",
    "\n",
    "\n",
    "def setup_config(_DIR=None):\n",
    "    global CONFIG_FILE\n",
    "    global DATA_DIR\n",
    "    global modelData_SaveDir\n",
    "    global OP_DIR\n",
    "    global DIR\n",
    "    global num_jobs\n",
    "    global Refresh_Embeddings\n",
    "    global logger\n",
    "    global CONFIG\n",
    "    global domain_dims\n",
    "    global train_data_file\n",
    "    \n",
    "    with open(CONFIG_FILE) as f:\n",
    "        CONFIG = yaml.safe_load(f)\n",
    "    if _DIR is None:\n",
    "        DIR = CONFIG['DIR']\n",
    "    else:\n",
    "        DIR = _DIR\n",
    "\n",
    "    DATA_DIR = os.path.join(CONFIG['DATA_DIR'])\n",
    "    modelData_SaveDir = os.path.join(\n",
    "        CONFIG['model_data_save_dir'],\n",
    "        DIR\n",
    "    )\n",
    "    train_data_file = CONFIG['train_data_file']\n",
    "    \n",
    "    if not os.path.exists(CONFIG['OP_DIR']):\n",
    "        os.mkdir(CONFIG['OP_DIR'])\n",
    "    OP_DIR = os.path.join(CONFIG['OP_DIR'], DIR)\n",
    "    if not os.path.exists(OP_DIR):\n",
    "        os.mkdir(OP_DIR)\n",
    "\n",
    "    Refresh_Embeddings = CONFIG[DIR]['Refresh_Embeddings']\n",
    "    cpu_count = mp.cpu_count()\n",
    "    num_jobs = min(cpu_count, CONFIG['num_jobs'])\n",
    "\n",
    "    if not os.path.exists(CONFIG['model_data_save_dir']):\n",
    "        os.mkdir(CONFIG['model_data_save_dir'])\n",
    "\n",
    "    if not os.path.exists(modelData_SaveDir):\n",
    "        os.mkdir(modelData_SaveDir)\n",
    "    \n",
    "    domain_dims_file = os.path.join(DATA_DIR, DIR, \"domain_dims.pkl\")\n",
    "    domain_dims = get_domain_dims(domain_dims_file)\n",
    "    print(' Set up config')\n",
    "    return\n",
    "\n",
    "setup_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_coocc_matrix(df, col_1, col_2):\n",
    "    set_elements_1 = set(list(df[col_1]))\n",
    "    set_elements_2 = set(list(df[col_2]))\n",
    "    count_1 = len(set_elements_1)\n",
    "    count_2 = len(set_elements_2)\n",
    "    coocc = np.zeros([count_1, count_2])\n",
    "    df = df[[col_1, col_2]]\n",
    "    new_df = df.groupby([col_1, col_2]).size().reset_index(name='count')\n",
    "\n",
    "    for _, row in new_df.iterrows():\n",
    "        i = row[col_1]\n",
    "        j = row[col_2]\n",
    "        coocc[i][j] = row['count']\n",
    "\n",
    "    print('Col 1 & 2', col_1, col_2, coocc.shape, '>>', (count_1, count_2))\n",
    "    return coocc\n",
    "\n",
    "\n",
    "\n",
    "def get_coOccMatrix_dict(df, id_col):\n",
    "    columns = list(df.columns)\n",
    "    columns.remove(id_col)\n",
    "    columns = list(sorted(columns))\n",
    "    columnWise_coOccMatrix_dict = {}\n",
    "\n",
    "    for i in range(len(columns)):\n",
    "        for j in range(i + 1, len(columns)):\n",
    "            col_1 = columns[i]\n",
    "            col_2 = columns[j]\n",
    "            key = col_1 + '_+_' + col_2\n",
    "            res = create_coocc_matrix(df, col_1, col_2)\n",
    "            columnWise_coOccMatrix_dict[key] = res\n",
    "    columnWise_coOccMatrix_dict = OrderedDict(columnWise_coOccMatrix_dict)\n",
    "    return columnWise_coOccMatrix_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Carrier', 'ConsigneePanjivaID', 'HSCode', 'PortOfLading', 'PortOfUnlading', 'ShipmentDestination', 'ShipmentOrigin', 'ShipperPanjivaID']\n"
     ]
    }
   ],
   "source": [
    "src_DIR = os.path.join(DATA_DIR, DIR)\n",
    "training_data_file = CONFIG['train_data_file']\n",
    "train_df = pd.read_csv(os.path.join(src_DIR, train_data_file))\n",
    "feature_cols = list(train_df.columns)\n",
    "feature_cols = list(feature_cols)\n",
    "feature_cols.remove(id_col)\n",
    "domains = sorted(feature_cols)\n",
    "print(feature_cols)\n",
    "\n",
    "model_data_save_dir = modelData_SaveDir\n",
    "\n",
    "data = train_df[feature_cols].values\n",
    "# ------------------------------- #\n",
    "coOcc_dict_file = os.path.join(model_data_save_dir, \"coOccMatrix_dict.pkl\")\n",
    "X_ij_file = os.path.join(model_data_save_dir, \"X_ij.npy\")\n",
    "domain_dims_file = os.path.join(src_DIR, \"domain_dims.pkl\")\n",
    "domain_dims = get_domain_dims(domain_dims_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Carrier': 548,\n",
       " 'ConsigneePanjivaID': 5113,\n",
       " 'HSCode': 95,\n",
       " 'PortOfLading': 238,\n",
       " 'PortOfUnlading': 64,\n",
       " 'ShipmentDestination': 113,\n",
       " 'ShipmentOrigin': 116,\n",
       " 'ShipperPanjivaID': 6193}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "domain_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----\n",
    "# Check if pairwise co-occurrence dictionary exists\n",
    "# -----\n",
    "if os.path.exists(coOcc_dict_file):\n",
    "    with open(coOcc_dict_file, 'rb') as fh:\n",
    "        coOccMatrix_dict = pickle.load(fh)\n",
    "else:\n",
    "    coOccMatrix_dict = get_coOccMatrix_dict(train_df, id_col='PanjivaRecordID')\n",
    "    with open(coOcc_dict_file, \"wb\") as fh:\n",
    "        pickle.dump(coOccMatrix_dict, fh, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "# ----------------\n",
    "# Ensure X_ij \n",
    "# ----------------\n",
    "if os.path.exists(X_ij_file):\n",
    "    with open(X_ij_file, 'rb') as fh:\n",
    "        X_ij = np.load(fh)\n",
    "\n",
    "else:\n",
    "   \n",
    "    nd = len(feature_cols)\n",
    "    X_ij = np.zeros([data.shape[0], nd, nd])\n",
    "    print( X_ij.shape )\n",
    "\n",
    "    for i in range(nd):\n",
    "        for j in range(nd):\n",
    "            if i == j :\n",
    "                for d in range(data.shape[0]):\n",
    "                    X_ij[d][i][j] = 0\n",
    "            else:\n",
    "                if i < j: \n",
    "                    _i =i\n",
    "                    _j =j\n",
    "                else : \n",
    "                    _i =j\n",
    "                    _j =i\n",
    "                key = feature_cols[_i] + '_+_' + feature_cols[_j]\n",
    "                \n",
    "                for d in range(data.shape[0]):\n",
    "                    e1 = data[d][_i]\n",
    "                    e2 = data[d][_j]\n",
    "                    X_ij[d][i][j] = coOccMatrix_dict[key][e1][e2]\n",
    "                    \n",
    "    X_ij = np.asarray(X_ij,np.int32)\n",
    "    with open(X_ij_file, \"wb\") as fh:\n",
    "        np.save(fh, X_ij)\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nd = len(feature_cols)\n",
    "X_ij_max = np.zeros([nd,nd])\n",
    "for i in range(nd):\n",
    "    for j in range(nd):\n",
    "        if i==j : continue\n",
    "        if i < j: \n",
    "            _i =i\n",
    "            _j =j\n",
    "        else : \n",
    "            _i =j\n",
    "            _j =i\n",
    "        key = feature_cols[_i] + '_+_' + feature_cols[_j]\n",
    "        X_ij_max[i][j] = np.max(coOccMatrix_dict[key])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ij_max = X_ij_max+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =================================\n",
    "# Co-occurrence based embedding model\n",
    "# Projecting GloVe to multivariate categorical \n",
    "# =================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y : shape [ ?, d, d]\n",
    "def custom_loss(y_pred, y_true):\n",
    "    # X_ij shape should be [ d,d ]\n",
    "    global X_ij_max\n",
    "    _X_ij_max = torch.FloatTensor(X_ij_max)\n",
    "    a = 0.5\n",
    "    epsilon = 0.000001\n",
    "\n",
    "    e1 = torch.pow(y_pred - torch.log(y_true + epsilon) , 2)\n",
    "#     _xij_m = torch.cat(y_pred.shape[0]*[_X_ij_max])\n",
    "    _xij_m = _X_ij_max.repeat(y_pred.size()[0], 1,1)\n",
    "    z = y_true / _xij_m \n",
    "    s1 = torch.pow( torch.clamp(z, 0.0, 1.0),a)\n",
    "    loss = s1 * e1\n",
    "    sample_loss = torch.sum(loss,keepdim = False, dim=-1)\n",
    "    sample_loss = torch.sum(sample_loss,keepdim = False, dim=-1)\n",
    "    return torch.mean(\n",
    "        sample_loss\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        emb_dim,\n",
    "        domain_dims\n",
    "    ):\n",
    "        super(Net, self).__init__()\n",
    "        self.num_domains = len(domain_dims)\n",
    "        self.domain_dims = domain_dims\n",
    "        self.emb_dim = emb_dim\n",
    "        \n",
    "        self.list_W_m = []\n",
    "        self.list_W_c = []\n",
    "        self.list_B_m = []\n",
    "        self.list_B_c = []\n",
    "        \n",
    "#         self.e = nn.Embedding(num_embeddings= domain_dims[0], embedding_dim=emb_dim)\n",
    "#         self.e.weight = Parameter(torch.Tensor(torch.empty(domain_dims[0], emb_dim).uniform_(-1, 1)))\n",
    "        \n",
    "        for d_idx in range(self.num_domains):\n",
    "            e = nn.Embedding(num_embeddings= domain_dims[d_idx], embedding_dim=emb_dim)\n",
    "            e.weight = Parameter(torch.Tensor(torch.empty(self.domain_dims[d_idx], emb_dim).uniform_(-1, 1)))\n",
    "            self.register_parameter('e_'+str(d_idx), e.weight)\n",
    "            self.list_W_m.append(e)\n",
    "            \n",
    "#             e1 = nn.Embedding(num_embeddings= domain_dims[d_idx], embedding_dim=emb_dim)\n",
    "#             e1.weight = Parameter(torch.Tensor(torch.empty(self.domain_dims[d_idx], emb_dim).uniform_(-1, 1)))\n",
    "#             self.register_parameter('e1_'+str(d_idx), e1.weight)\n",
    "#             self.list_W_c.append(e1)\n",
    "            \n",
    "            b = nn.Embedding(num_embeddings= domain_dims[d_idx], embedding_dim=1)\n",
    "            b.weight = Parameter(torch.Tensor(torch.empty(domain_dims[d_idx], 1).uniform_(-1, 1)))\n",
    "            self.register_parameter('b_'+str(d_idx), b.weight)\n",
    "            self.list_B_m.append(b)\n",
    "            \n",
    "#             b1 = nn.Embedding(num_embeddings=domain_dims[d_idx], embedding_dim=1)\n",
    "#             b1._weight = Parameter(torch.Tensor(torch.empty(domain_dims[d_idx], 1).uniform_(-1, 1)))\n",
    "#             self.register_parameter('b1_'+str(d_idx), b1.weight)\n",
    "#             self.list_B_c.append(b1) \n",
    "            \n",
    "    \n",
    "    \n",
    "    # --------------------------------------\n",
    "    # Define network structure\n",
    "    # x : [? , dims]\n",
    "    # --------------------------------------\n",
    "    def forward(self, x):\n",
    "        split_x = torch.chunk(\n",
    "            x, \n",
    "            chunks = self.num_domains, \n",
    "            dim = 1\n",
    "        )\n",
    "        \n",
    "        nd = self.num_domains\n",
    "        res = []\n",
    "        for m_idx in range(nd):\n",
    "            _zero = split_x[m_idx]*0\n",
    "            _zero = _zero.type(torch.FloatTensor).view([-1,1,1])\n",
    "            \n",
    "            \n",
    "            w_i = self.list_W_m[m_idx](split_x[m_idx])\n",
    "            b_i = self.list_B_m[m_idx](split_x[m_idx])\n",
    "            \n",
    "            for c_idx in range(nd):\n",
    "                if m_idx == c_idx : \n",
    "                    res.append(_zero)\n",
    "                else:\n",
    "                    w_j = self.list_W_m[c_idx](split_x[c_idx])\n",
    "                    b_j = self.list_B_m[c_idx](split_x[c_idx])\n",
    "\n",
    "                    s = torch.bmm(\n",
    "                        w_i.view(-1, 1, self.emb_dim), \n",
    "                        w_j.view(-1, self.emb_dim, 1)\n",
    "                    ) \n",
    "                    s = s + b_i + b_j\n",
    "                    \n",
    "                    res.append(s)\n",
    "\n",
    "        # Reshape from  list [ [?,1] ...[?,d*d] ] to  [?, d ,d]\n",
    "      \n",
    "        res = torch.stack(\n",
    "            res,\n",
    "            dim=1\n",
    "        )\n",
    "        res = torch.squeeze(res,dim=-1)\n",
    "        \n",
    "        res = torch.squeeze(res,dim=-1)\n",
    "        \n",
    "        res = res.view([-1, nd, nd])\n",
    "        return res\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 256\n",
    "domain_dims_vals = list( domain_dims.values() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(emb_dim,domain_dims_vals)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "criterion = custom_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(net.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_df[feature_cols].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140382, 8, 8)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ij.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 25\n",
    "bs = 256\n",
    "log_interval = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = train_x.shape[0]//bs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train ::  Epoch: 0, Batch 0, Loss 1565.781006\n",
      "Train ::  Epoch: 0, Batch 50, Loss 759.853210\n",
      "Train ::  Epoch: 0, Batch 100, Loss 406.476044\n",
      "Train ::  Epoch: 0, Batch 150, Loss 243.695419\n",
      "Train ::  Epoch: 0, Batch 200, Loss 157.073227\n",
      "Train ::  Epoch: 0, Batch 250, Loss 110.142258\n",
      "Train ::  Epoch: 0, Batch 300, Loss 79.698776\n",
      "Train ::  Epoch: 0, Batch 350, Loss 57.867954\n",
      "Train ::  Epoch: 0, Batch 400, Loss 52.459496\n",
      "Train ::  Epoch: 0, Batch 450, Loss 40.097603\n",
      "Train ::  Epoch: 0, Batch 500, Loss 37.207954\n",
      "Train ::  Epoch: 1, Batch 0, Loss 32.085220\n",
      "Train ::  Epoch: 1, Batch 50, Loss 25.074657\n",
      "Train ::  Epoch: 1, Batch 100, Loss 22.791748\n",
      "Train ::  Epoch: 1, Batch 150, Loss 19.727421\n",
      "Train ::  Epoch: 1, Batch 200, Loss 17.088781\n",
      "Train ::  Epoch: 1, Batch 250, Loss 15.921331\n",
      "Train ::  Epoch: 1, Batch 300, Loss 12.981608\n",
      "Train ::  Epoch: 1, Batch 350, Loss 12.816526\n",
      "Train ::  Epoch: 1, Batch 400, Loss 12.145791\n",
      "Train ::  Epoch: 1, Batch 450, Loss 11.652188\n",
      "Train ::  Epoch: 1, Batch 500, Loss 10.210524\n",
      "Train ::  Epoch: 2, Batch 0, Loss 8.740579\n",
      "Train ::  Epoch: 2, Batch 50, Loss 8.302199\n",
      "Train ::  Epoch: 2, Batch 100, Loss 6.665402\n",
      "Train ::  Epoch: 2, Batch 150, Loss 6.762632\n",
      "Train ::  Epoch: 2, Batch 200, Loss 7.317950\n",
      "Train ::  Epoch: 2, Batch 250, Loss 5.476325\n",
      "Train ::  Epoch: 2, Batch 300, Loss 6.337983\n",
      "Train ::  Epoch: 2, Batch 350, Loss 5.798812\n",
      "Train ::  Epoch: 2, Batch 400, Loss 5.552505\n",
      "Train ::  Epoch: 2, Batch 450, Loss 4.962680\n",
      "Train ::  Epoch: 2, Batch 500, Loss 4.969464\n",
      "Train ::  Epoch: 3, Batch 0, Loss 3.753544\n",
      "Train ::  Epoch: 3, Batch 50, Loss 4.280892\n",
      "Train ::  Epoch: 3, Batch 100, Loss 4.436104\n",
      "Train ::  Epoch: 3, Batch 150, Loss 3.794436\n",
      "Train ::  Epoch: 3, Batch 200, Loss 2.701538\n",
      "Train ::  Epoch: 3, Batch 250, Loss 2.908685\n",
      "Train ::  Epoch: 3, Batch 300, Loss 3.041650\n",
      "Train ::  Epoch: 3, Batch 350, Loss 3.035235\n",
      "Train ::  Epoch: 3, Batch 400, Loss 3.193234\n",
      "Train ::  Epoch: 3, Batch 450, Loss 2.662952\n",
      "Train ::  Epoch: 3, Batch 500, Loss 2.795464\n",
      "Train ::  Epoch: 4, Batch 0, Loss 2.224692\n",
      "Train ::  Epoch: 4, Batch 50, Loss 2.298919\n",
      "Train ::  Epoch: 4, Batch 100, Loss 1.570099\n",
      "Train ::  Epoch: 4, Batch 150, Loss 1.803711\n",
      "Train ::  Epoch: 4, Batch 200, Loss 1.953431\n",
      "Train ::  Epoch: 4, Batch 250, Loss 1.681813\n",
      "Train ::  Epoch: 4, Batch 300, Loss 1.985212\n",
      "Train ::  Epoch: 4, Batch 350, Loss 1.692884\n",
      "Train ::  Epoch: 4, Batch 400, Loss 1.723433\n",
      "Train ::  Epoch: 4, Batch 450, Loss 1.471968\n",
      "Train ::  Epoch: 4, Batch 500, Loss 1.279357\n",
      "Train ::  Epoch: 5, Batch 0, Loss 1.280119\n",
      "Train ::  Epoch: 5, Batch 50, Loss 1.326126\n",
      "Train ::  Epoch: 5, Batch 100, Loss 1.323879\n",
      "Train ::  Epoch: 5, Batch 150, Loss 1.082009\n",
      "Train ::  Epoch: 5, Batch 200, Loss 1.172898\n",
      "Train ::  Epoch: 5, Batch 250, Loss 1.057557\n",
      "Train ::  Epoch: 5, Batch 300, Loss 0.960695\n",
      "Train ::  Epoch: 5, Batch 350, Loss 1.097515\n",
      "Train ::  Epoch: 5, Batch 400, Loss 1.093106\n",
      "Train ::  Epoch: 5, Batch 450, Loss 1.048126\n",
      "Train ::  Epoch: 5, Batch 500, Loss 0.959407\n",
      "Train ::  Epoch: 6, Batch 0, Loss 0.734529\n",
      "Train ::  Epoch: 6, Batch 50, Loss 0.906216\n",
      "Train ::  Epoch: 6, Batch 100, Loss 0.889434\n",
      "Train ::  Epoch: 6, Batch 150, Loss 0.884263\n",
      "Train ::  Epoch: 6, Batch 200, Loss 0.899868\n",
      "Train ::  Epoch: 6, Batch 250, Loss 0.726734\n",
      "Train ::  Epoch: 6, Batch 300, Loss 0.763244\n",
      "Train ::  Epoch: 6, Batch 350, Loss 0.981745\n",
      "Train ::  Epoch: 6, Batch 400, Loss 0.668092\n",
      "Train ::  Epoch: 6, Batch 450, Loss 0.635138\n",
      "Train ::  Epoch: 6, Batch 500, Loss 0.803047\n",
      "Train ::  Epoch: 7, Batch 0, Loss 0.664326\n",
      "Train ::  Epoch: 7, Batch 50, Loss 0.547993\n",
      "Train ::  Epoch: 7, Batch 100, Loss 0.470201\n",
      "Train ::  Epoch: 7, Batch 150, Loss 0.536046\n",
      "Train ::  Epoch: 7, Batch 200, Loss 0.531412\n",
      "Train ::  Epoch: 7, Batch 250, Loss 0.778898\n",
      "Train ::  Epoch: 7, Batch 300, Loss 0.604187\n",
      "Train ::  Epoch: 7, Batch 350, Loss 0.620276\n",
      "Train ::  Epoch: 7, Batch 400, Loss 0.464360\n",
      "Train ::  Epoch: 7, Batch 450, Loss 0.371043\n",
      "Train ::  Epoch: 7, Batch 500, Loss 0.429045\n",
      "Train ::  Epoch: 8, Batch 0, Loss 0.394515\n",
      "Train ::  Epoch: 8, Batch 50, Loss 0.507654\n",
      "Train ::  Epoch: 8, Batch 100, Loss 0.423209\n",
      "Train ::  Epoch: 8, Batch 150, Loss 0.451083\n",
      "Train ::  Epoch: 8, Batch 200, Loss 0.503520\n",
      "Train ::  Epoch: 8, Batch 250, Loss 0.362439\n",
      "Train ::  Epoch: 8, Batch 300, Loss 0.440956\n",
      "Train ::  Epoch: 8, Batch 350, Loss 0.428266\n",
      "Train ::  Epoch: 8, Batch 400, Loss 0.325892\n",
      "Train ::  Epoch: 8, Batch 450, Loss 0.295788\n",
      "Train ::  Epoch: 8, Batch 500, Loss 0.290686\n",
      "Train ::  Epoch: 9, Batch 0, Loss 0.243720\n",
      "Train ::  Epoch: 9, Batch 50, Loss 0.245041\n",
      "Train ::  Epoch: 9, Batch 100, Loss 0.258549\n",
      "Train ::  Epoch: 9, Batch 150, Loss 0.427963\n",
      "Train ::  Epoch: 9, Batch 200, Loss 0.223969\n",
      "Train ::  Epoch: 9, Batch 250, Loss 0.308751\n",
      "Train ::  Epoch: 9, Batch 300, Loss 0.300321\n",
      "Train ::  Epoch: 9, Batch 350, Loss 0.308709\n",
      "Train ::  Epoch: 9, Batch 400, Loss 0.298138\n",
      "Train ::  Epoch: 9, Batch 450, Loss 0.245471\n",
      "Train ::  Epoch: 9, Batch 500, Loss 0.278822\n",
      "Train ::  Epoch: 10, Batch 0, Loss 0.206078\n",
      "Train ::  Epoch: 10, Batch 50, Loss 0.221293\n",
      "Train ::  Epoch: 10, Batch 100, Loss 0.185749\n",
      "Train ::  Epoch: 10, Batch 150, Loss 0.171877\n",
      "Train ::  Epoch: 10, Batch 200, Loss 0.246284\n",
      "Train ::  Epoch: 10, Batch 250, Loss 0.189991\n",
      "Train ::  Epoch: 10, Batch 300, Loss 0.147011\n",
      "Train ::  Epoch: 10, Batch 350, Loss 0.195231\n",
      "Train ::  Epoch: 10, Batch 400, Loss 0.209490\n",
      "Train ::  Epoch: 10, Batch 450, Loss 0.218949\n",
      "Train ::  Epoch: 10, Batch 500, Loss 0.165174\n",
      "Train ::  Epoch: 11, Batch 0, Loss 0.149384\n",
      "Train ::  Epoch: 11, Batch 50, Loss 0.162085\n",
      "Train ::  Epoch: 11, Batch 100, Loss 0.200898\n",
      "Train ::  Epoch: 11, Batch 150, Loss 0.163160\n",
      "Train ::  Epoch: 11, Batch 200, Loss 0.167084\n",
      "Train ::  Epoch: 11, Batch 250, Loss 0.137074\n",
      "Train ::  Epoch: 11, Batch 300, Loss 0.168463\n",
      "Train ::  Epoch: 11, Batch 350, Loss 0.116343\n",
      "Train ::  Epoch: 11, Batch 400, Loss 0.136611\n",
      "Train ::  Epoch: 11, Batch 450, Loss 0.115283\n",
      "Train ::  Epoch: 11, Batch 500, Loss 0.126897\n",
      "Train ::  Epoch: 12, Batch 0, Loss 0.097562\n",
      "Train ::  Epoch: 12, Batch 50, Loss 0.149881\n",
      "Train ::  Epoch: 12, Batch 100, Loss 0.084844\n",
      "Train ::  Epoch: 12, Batch 150, Loss 0.103662\n",
      "Train ::  Epoch: 12, Batch 200, Loss 0.117134\n",
      "Train ::  Epoch: 12, Batch 250, Loss 0.102296\n",
      "Train ::  Epoch: 12, Batch 300, Loss 0.067187\n",
      "Train ::  Epoch: 12, Batch 350, Loss 0.121032\n",
      "Train ::  Epoch: 12, Batch 400, Loss 0.093129\n",
      "Train ::  Epoch: 12, Batch 450, Loss 0.100324\n",
      "Train ::  Epoch: 12, Batch 500, Loss 0.084727\n",
      "Train ::  Epoch: 13, Batch 0, Loss 0.118542\n",
      "Train ::  Epoch: 13, Batch 50, Loss 0.112733\n",
      "Train ::  Epoch: 13, Batch 100, Loss 0.115716\n",
      "Train ::  Epoch: 13, Batch 150, Loss 0.074553\n",
      "Train ::  Epoch: 13, Batch 200, Loss 0.118569\n",
      "Train ::  Epoch: 13, Batch 250, Loss 0.108069\n",
      "Train ::  Epoch: 13, Batch 300, Loss 0.120419\n",
      "Train ::  Epoch: 13, Batch 350, Loss 0.099289\n",
      "Train ::  Epoch: 13, Batch 400, Loss 0.152258\n",
      "Train ::  Epoch: 13, Batch 450, Loss 0.125291\n",
      "Train ::  Epoch: 13, Batch 500, Loss 0.087992\n",
      "Train ::  Epoch: 14, Batch 0, Loss 0.092143\n",
      "Train ::  Epoch: 14, Batch 50, Loss 0.067456\n",
      "Train ::  Epoch: 14, Batch 100, Loss 0.073332\n",
      "Train ::  Epoch: 14, Batch 150, Loss 0.069607\n",
      "Train ::  Epoch: 14, Batch 200, Loss 0.102176\n",
      "Train ::  Epoch: 14, Batch 250, Loss 0.071248\n",
      "Train ::  Epoch: 14, Batch 300, Loss 0.082755\n",
      "Train ::  Epoch: 14, Batch 350, Loss 0.092726\n",
      "Train ::  Epoch: 14, Batch 400, Loss 0.088274\n",
      "Train ::  Epoch: 14, Batch 450, Loss 0.080586\n",
      "Train ::  Epoch: 14, Batch 500, Loss 0.078486\n",
      "Train ::  Epoch: 15, Batch 0, Loss 0.070179\n",
      "Train ::  Epoch: 15, Batch 50, Loss 0.083174\n",
      "Train ::  Epoch: 15, Batch 100, Loss 0.039550\n",
      "Train ::  Epoch: 15, Batch 150, Loss 0.072361\n",
      "Train ::  Epoch: 15, Batch 200, Loss 0.085303\n",
      "Train ::  Epoch: 15, Batch 250, Loss 0.082631\n",
      "Train ::  Epoch: 15, Batch 300, Loss 0.100639\n",
      "Train ::  Epoch: 15, Batch 350, Loss 0.085916\n",
      "Train ::  Epoch: 15, Batch 400, Loss 0.066886\n",
      "Train ::  Epoch: 15, Batch 450, Loss 0.069492\n",
      "Train ::  Epoch: 15, Batch 500, Loss 0.067484\n",
      "Train ::  Epoch: 16, Batch 0, Loss 0.060696\n",
      "Train ::  Epoch: 16, Batch 50, Loss 0.061808\n",
      "Train ::  Epoch: 16, Batch 100, Loss 0.060928\n",
      "Train ::  Epoch: 16, Batch 150, Loss 0.053041\n",
      "Train ::  Epoch: 16, Batch 200, Loss 0.034981\n",
      "Train ::  Epoch: 16, Batch 250, Loss 0.078308\n",
      "Train ::  Epoch: 16, Batch 300, Loss 0.061467\n",
      "Train ::  Epoch: 16, Batch 350, Loss 0.066878\n",
      "Train ::  Epoch: 16, Batch 400, Loss 0.081620\n",
      "Train ::  Epoch: 16, Batch 450, Loss 0.058684\n",
      "Train ::  Epoch: 16, Batch 500, Loss 0.059138\n",
      "Train ::  Epoch: 17, Batch 0, Loss 0.042023\n",
      "Train ::  Epoch: 17, Batch 50, Loss 0.050974\n",
      "Train ::  Epoch: 17, Batch 100, Loss 0.054711\n",
      "Train ::  Epoch: 17, Batch 150, Loss 0.063422\n",
      "Train ::  Epoch: 17, Batch 200, Loss 0.070833\n",
      "Train ::  Epoch: 17, Batch 250, Loss 0.053153\n",
      "Train ::  Epoch: 17, Batch 300, Loss 0.050504\n",
      "Train ::  Epoch: 17, Batch 350, Loss 0.058005\n",
      "Train ::  Epoch: 17, Batch 400, Loss 0.081768\n",
      "Train ::  Epoch: 17, Batch 450, Loss 0.064902\n",
      "Train ::  Epoch: 17, Batch 500, Loss 0.052902\n",
      "Train ::  Epoch: 18, Batch 0, Loss 0.054994\n",
      "Train ::  Epoch: 18, Batch 50, Loss 0.047112\n",
      "Train ::  Epoch: 18, Batch 100, Loss 0.054998\n",
      "Train ::  Epoch: 18, Batch 150, Loss 0.057686\n",
      "Train ::  Epoch: 18, Batch 200, Loss 0.047840\n",
      "Train ::  Epoch: 18, Batch 250, Loss 0.058594\n",
      "Train ::  Epoch: 18, Batch 300, Loss 0.063559\n",
      "Train ::  Epoch: 18, Batch 350, Loss 0.065401\n",
      "Train ::  Epoch: 18, Batch 400, Loss 0.071680\n",
      "Train ::  Epoch: 18, Batch 450, Loss 0.041992\n",
      "Train ::  Epoch: 18, Batch 500, Loss 0.059280\n",
      "Train ::  Epoch: 19, Batch 0, Loss 0.051301\n",
      "Train ::  Epoch: 19, Batch 50, Loss 0.066546\n",
      "Train ::  Epoch: 19, Batch 100, Loss 0.066516\n",
      "Train ::  Epoch: 19, Batch 150, Loss 0.040314\n",
      "Train ::  Epoch: 19, Batch 200, Loss 0.039908\n",
      "Train ::  Epoch: 19, Batch 250, Loss 0.039124\n",
      "Train ::  Epoch: 19, Batch 300, Loss 0.047840\n",
      "Train ::  Epoch: 19, Batch 350, Loss 0.046727\n",
      "Train ::  Epoch: 19, Batch 400, Loss 0.053039\n",
      "Train ::  Epoch: 19, Batch 450, Loss 0.042337\n",
      "Train ::  Epoch: 19, Batch 500, Loss 0.047476\n",
      "Train ::  Epoch: 20, Batch 0, Loss 0.059601\n",
      "Train ::  Epoch: 20, Batch 50, Loss 0.056355\n",
      "Train ::  Epoch: 20, Batch 100, Loss 0.038049\n",
      "Train ::  Epoch: 20, Batch 150, Loss 0.037492\n",
      "Train ::  Epoch: 20, Batch 200, Loss 0.044887\n",
      "Train ::  Epoch: 20, Batch 250, Loss 0.069375\n",
      "Train ::  Epoch: 20, Batch 300, Loss 0.058771\n",
      "Train ::  Epoch: 20, Batch 350, Loss 0.053900\n",
      "Train ::  Epoch: 20, Batch 400, Loss 0.039086\n",
      "Train ::  Epoch: 20, Batch 450, Loss 0.048529\n",
      "Train ::  Epoch: 20, Batch 500, Loss 0.037925\n",
      "Train ::  Epoch: 21, Batch 0, Loss 0.051286\n",
      "Train ::  Epoch: 21, Batch 50, Loss 0.054245\n",
      "Train ::  Epoch: 21, Batch 100, Loss 0.049300\n",
      "Train ::  Epoch: 21, Batch 150, Loss 0.037144\n",
      "Train ::  Epoch: 21, Batch 200, Loss 0.037857\n",
      "Train ::  Epoch: 21, Batch 250, Loss 0.042859\n",
      "Train ::  Epoch: 21, Batch 300, Loss 0.051197\n",
      "Train ::  Epoch: 21, Batch 350, Loss 0.047903\n",
      "Train ::  Epoch: 21, Batch 400, Loss 0.061488\n",
      "Train ::  Epoch: 21, Batch 450, Loss 0.040808\n",
      "Train ::  Epoch: 21, Batch 500, Loss 0.038910\n",
      "Train ::  Epoch: 22, Batch 0, Loss 0.050528\n",
      "Train ::  Epoch: 22, Batch 50, Loss 0.048898\n",
      "Train ::  Epoch: 22, Batch 100, Loss 0.052512\n",
      "Train ::  Epoch: 22, Batch 150, Loss 0.062423\n",
      "Train ::  Epoch: 22, Batch 200, Loss 0.063054\n",
      "Train ::  Epoch: 22, Batch 250, Loss 0.041225\n",
      "Train ::  Epoch: 22, Batch 300, Loss 0.055251\n",
      "Train ::  Epoch: 22, Batch 350, Loss 0.048018\n",
      "Train ::  Epoch: 22, Batch 400, Loss 0.046302\n",
      "Train ::  Epoch: 22, Batch 450, Loss 0.037800\n",
      "Train ::  Epoch: 22, Batch 500, Loss 0.050883\n",
      "Train ::  Epoch: 23, Batch 0, Loss 0.041012\n",
      "Train ::  Epoch: 23, Batch 50, Loss 0.043229\n",
      "Train ::  Epoch: 23, Batch 100, Loss 0.059863\n",
      "Train ::  Epoch: 23, Batch 150, Loss 0.042256\n",
      "Train ::  Epoch: 23, Batch 200, Loss 0.046293\n",
      "Train ::  Epoch: 23, Batch 250, Loss 0.045371\n",
      "Train ::  Epoch: 23, Batch 300, Loss 0.055540\n",
      "Train ::  Epoch: 23, Batch 350, Loss 0.059903\n",
      "Train ::  Epoch: 23, Batch 400, Loss 0.055109\n",
      "Train ::  Epoch: 23, Batch 450, Loss 0.047728\n",
      "Train ::  Epoch: 23, Batch 500, Loss 0.050482\n",
      "Train ::  Epoch: 24, Batch 0, Loss 0.047442\n",
      "Train ::  Epoch: 24, Batch 50, Loss 0.054937\n",
      "Train ::  Epoch: 24, Batch 100, Loss 0.041356\n",
      "Train ::  Epoch: 24, Batch 150, Loss 0.043745\n",
      "Train ::  Epoch: 24, Batch 200, Loss 0.066936\n",
      "Train ::  Epoch: 24, Batch 250, Loss 0.038926\n",
      "Train ::  Epoch: 24, Batch 300, Loss 0.058184\n",
      "Train ::  Epoch: 24, Batch 350, Loss 0.056602\n",
      "Train ::  Epoch: 24, Batch 400, Loss 0.050671\n",
      "Train ::  Epoch: 24, Batch 450, Loss 0.042951\n",
      "Train ::  Epoch: 24, Batch 500, Loss 0.055552\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Shuffle\n",
    "    ind_list = list(range(train_x.shape[0]))\n",
    "    shuffle(ind_list)\n",
    "    _train_x = train_x[ind_list,:]\n",
    "    _y = X_ij[ind_list,:,:]\n",
    "    \n",
    "    for batch_idx in range(num_batches+1):\n",
    "        _x_pos = _train_x[batch_idx*bs:(batch_idx+1)*bs]\n",
    "        _y_true = _y[batch_idx*bs:(batch_idx+1)*bs]\n",
    "        # feed tensor\n",
    "        _x_pos = torch.LongTensor(_x_pos)\n",
    "        _y_true = torch.FloatTensor(_y_true)\n",
    "        # ----- #\n",
    "        optimizer.zero_grad()\n",
    "        output = net(_x_pos)\n",
    "       \n",
    "        loss = criterion(output, _y_true)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # ----- #\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train ::  Epoch: {}, Batch {}, Loss {:4f}'.format(epoch, batch_idx,loss))\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
