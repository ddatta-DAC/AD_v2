{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import os\n",
    "from keras import Model\n",
    "from keras.layers import Input, Embedding, Dot, Reshape, Add\n",
    "from keras.layers import Lambda\n",
    "import sys\n",
    "import os\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from collections import OrderedDict\n",
    "import pickle\n",
    "import logging\n",
    "import multiprocessing as mp\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "try:\n",
    "    from . import utils_1\n",
    "except:\n",
    "    import utils_1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ij_max = None\n",
    "# =================================\n",
    "# Co-occurrence based embedding model\n",
    "# Projecting GloVe to multivariate categorical \n",
    "# =================================\n",
    "\n",
    "\n",
    "def get_model(\n",
    "    domain_dimesnsions = None,\n",
    "    num_domains = 4,\n",
    "    embed_dim = 16,\n",
    "    _X_ij_max = None\n",
    "):\n",
    "\n",
    "    global X_ij_max\n",
    "    X_ij_max = _X_ij_max\n",
    "    embedding_layer = []\n",
    "    bias_layer = []\n",
    "\n",
    "    input_layer = Input(\n",
    "        shape=(num_domains,)\n",
    "    )\n",
    "\n",
    "    # =======================\n",
    "    # Input record\n",
    "    # =======================\n",
    "    split_input_record = Lambda(\n",
    "        lambda x:\n",
    "        tf.split(\n",
    "            x,\n",
    "            num_or_size_splits=num_domains,\n",
    "            axis=-1\n",
    "        ),\n",
    "        name='split_layer'\n",
    "    )(input_layer)\n",
    "    \n",
    "    for i in range(num_domains):\n",
    "        emb_i = Embedding(\n",
    "            input_dim = domain_dimesnsions[i],\n",
    "            output_dim= embed_dim,\n",
    "            embeddings_initializer='random_uniform',\n",
    "            name='embedding_w_'+str(i)\n",
    "        )(split_input_record[i])\n",
    "        embedding_layer.append(emb_i)\n",
    "\n",
    "        bias_i = Embedding(\n",
    "            input_dim = domain_dimesnsions[i],\n",
    "            output_dim=1,\n",
    "            input_length=1,\n",
    "            embeddings_initializer='random_uniform',\n",
    "            name= 'embedding_b_'+str(i)\n",
    "        )(split_input_record[i])\n",
    "        bias_layer.append(bias_i)\n",
    "\n",
    "    y_pred = []\n",
    "\n",
    "    for i in range(num_domains):\n",
    "        for j in range(i+1,num_domains):\n",
    "            w_i__w_j = Dot(axes=-1)([\n",
    "                embedding_layer[i],\n",
    "                embedding_layer[j]\n",
    "            ])\n",
    "            w_i__w_j = Reshape(target_shape=(1,))(w_i__w_j)\n",
    "            pred_logXij = Add()([w_i__w_j, bias_layer[i],bias_layer[j]])\n",
    "            pred_logXij = Reshape(target_shape=(1,))(pred_logXij)\n",
    "            y_pred.append(pred_logXij)\n",
    "\n",
    "    y_pred_stacked = Lambda(\n",
    "        lambda x:\n",
    "        tf.stack(\n",
    "            x,\n",
    "            axis=1\n",
    "        ),\n",
    "        name='stack_layer'\n",
    "    )(y_pred)\n",
    "\n",
    "    y_pred_final = Lambda(\n",
    "        lambda x:\n",
    "        tf.squeeze(\n",
    "            x,\n",
    "            axis=-1\n",
    "        ),\n",
    "        name='squeeze_layer'\n",
    "    )(y_pred_stacked)\n",
    "\n",
    "    model = Model(\n",
    "        input_layer,\n",
    "        y_pred_final\n",
    "    )\n",
    "    model.compile(\n",
    "        loss = custom_loss_function,\n",
    "        optimizer='adam'\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def custom_loss_function(\n",
    "        y_true,\n",
    "        y_pred\n",
    "):\n",
    "    global X_ij_max\n",
    "    a = 0.75\n",
    "    epsilon = 0.000001\n",
    "\n",
    "    _err1 = K.square(y_pred - K.log(y_true + epsilon))\n",
    "    _scale1 = K.pow(\n",
    "        K.clip(y_true / X_ij_max, 0.0, 1.0),\n",
    "        a\n",
    "    )\n",
    "    loss = _scale1 * _err1\n",
    "    return K.sum(\n",
    "        loss,\n",
    "        axis=-1\n",
    "    )\n",
    "\n",
    "def train_model(\n",
    "        model,\n",
    "        x,\n",
    "        y_true,\n",
    "        file_save_loc,\n",
    "        epochs=100\n",
    "):\n",
    "    model.summary()\n",
    "    model.fit(\n",
    "        x=x,\n",
    "        y=y_true,\n",
    "        batch_size=256,\n",
    "        epochs=epochs,\n",
    "        verbose=1,\n",
    "        shuffle=True\n",
    "    )\n",
    "    save_model(model,file_save_loc)\n",
    "\n",
    "    return model\n",
    "\n",
    "def save_model(model, file_save_loc):\n",
    "\n",
    "    for layer in model.layers:\n",
    "        if 'embedding_w' in layer.name:\n",
    "            f_path = os.path.join( file_save_loc, layer.name + \".npy\")\n",
    "            np.save(f_path, arr=layer.get_weights()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Global variables ===================== #\n",
    "CONFIG_FILE = 'config_1.yaml'\n",
    "# ============================================================ #\n",
    "\n",
    "\n",
    "CONFIG_FILE = 'config_1.yaml'\n",
    "DIR = None\n",
    "OP_DIR = None\n",
    "modelData_SaveDir = None\n",
    "DATA_DIR = None\n",
    "num_jobs = None\n",
    "CONFIG = None\n",
    "Refresh_Embeddings = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Set up config\n"
     ]
    }
   ],
   "source": [
    "def setup_config(_DIR=None):\n",
    "    global CONFIG_FILE\n",
    "    global DATA_DIR\n",
    "    global modelData_SaveDir\n",
    "    global OP_DIR\n",
    "    global DIR\n",
    "    global num_jobs\n",
    "    global Refresh_Embeddings\n",
    "    global CONFIG\n",
    "\n",
    "    with open(CONFIG_FILE) as f:\n",
    "        CONFIG = yaml.safe_load(f)\n",
    "    if _DIR is None:\n",
    "        DIR = CONFIG['DIR']\n",
    "    else:\n",
    "        DIR = _DIR\n",
    "\n",
    "    DATA_DIR = os.path.join(CONFIG['DATA_DIR'])\n",
    "\n",
    "    modelData_SaveDir = os.path.join(\n",
    "        CONFIG['model_data_save_dir'],\n",
    "        DIR\n",
    "    )\n",
    "\n",
    "    if not os.path.exists(CONFIG['OP_DIR']):\n",
    "        os.mkdir(CONFIG['OP_DIR'])\n",
    "    OP_DIR = os.path.join(CONFIG['OP_DIR'], DIR)\n",
    "    if not os.path.exists(OP_DIR):\n",
    "        os.mkdir(OP_DIR)\n",
    "\n",
    "    Refresh_Embeddings = CONFIG[DIR]['Refresh_Embeddings']\n",
    "    cpu_count = mp.cpu_count()\n",
    "    num_jobs = min(cpu_count, CONFIG['num_jobs'])\n",
    "\n",
    "    if not os.path.exists(CONFIG['model_data_save_dir']):\n",
    "        os.mkdir(CONFIG['model_data_save_dir'])\n",
    "\n",
    "    if not os.path.exists(modelData_SaveDir):\n",
    "        os.mkdir(modelData_SaveDir)\n",
    "    \n",
    "    print(' Set up config')\n",
    "    return\n",
    "\n",
    "setup_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_coocc_matrix(df, col_1, col_2):\n",
    "    set_elements_1 = set(list(df[col_1]))\n",
    "    set_elements_2 = set(list(df[col_2]))\n",
    "    count_1 = len(set_elements_1)\n",
    "    count_2 = len(set_elements_2)\n",
    "    coocc = np.zeros([count_1, count_2])\n",
    "    df = df[[col_1, col_2]]\n",
    "    new_df = df.groupby([col_1, col_2]).size().reset_index(name='count')\n",
    "\n",
    "    for _, row in new_df.iterrows():\n",
    "        i = row[col_1]\n",
    "        j = row[col_2]\n",
    "        coocc[i][j] = row['count']\n",
    "\n",
    "    print('Col 1 & 2', col_1, col_2, coocc.shape, '>>', (count_1, count_2))\n",
    "    return coocc\n",
    "\n",
    "\n",
    "\n",
    "def get_coOccMatrix_dict(df, id_col):\n",
    "    columns = list(df.columns)\n",
    "    columns.remove(id_col)\n",
    "    columns = list(sorted(columns))\n",
    "    columnWise_coOccMatrix_dict = {}\n",
    "\n",
    "    for i in range(len(columns)):\n",
    "        for j in range(i + 1, len(columns)):\n",
    "            col_1 = columns[i]\n",
    "            col_2 = columns[j]\n",
    "            key = col_1 + '_+_' + col_2\n",
    "            res = create_coocc_matrix(df, col_1, col_2)\n",
    "            columnWise_coOccMatrix_dict[key] = res\n",
    "    columnWise_coOccMatrix_dict = OrderedDict(columnWise_coOccMatrix_dict)\n",
    "    return columnWise_coOccMatrix_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_initial_entity_embeddings(\n",
    "        train_data_file,\n",
    "        model_data_save_dir,\n",
    "        DATA_DIR,\n",
    "        embedding_dims,\n",
    "        num_epochs,\n",
    "        id_col='PanjivaRecordID'\n",
    "):\n",
    "    train_df = pd.read_csv(os.path.join(DATA_DIR, train_data_file))\n",
    "    feature_cols = list(train_df.columns)\n",
    "    feature_cols = list(feature_cols)\n",
    "    feature_cols.remove(id_col)\n",
    "    domains = sorted(feature_cols)\n",
    "    print(feature_cols)\n",
    "\n",
    "    data = train_df[feature_cols].values\n",
    "    # ------------------------------- #\n",
    "    coOcc_dict_file = os.path.join(model_data_save_dir, \"coOccMatrix_dict.pkl\")\n",
    "    X_ij_file = os.path.join(model_data_save_dir, \"X_ij.npy\")\n",
    "    domain_dims_file = os.path.join(DATA_DIR, \"domain_dims.pkl\")\n",
    "    domain_dims = utils_1.get_domain_dims(domain_dims_file)\n",
    "\n",
    "    # -----\n",
    "    # Check if pairwise co-occurrence dictionary exists\n",
    "    # -----\n",
    "    if os.path.exists(coOcc_dict_file):\n",
    "        with open(coOcc_dict_file, 'rb') as fh:\n",
    "            coOccMatrix_dict = pickle.load(fh)\n",
    "    else:\n",
    "        coOccMatrix_dict = get_coOccMatrix_dict(train_df, id_col='PanjivaRecordID')\n",
    "        with open(coOcc_dict_file, \"wb\") as fh:\n",
    "            pickle.dump(coOccMatrix_dict, fh, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "    # ----------------\n",
    "    # Ensure X_ij is in a flattened format ; i < j\n",
    "    # ----------------\n",
    "    if os.path.exists(X_ij_file):\n",
    "        with open(X_ij_file, 'rb') as fh:\n",
    "            X_ij = np.load(fh)\n",
    "\n",
    "    else:\n",
    "        nd = len(feature_cols)\n",
    "        num_c = nd * (nd - 1) // 2\n",
    "        X_ij = np.zeros([data.shape[0], num_c])\n",
    "        k = 0\n",
    "        for i in range(len(feature_cols)):\n",
    "            for j in range(i + 1, len(feature_cols)):\n",
    "                key = feature_cols[i] + '_+_' + feature_cols[j]\n",
    "                for d in range(data.shape[0]):\n",
    "                    e1 = data[d][i]\n",
    "                    e2 = data[d][j]\n",
    "                    X_ij[d][k] = coOccMatrix_dict[key][e1][e2]\n",
    "                k += 1\n",
    "        X_ij = np.asarray(X_ij,np.int32)\n",
    "        with open(X_ij_file, \"wb\") as fh:\n",
    "            np.save(fh, X_ij)\n",
    "\n",
    "    # -------------------------------- #\n",
    "\n",
    "    # X_ij_max needed for scaling\n",
    "    X_ij_max = []\n",
    "    for k, v in coOccMatrix_dict.items():\n",
    "        X_ij_max.append(np.max(v))\n",
    "\n",
    "    num_domains = len(domain_dims)\n",
    "    print(domain_dims.values())\n",
    "\n",
    "    model = get_model(\n",
    "        domain_dimesnsions=list(domain_dims.values()),\n",
    "        num_domains=num_domains,\n",
    "        embed_dim=embedding_dims,\n",
    "        _X_ij_max=X_ij_max\n",
    "    )\n",
    "\n",
    "    # check if model present !!\n",
    "    _present = len(glob.glob(os.path.join(model_data_save_dir, 'embedding_w_**')))>0\n",
    "    if not _present :\n",
    "        model = train_model(\n",
    "            model,\n",
    "            data,\n",
    "            X_ij,\n",
    "            file_save_loc=model_data_save_dir,\n",
    "            epochs=num_epochs\n",
    "        )\n",
    "    # ----\n",
    "    # Save the embeddings (weights) in a dictionary\n",
    "    # ----\n",
    "    emb_w = {}\n",
    "    for i in range(len(feature_cols)):\n",
    "        dom = feature_cols[i]\n",
    "        f_path = os.path.join(model_data_save_dir, 'embedding_w_{}.npy'.format(i))\n",
    "        w = np.load(f_path)\n",
    "        emb_w[dom] = w\n",
    "\n",
    "    # ================== \n",
    "    # Modifying concept of  GloVe\n",
    "    # emb ( entity = E in D)\n",
    "    #  x = 0\n",
    "    #  For d in {Doamian} - D\n",
    "    #     x += Sum (CoOcc( E, E_d`)/max(CoOcc( E, E_d`)) *  emb ( entity = E ))\n",
    "    #  x = 1/2(emb_old(E) + x)\n",
    "    # ==================\n",
    "\n",
    "    new_embeddings = {}\n",
    "    for domain_i in domains:\n",
    "        new_embeddings[domain_i] = np.zeros(\n",
    "            emb_w[domain_i].shape\n",
    "        )\n",
    "\n",
    "        domain_dim = domain_dims[domain_i]\n",
    "        # For each entity in domain i \n",
    "        for entity_id in range(domain_dim):\n",
    "            res = 0\n",
    "            # For each entity in domain j != i\n",
    "            for domain_j in domains:\n",
    "                if domain_j == domain_i: continue\n",
    "                pair = sorted([domain_i, domain_j])\n",
    "\n",
    "                key = '_+_'.join(pair)\n",
    "                coOcc_matrix = coOccMatrix_dict[key]\n",
    "                if domain_i == pair[0]:\n",
    "                    arr = coOcc_matrix[entity_id, :]\n",
    "                else:\n",
    "                    arr = coOcc_matrix[:, entity_id]\n",
    "\n",
    "                sum_co_occ = max(np.sum(arr), 1)\n",
    "                scale = np.reshape(arr / sum_co_occ, [-1, 1])\n",
    "\n",
    "                emb_domain_j = emb_w[domain_j]\n",
    "                res_j = np.sum(scale * scale * emb_domain_j, axis=0)\n",
    "                res = res + res_j\n",
    "\n",
    "            res = 0.5 * (res + emb_w[domain_i][entity_id])\n",
    "#             res = emb_w[domain_i][entity_id]\n",
    "            new_embeddings[domain_i][entity_id] = res\n",
    "\n",
    "    # Write the embeddings to file \n",
    "    for domain_i in domains:\n",
    "        print(' >> ', domain_i)\n",
    "        file_name = os.path.join(\n",
    "            model_data_save_dir,\n",
    "            'init_embedding_' + domain_i + '_' + str(embedding_dims) + '.npy'\n",
    "        )\n",
    "        np.save(\n",
    "            file=file_name,\n",
    "            arr=new_embeddings[domain_i]\n",
    "        )\n",
    "    \n",
    "    # =================================\n",
    "    # This is only for testing whether the model works\n",
    "    # Usually not called, only for debugging\n",
    "    # =================================\n",
    "    def test():\n",
    "        hscodeList = [10,25,35,40,50,55,75,90]\n",
    "        \n",
    "        for hscode in hscodeList:\n",
    "            print('-----> ::: ',hscode)\n",
    "            # find the 10 closest  to ShipmentDestination to HSCode in data\n",
    "            df = train_df.loc[train_df['HSCode'] == hscode]\n",
    "            df = df.groupby(['HSCode', 'PortOfLading']).size().reset_index(name='counts')\n",
    "            df = df.sort_values(by=['counts'])\n",
    "\n",
    "            k_closest = df.tail(10)['PortOfLading'].values\n",
    "            print(k_closest)\n",
    "\n",
    "            # hs_code_vec = wt[0][hscode] + bias[0][hscode]\n",
    "            hs_code_vec = new_embeddings['HSCode'][hscode]\n",
    "\n",
    "            shp_dest_vec = []\n",
    "            wt = new_embeddings['PortOfLading']\n",
    "            for i in range(wt.shape[0]):\n",
    "                r = wt[i]\n",
    "                shp_dest_vec.append(r)\n",
    "\n",
    "            res = {}\n",
    "            for i in range(wt.shape[0]):\n",
    "                a = np.reshape(shp_dest_vec[i], [1, -1])\n",
    "                b = np.reshape(hs_code_vec, [1, -1])\n",
    "                res[i] = cosine_similarity(a, b)\n",
    "\n",
    "            new_df = pd.DataFrame(list(res.items()))\n",
    "            new_df = new_df.sort_values(by=[1])\n",
    "            new_df = new_df.tail(10)\n",
    "            print(list(new_df[0]))\n",
    "            \n",
    "            \n",
    "            print('----->')\n",
    "    test()\n",
    "    \n",
    "    return new_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Carrier', 'ConsigneePanjivaID', 'HSCode', 'PortOfLading', 'PortOfUnlading', 'ShipmentDestination', 'ShipmentOrigin', 'ShipperPanjivaID']\n",
      "dict_values([548, 5113, 95, 238, 64, 113, 116, 6193])\n",
      " >>  Carrier\n",
      " >>  ConsigneePanjivaID\n",
      " >>  HSCode\n",
      " >>  PortOfLading\n",
      " >>  PortOfUnlading\n",
      " >>  ShipmentDestination\n",
      " >>  ShipmentOrigin\n",
      " >>  ShipperPanjivaID\n",
      "-----> :::  10\n",
      "[ 63  79 107 234]\n",
      "[37, 54, 235, 10, 84, 62, 49, 127, 11, 234]\n",
      "----->\n",
      "-----> :::  25\n",
      "[  4  58 126 217 184 106 164  96 130 146]\n",
      "[153, 34, 115, 202, 9, 161, 96, 51, 204, 146]\n",
      "----->\n",
      "-----> :::  35\n",
      "[ 27  80  96 151 164 146 184 130]\n",
      "[204, 9, 96, 153, 152, 184, 53, 118, 52, 130]\n",
      "----->\n",
      "-----> :::  40\n",
      "[217  66  10 211   5  84 111 216 138 184]\n",
      "[84, 220, 236, 24, 111, 216, 99, 235, 205, 138]\n",
      "----->\n",
      "-----> :::  50\n",
      "[234 146 164  54 184 126 232 217  96 130]\n",
      "[233, 130, 152, 181, 137, 151, 75, 32, 144, 52]\n",
      "----->\n",
      "-----> :::  55\n",
      "[111 102 163 130 184  41  71 220 103  96]\n",
      "[68, 130, 153, 20, 71, 161, 43, 28, 118, 96]\n",
      "----->\n",
      "-----> :::  75\n",
      "[ 82 106 126 145 177 184 234  96 130  54]\n",
      "[83, 180, 110, 147, 14, 152, 93, 130, 13, 54]\n",
      "----->\n",
      "-----> :::  90\n",
      "[146 234 211  57 217  66 177 139  46   7]\n",
      "[56, 225, 125, 193, 174, 158, 135, 46, 167, 7]\n",
      "----->\n"
     ]
    }
   ],
   "source": [
    "training_data_file = CONFIG['train_data_file']\n",
    " \n",
    "src_DIR = os.path.join(DATA_DIR, DIR)\n",
    "embeddings = get_initial_entity_embeddings(\n",
    "    training_data_file,\n",
    "    modelData_SaveDir,\n",
    "    src_DIR,\n",
    "    embedding_dims=256,\n",
    "    num_epochs=40\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
